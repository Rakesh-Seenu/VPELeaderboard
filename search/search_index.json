{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to Team VPE's Leaderboard Repository! This project provides an interactive framework for biomodel simulation and time series prediction. By integrating AI-driven time series models, we extend traditional biomodel simulations to forecast future states based on initial conditions.</p> <p>(This repository is a work in progress..)</p> <ul> <li> <p> Get Started</p> <p>Kickstart your journey with VPE Leaderboard \u2013 install the PyPI package and follow an easy step-by-step guide for effortless setup.</p> <p>Get Started</p> </li> <li> <p> Data</p> <p>Discover datasets integrated seamlessly into the framework for streamlined analysis and experimentation.</p> <p>Explore Data</p> </li> <li> <p> Algorithms</p> <p>Discover cutting-edge algorithms designed for time series forecasting and predictive modeling.</p> <p>Explore Algorithms</p> </li> <li> <p> Leaderboard</p> <p>Explore algorithmic outputs and performance metrics in the leaderboard to evaluate and compare results effortlessly.</p> <p>Explore Leaderboard</p> </li> </ul>"},{"location":"algorithms/","title":"Algorithms","text":"Algorithm Metrics <p>This page was last updated on 2025-06-09 09:42:59 UTC</p>"},{"location":"algorithms/#algorithm_metrics","title":"Algorithm Metrics Table","text":"Algorithm Metric 1 Metric 2 Metric 3 Algo 1 0.86 0.35 0.8 Algo 2 0.94 0.32 0.9 Algo 3 0.91 0.34 0.83 Algo 4 0.89 0.33 0.87 Algo 5 0.82 0.29 0.97 Algo 6 0.82 0.34 0.81 Algo 7 0.81 0.21 0.85 Algo 8 0.93 0.23 0.93 Algo 9 0.89 0.21 0.8 Algo 10 0.91 0.25 0.77 Algo 11 0.8 0.26 0.82 Algo 12 0.95 0.24 0.79 Algo 13 0.92 0.32 0.97 Algo 14 0.83 0.25 0.94 Algo 15 0.83 0.24 0.9 Algo 16 0.83 0.28 0.96 Algo 17 0.85 0.22 0.94 Algo 18 0.88 0.32 0.79 Algo 19 0.86 0.21 0.96 Algo 20 0.84 0.35 0.88 Algo 21 0.89 0.32 0.94 Algo 22 0.82 0.23 0.97 Algo 23 0.84 0.2 0.83 Algo 24 0.85 0.32 0.78 Algo 25 0.87 0.31 0.8 Algo 26 0.92 0.31 0.85 Algo 27 0.83 0.32 0.95 Algo 28 0.88 0.21 0.96 Algo 29 0.89 0.25 0.75 Algo 30 0.81 0.22 0.87 Algo 31 0.89 0.33 0.85 Algo 32 0.83 0.29 0.8 Algo 33 0.81 0.25 0.78 Algo 34 0.94 0.21 0.83 Algo 35 0.94 0.25 0.98 Algo 36 0.92 0.25 0.83 Algo 37 0.85 0.31 0.87 Algo 38 0.81 0.3 0.92 Algo 39 0.9 0.33 0.84 Algo 40 0.87 0.27 0.98 Algo 41 0.82 0.22 0.98 Algo 42 0.87 0.31 0.81 Algo 43 0.81 0.31 0.87 Algo 44 0.94 0.28 0.82 Algo 45 0.84 0.32 0.82 Algo 46 0.9 0.27 0.76 Algo 47 0.85 0.28 0.9 Algo 48 0.88 0.26 0.87 Algo 49 0.88 0.2 0.76 Algo 50 0.83 0.22 0.82"},{"location":"data/","title":"SBML Data","text":"Time-series Forecasting Articles <p>This page was last updated on 2025-06-09 09:43:05 UTC</p>"},{"location":"data/#algorithm_metrics","title":"System Biology Models","text":"Abstract Model Name Number of Species Number of Parameters visibility_off Teusink2000_Glycolysis 26 15 <p> Can yeast glycolysis be understood in terms of in vitro kinetics of the constituent enzymes? Testing biochemistry. Teusink,B et al.: Eur J Biochem 2000 Sep;267(17):5313-29.            The model reproduces the steady-state fluxes and metabolite concentrations of the branched model as given in Table 4 of the paper. It is derived from the model on JWS online, but has the ATP consumption in the succinate branch with the same stoichiometrie as in the publication. The model was successfully tested on copasi v.4.4(build 26).                 For Vmax values, please note that there is a conversion factor of approx. 270 to convert from U/mg-protein as shown in Table 1 of the paper to mmol/(min*L_cytosol). The equilibrium constant for the ADH reaction in the paper is given for the reverse reaction (Keq = 1.45*10      <sup>4</sup>           ). The value used in this model is for the forward reaction: 1/Keq = 6.9*10      <sup>-5</sup>           .                 Vmax parameters values used (in [mM/min] except VmGLT):       VmGLT 97.264 mmol/min VmGLK 226.45 VmPGI 339.667 VmPFK 182.903 VmALD 322.258 VmGAPDH_f 1184.52 VmGAPDH_r 6549.68 VmPGK 1306.45 VmPGM 2525.81 VmENO 365.806 VmPYK 1088.71 VmPDC 174.194 VmG3PDH 70.15            The result of the G6P steady state concentration (marked in red) differs slightly from the one given in table 4. of the publication                 Results for steady state:       orig. article this model Fluxes[mM/min] Glucose\u00a0 88\u00a0 88\u00a0 Ethanol\u00a0 129\u00a0 129\u00a0 Glycogen\u00a0 6\u00a0 6\u00a0 Trehalose\u00a0 4.8\u00a0 4.8\u00a0 (G6P flux through trehalose branch) Glycerol\u00a0 18.2\u00a0 18.2\u00a0 Succinate\u00a0 3.6\u00a0 3.6\u00a0 Conc.[mM] G6P\u00a0 1.07\u00a0 1.03\u00a0 F6P\u00a0 0.11\u00a0 0.11\u00a0 F1,6P\u00a0 0.6\u00a0 0.6\u00a0 DHAP\u00a0 0.74\u00a0 0.74\u00a0 3PGA\u00a0 0.36\u00a0 0.36\u00a0 2PGA\u00a0 0.04\u00a0 0.04\u00a0 PEP\u00a0 0.07\u00a0 0.07\u00a0 PYR\u00a0 8.52\u00a0 8.52\u00a0 AcAld\u00a0 0.17\u00a0 0.17\u00a0 ATP\u00a0 2.51\u00a0 2.51\u00a0 ADP\u00a0 1.29\u00a0 1.29\u00a0 AMP\u00a0 0.3\u00a0 0.3\u00a0 NAD\u00a0 1.55\u00a0 1.55\u00a0 NADH\u00a0 0.04\u00a0 0.04\u00a0            Authors of the publication also mentioned a few misprints in the original article:                 in the kinetic law for      ADH           :      <ol> <li>the species          a               should denote          NAD               and          b Ethanol</li> <li>the last term in the equation should read          bpq               /(          K            <sub>ib</sub>                 K            <sub>iq</sub>                 K            <sub>p</sub>               )          </li> </ol>           in the kinetic law for      PFK           :      <ol> <li>R = 1 + \u03bb          <sub>1</sub>               + \u03bb          <sub>2</sub>               + g          <sub>r</sub>               \u03bb          <sub>1</sub>               \u03bb          <sub>2</sub></li> <li>equation L  should read: L = L0*(..)          <sup>2</sup>               *(..)          <sup>2</sup>               *(..)          <sup>2</sup>               not L = L0*(..)          <sup>2</sup>               *(..)          <sup>2</sup>               *(..)          </li> </ol>           To make the model easier to curate, the species      ATP           ,      ADP           and      AMP           were added. These are calculated via assignment rules from the active phosphate species,      P           , and the sum of all      AXP           ,      SUM_P           .      </p> <p>To the extent possible under law, all copyright and related or neighbouring rights to this encoded model have been dedicated to the public domain worldwide. Please refer to      CC0 Public Domain Dedication           for more information.      </p> <p>In summary, you are entitled to use this encoded model in absolutely any manner you deem suitable, verbatim, or with modification, alone or embedded it in a larger context, redistribute it, commercially or not, in a restricted way or not.</p> <p>To cite BioModels Database, please use:      Li C, Donizelli M, Rodriguez N, Dharuri H, Endler L, Chelliah V, Li L, He E, Henry A, Stefan MI, Snoep JL, Hucka M, Le Nov\u00e8re N, Laibe C (2010) BioModels Database: An enhanced, curated and annotated resource for published quantitative kinetic models. BMC Syst Biol., 4:92.</p> visibility_off Tang2020 - Estimation of transmission risk of COVID-19 and impact of public health interventions 8 16 <p>Since the emergence of the first cases in Wuhan, China, the novel coronavirus (2019-nCoV) infection has been quickly spreading out to other provinces and neighboring countries. Estimation of the basic reproduction number by means of mathematical modeling can be helpful for determining the potential and severity of an outbreak and providing critical information for identifying the type of disease interventions and intensity. A deterministic compartmental model was devised based on the clinical progression of the disease, epidemiological status of the individuals, and intervention measures. The estimations based on likelihood and model analysis show that the control reproduction number may be as high as 6.47 (95% CI 5.71\u20137.23). Sensitivity analyses show that interventions, such as intensive contact tracing followed by quarantine and isolation, can effectively reduce the control reproduction number and transmission risk, with the effect of travel restriction adopted by Wuhan on 2019-nCoV infection in Beijing being almost equivalent to increasing quarantine by a 100 thousand baseline value. It is essential to assess how the expensive, resource-intensive measures implemented by the Chinese authorities can contribute to the prevention and control of the 2019-nCoV infection, and how long they should be maintained. Under the most restrictive measures, the outbreak is expected to peak within two weeks (since 23 January 2020) with a significant low peak value. With travel restriction (no imported exposed individuals to Beijing), the number of infected individuals in seven days will decrease by 91.14% in Beijing, compared with the scenario of no travel restriction.</p> visibility_off Dwivedi2014 - Crohns IL6 Disease model - Anti-IL6R Antibody 44 53 Dwivedi2014 - Crohns IL6 Disease model - Anti-IL6R Antibody This model is comprised of four models:  <ul> <li> [BIOMD0000000534]   Healthy Volunteer model   </li> <li> [BIOMD0000000535]   Crohn's Disease - IL-6 Antibody   </li> <li> [BIOMD0000000536]   Crohn's Disease - sgp130FC</li> <li> [BIOMD0000000537]   Crohn's Disease - IL-6Ra Antibody   </li> </ul>Possible avenues for Interleukin-6 (IL-6) inhibition in treating Crohn's disease are compared here. Each model refers to separate ligands. The system simulates differential activity of the ligands on the signalling of IL-6.  This affects Signal Transducer and Activator of Transcription 3 (STAT3) activity on the production of biomarker C-Reactive Protein (CRP) expression. Figures referring to this Crohn's Disease model are 3a, 4d, 4e, 4f and 5b.  <p>This model is described in the article:</p> A multiscale model of     interleukin-6-mediated immune regulation in Crohn's disease and     its application in drug discovery and development. Dwivedi G, Fitz L, Hegen M, Martin   SW, Harrold J, Heatherington A, Li C. CPT Pharmacometrics Syst Pharmacol   2014; 3: e89 <p>Abstract:</p> <p>In this study, we have developed a multiscale systems model     of interleukin (IL)-6-mediated immune regulation in Crohn's     disease, by integrating intracellular signaling with     organ-level dynamics of pharmacological markers underlying the     disease. This model was linked to a general pharmacokinetic     model for therapeutic monoclonal antibodies and used to     comparatively study various biotherapeutic strategies targeting     IL-6-mediated signaling in Crohn's disease. Our work     illustrates techniques to develop mechanistic models of disease     biology to study drug-system interaction. Despite a sparse     training data set, predictions of the model were qualitatively     validated by clinical biomarker data from a pilot trial with     tocilizumab. Model-based analysis suggests that strategies     targeting IL-6, IL-6R?, or the IL-6/sIL-6R? complex are less     effective at suppressing pharmacological markers of Crohn's     than dual targeting the IL-6/sIL-6R? complex in addition to     IL-6 or IL-6R?. The potential value of multiscale system     pharmacology modeling in drug discovery and development is also     discussed.CPT: Pharmacometrics &amp; Systems Pharmacology     (2014) 3, e89; doi:10.1038/psp.2013.64; advance online     publication 8 January 2014.</p> <p>This model is hosted on    BioModels Database   and identified by:    BIOMD0000000537.</p> <p>To cite BioModels Database, please use:    BioModels Database:   An enhanced, curated and annotated resource for published   quantitative kinetic models.</p> <p>To the extent possible under law, all copyright and related or   neighbouring rights to this encoded model have been dedicated to   the public domain worldwide. Please refer to    CC0   Public Domain Dedication for more information.</p>"},{"location":"data/#how_to_contribute","title":"How To Add Your Models","text":"<p>       To contribute new models to the leaderboard, please follow the instructions in the Model Submission Guide section. This guide provides the necessary steps for preparing and submitting your models, ensuring they are automatically validated and integrated into the leaderboard system via our CI/CD pipeline.     </p>"},{"location":"data/basico_model/","title":"Basico Model","text":"<p>BasicoModel class for loading SBML models using the basico package.</p>"},{"location":"data/basico_model/#vpeleaderboard.data.src.sbml.basico_model.BasicoModel","title":"<code>BasicoModel</code>","text":"<p>               Bases: <code>SysBioModel</code></p> <p>Model that loads SBML models using the basico package. Ensures a single instance per component.</p> Source code in <code>vpeleaderboard/data/src/sbml/basico_model.py</code> <pre><code>class BasicoModel(SysBioModel):\n    \"\"\"\n    Model that loads SBML models using the basico package.\n    Ensures a single instance per component.\n    \"\"\"\n    sbml_file_path: str = Field(..., description=\"Path to an SBML file \")\n    simulation_results: Optional[Any] = Field(None, exclude=True)\n    name: Optional[str] = \"\"\n    description: Optional[str] = \"\"\n    copasi_model: Optional[object] = Field(None, exclude=True)\n\n    def __init__(self, sbml_file_path: str ,\n                 name: Optional[str] = \"\", description: Optional[str] = \"\"):\n        super().__init__(sbml_file_path=sbml_file_path,\n                         name=name, description=description)\n        # sbml_file_path = os.path.abspath(sbml_file_path)\n        self.sbml_file_path = sbml_file_path\n        self.validate_sbml_file_path()\n\n    def validate_sbml_file_path(self):\n        \"\"\"\n        Validate that the SBML folder exists and contains XML files.\n        \"\"\"\n        if not self.sbml_file_path:\n            raise ValueError(\"SBML file must be provided.\")\n\n        if not os.path.exists(self.sbml_file_path):\n            raise ValueError(f\"SBML file not found: {self.sbml_file_path}\")\n\n    def get_model_metadata(self) -&gt; Dict[str, Union[str, int]]:\n        \"\"\"\n        Retrieve metadata for a single SBML model.\n        \"\"\"\n        # file_path = os.path.join(self.sbml_file_path)\n        copasi_model = basico.load_model(self.sbml_file_path)\n        model_name = basico.model_info.get_model_name(model=copasi_model)\n        species_count = len(basico.model_info.get_species(model=copasi_model))\n        parameter_count = len(basico.model_info.get_parameters(model=copasi_model))\n        model_description = basico.model_info.get_notes(model=copasi_model)\n\n        return {\n            \"Model Name\": model_name,\n            \"Number of Species\": species_count,\n            \"Number of Parameters\": parameter_count,\n            \"Description\": model_description.strip()\n\n        }\n</code></pre>"},{"location":"data/basico_model/#vpeleaderboard.data.src.sbml.basico_model.BasicoModel.get_model_metadata","title":"<code>get_model_metadata()</code>","text":"<p>Retrieve metadata for a single SBML model.</p> Source code in <code>vpeleaderboard/data/src/sbml/basico_model.py</code> <pre><code>def get_model_metadata(self) -&gt; Dict[str, Union[str, int]]:\n    \"\"\"\n    Retrieve metadata for a single SBML model.\n    \"\"\"\n    # file_path = os.path.join(self.sbml_file_path)\n    copasi_model = basico.load_model(self.sbml_file_path)\n    model_name = basico.model_info.get_model_name(model=copasi_model)\n    species_count = len(basico.model_info.get_species(model=copasi_model))\n    parameter_count = len(basico.model_info.get_parameters(model=copasi_model))\n    model_description = basico.model_info.get_notes(model=copasi_model)\n\n    return {\n        \"Model Name\": model_name,\n        \"Number of Species\": species_count,\n        \"Number of Parameters\": parameter_count,\n        \"Description\": model_description.strip()\n\n    }\n</code></pre>"},{"location":"data/basico_model/#vpeleaderboard.data.src.sbml.basico_model.BasicoModel.validate_sbml_file_path","title":"<code>validate_sbml_file_path()</code>","text":"<p>Validate that the SBML folder exists and contains XML files.</p> Source code in <code>vpeleaderboard/data/src/sbml/basico_model.py</code> <pre><code>def validate_sbml_file_path(self):\n    \"\"\"\n    Validate that the SBML folder exists and contains XML files.\n    \"\"\"\n    if not self.sbml_file_path:\n        raise ValueError(\"SBML file must be provided.\")\n\n    if not os.path.exists(self.sbml_file_path):\n        raise ValueError(f\"SBML file not found: {self.sbml_file_path}\")\n</code></pre>"},{"location":"data/biobridge_primekg/","title":"Biobridge Primekg","text":"<p>Class for loading BioBridgePrimeKG dataset and  PrimeKG nodes and edges data set.</p>"},{"location":"data/biobridge_primekg/#vpeleaderboard.data.src.kg.biobridge_primekg.BioBridgePrimeKG","title":"<code>BioBridgePrimeKG</code>","text":"<p>Class for loading BioBridgePrimeKG dataset. It downloads the data from the BioBridge repo and stores it in the local directory. The data is then loaded into pandas DataFrame of nodes and edges. This class was adapted from the BioBridge repo: https://github.com/RyanWangZf/BioBridge</p> Source code in <code>vpeleaderboard/data/src/kg/biobridge_primekg.py</code> <pre><code>class BioBridgePrimeKG:\n    \"\"\"\n    Class for loading BioBridgePrimeKG dataset.\n    It downloads the data from the BioBridge repo and stores it in the local directory.\n    The data is then loaded into pandas DataFrame of nodes and edges.\n    This class was adapted from the BioBridge repo:\n    https://github.com/RyanWangZf/BioBridge\n    \"\"\"\n\n    def __init__(self, cfg: DictConfig):\n        \"\"\"\n        Constructor for BioBridgePrimeKG class.\n\n        Args:\n            primekg_dir (str): The directory of PrimeKG dataset.\n            local_dir (str): The directory to store the downloaded data.\n            random_seed (int): The random seed value.\n        \"\"\"\n        self.name: str = \"biobridge_primekg\"\n        # self.cfg = cfg\n        self.primekg_dir: str = cfg.data.primekg_dir\n        self.local_dir: str = cfg.data.biobridge_dir\n        self.random_seed: int = cfg.data.random_seed if \"random_seed\" in cfg.data else 0\n        self.n_neg_samples: int = cfg.data.n_neg_samples if \"n_neg_samples\" in cfg.data else 5\n        # Preselected node types:\n        # protein, molecular function, cellular component, biological process, drug, disease\n        self.preselected_node_types = [\"protein\", \"mf\", \"cc\", \"bp\", \"drug\", \"disease\"]\n        self.node_type_map = {\n            \"protein\": \"gene/protein\",\n            \"mf\": \"molecular_function\",\n            \"cc\": \"cellular_component\",\n            \"bp\": \"biological_process\",\n            \"drug\": \"drug\",\n            \"disease\": \"disease\",\n        }\n\n        # Attributes to store the data\n        self.primekg = None\n        self.primekg_triplets = None\n        # self.primekg_triplets_negative = None\n        self.data_config = None\n        self.emb_dict = None\n        self.df_train = None\n        self.df_node_train = None\n        self.df_test = None\n        self.df_node_test = None\n        self.node_info_dict = None\n\n        # Set up the dataset\n        self.setup()\n\n    def setup(self):\n        \"\"\"\n        A method to set up the dataset.\n        \"\"\"\n        # Make the directories if it doesn't exist\n        # os.makedirs(os.path.dirname(self.primekg_dir), exist_ok=True)\n        os.makedirs(os.path.dirname(self.local_dir), exist_ok=True)\n\n        # Set the random seed\n        self.set_random_seed(self.random_seed)\n\n        # Set SettingWithCopyWarning  warnings to none\n        pd.options.mode.chained_assignment = None\n\n    def _load_primekg(self) -&gt; PrimeKG:\n        \"\"\"\n        Private method to load related files of PrimeKG dataset.\n\n        Returns:\n            The PrimeKG dataset.\n        \"\"\"\n        primekg_data = PrimeKG(self.primekg_dir)\n        primekg_data.load_data()\n\n        return primekg_data\n\n    def _download_file(self,\n                       remote_url:str,\n                       local_dir: str,\n                       local_filename: str):\n        \"\"\"\n        A helper function to download a file from remote URL to the local directory.\n\n        Args:\n            remote_url (str): The remote URL of the file to be downloaded.\n            local_dir (str): The local directory to store the downloaded file.\n            local_filename (str): The local filename to store the downloaded file.\n        \"\"\"\n        # Make the local directory if it does not exist\n        if not os.path.exists(local_dir):\n            os.makedirs(local_dir)\n        # Download the file from remote URL to local directory\n        local_path = os.path.join(local_dir, local_filename)\n        if os.path.exists(local_path):\n            print(f\"File {local_filename} already exists in {local_dir}.\")\n        else:\n            print(f\"Downloading {local_filename} from {remote_url} to {local_dir}...\")\n            response = requests.get(remote_url, stream=True, timeout=300)\n            response.raise_for_status()\n            progress_bar = tqdm(\n                total=int(response.headers.get(\"content-length\", 0)),\n                unit=\"iB\",\n                unit_scale=True,\n            )\n            with open(os.path.join(local_dir, local_filename), \"wb\") as file:\n                for data in response.iter_content(1024):\n                    progress_bar.update(len(data))\n                    file.write(data)\n            progress_bar.close()\n\n    def _load_data_config(self) -&gt; dict:\n        \"\"\"\n        Load the data config file of BioBridgePrimeKG dataset.\n\n        Returns:\n            The data config file of BioBridgePrimeKG dataset.\n        \"\"\"\n        # Download the data config file of BioBridgePrimeKG\n        self._download_file(\n            remote_url= ('https://raw.githubusercontent.com/RyanWangZf/BioBridge/'\n                         'refs/heads/main/data/BindData/data_config.json'),\n            local_dir=self.local_dir,\n            local_filename='data_config.json')\n\n        # Load the downloaded data config file\n        with open(os.path.join(self.local_dir, 'data_config.json'), 'r', encoding='utf-8') as f:\n            data_config = json.load(f)\n\n        return data_config\n\n    def _build_node_embeddings(self) -&gt; dict:\n        \"\"\"\n        Build the node embeddings for BioBridgePrimeKG dataset.\n\n        Returns:\n            The dictionary of node embeddings.\n        \"\"\"\n        processed_file_path = os.path.join(self.local_dir, \"embeddings\", \"embedding_dict.pkl\")\n        if os.path.exists(processed_file_path):\n            # Load the embeddings from the local directory\n            with open(processed_file_path, \"rb\") as f:\n                emb_dict_all = pickle.load(f)\n        else:\n            # Download the embeddings from the BioBridge repo and further process them\n            # List of embedding source files\n            url = ('https://media.githubusercontent.com/media/RyanWangZf/BioBridge/'\n                   'refs/heads/main/data/embeddings/esm2b_unimo_pubmedbert/')\n            file_list = [f\"{n}.pkl\" for n in self.preselected_node_types]\n\n            # Download the embeddings\n            for file in file_list:\n                self._download_file(remote_url=os.path.join(url, file),\n                                    local_dir=os.path.join(self.local_dir, \"embeddings\"),\n                                    local_filename=file)\n\n            # Unified embeddings\n            emb_dict_all = {}\n            for file in file_list:\n                with open(os.path.join(self.local_dir, \"embeddings\", file), \"rb\") as f:\n                    emb = pickle.load(f)\n                emb_ar = emb[\"embedding\"]\n                if not isinstance(emb_ar, list):\n                    emb_ar = emb_ar.tolist()\n                emb_dict_all.update(dict(zip(emb[\"node_index\"], emb_ar)))\n\n            # Store embeddings\n            with open(processed_file_path, \"wb\") as f:\n                pickle.dump(emb_dict_all, f)\n\n        return emb_dict_all\n\n    def _build_full_triplets(self) -&gt; tuple[pd.DataFrame, dict]:\n        \"\"\"\n        Build the full triplets for BioBridgePrimeKG dataset.\n\n        Returns:\n            The full triplets for BioBridgePrimeKG dataset.\n            The dictionary of node information.\n        \"\"\"\n        processed_file_path = os.path.join(self.local_dir, \"processed\", \"triplet_full.tsv.gz\")\n        if os.path.exists(processed_file_path):\n            # Load the file from the local directory\n            with open(processed_file_path, \"rb\") as f:\n                primekg_triplets = pd.read_csv(f, sep=\"\\t\", compression=\"gzip\", low_memory=False)\n\n            # Load each dataframe in the local directory\n            node_info_dict = {}\n            for i, node_type in enumerate(self.preselected_node_types):\n                with open(os.path.join(self.local_dir, \"processed\",\n                                       f\"{node_type}.csv\"), \"rb\") as f:\n                    df_node = pd.read_csv(f)\n                node_info_dict[self.node_type_map[node_type]] = df_node\n        else:\n            # Download the related files from the BioBridge repo and further process them\n            # List of processed files\n            url = ('https://media.githubusercontent.com/media/RyanWangZf/BioBridge/'\n                   'refs/heads/main/data/Processed/')\n            file_list = [\"protein\", \"molecular\", \"cellular\", \"biological\", \"drug\", \"disease\"]\n\n            # Download the processed files\n            for i, file in enumerate(file_list):\n                self._download_file(remote_url=os.path.join(url, f\"{file}.csv\"),\n                                    local_dir=os.path.join(self.local_dir, \"processed\"),\n                                    local_filename=f\"{self.preselected_node_types[i]}.csv\")\n\n            # Build the node index list\n            node_info_dict = {}\n            node_index_list = []\n            for i, file in enumerate(file_list):\n                df_node = pd.read_csv(os.path.join(self.local_dir, \"processed\",\n                                                   f\"{self.preselected_node_types[i]}.csv\"))\n                node_info_dict[self.node_type_map[self.preselected_node_types[i]]] = df_node\n                node_index_list.extend(df_node[\"node_index\"].tolist())\n\n            # Filter the PrimeKG dataset to take into account only the selected node types\n            primekg_triplets = self.primekg.get_edges().copy()\n            primekg_triplets = primekg_triplets[\n                primekg_triplets[\"head_index\"].isin(node_index_list) &amp;\\\n                primekg_triplets[\"tail_index\"].isin(node_index_list)\n            ]\n            primekg_triplets = primekg_triplets.reset_index(drop=True)\n\n            # Perform mapping of node types\n            primekg_triplets[\"head_type\"] = primekg_triplets[\"head_type\"].apply(\n                lambda x: self.data_config[\"node_type\"][x]\n            )\n            primekg_triplets[\"tail_type\"] = primekg_triplets[\"tail_type\"].apply(\n                lambda x: self.data_config[\"node_type\"][x]\n            )\n\n            # Perform mapping of relation types\n            primekg_triplets[\"display_relation\"] = primekg_triplets[\"display_relation\"].apply(\n                lambda x: self.data_config[\"relation_type\"][x]\n            )\n\n            # Store the processed triplets\n            primekg_triplets.to_csv(processed_file_path, sep=\"\\t\", compression=\"gzip\", index=False)\n\n        return primekg_triplets, node_info_dict\n\n    def _build_train_test_split(self) -&gt; tuple[pd.DataFrame, pd.DataFrame,\n                                               pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n        \"\"\"\n        Build the train-test split for BioBridgePrimeKG dataset.\n\n        Returns:\n            The train triplets for BioBridgePrimeKG dataset.\n            The train nodes for BioBridgePrimeKG dataset.\n            The test triplets for BioBridgePrimeKG dataset.\n            The test nodes for BioBridgePrimeKG dataset.\n            The full triplets for BioBridgePrimeKG dataset.\n        \"\"\"\n        if os.path.exists(os.path.join(self.local_dir, \"processed\",\n                                       \"triplet_full_altered.tsv.gz\")):\n            # Load each dataframe in the local directory\n            with open(os.path.join(self.local_dir, \"processed\",\n                                   \"triplet_train.tsv.gz\"), \"rb\") as f:\n                df_train = pd.read_csv(f, sep=\"\\t\", compression=\"gzip\", low_memory=False)\n\n            with open(os.path.join(self.local_dir, \"processed\",\n                                   \"node_train.tsv.gz\"), \"rb\") as f:\n                df_node_train = pd.read_csv(f, sep=\"\\t\", compression=\"gzip\", low_memory=False)\n\n            with open(os.path.join(self.local_dir, \"processed\",\n                                   \"triplet_test.tsv.gz\"), \"rb\") as f:\n                df_test = pd.read_csv(f, sep=\"\\t\", compression=\"gzip\", low_memory=False)\n\n            with open(os.path.join(self.local_dir, \"processed\",\n                                   \"node_test.tsv.gz\"), \"rb\") as f:\n                df_node_test = pd.read_csv(f, sep=\"\\t\", compression=\"gzip\", low_memory=False)\n\n            with open(os.path.join(self.local_dir, \"processed\",\n                                   \"triplet_full_altered.tsv.gz\"), \"rb\") as f:\n                triplets = pd.read_csv(f, sep=\"\\t\", compression=\"gzip\", low_memory=False)\n        else:\n            # Filtering out some nodes in the embedding dictionary\n            triplets = self.primekg_triplets.copy()\n            triplets = triplets[\n                triplets[\"head_index\"].isin(list(self.emb_dict.keys())) &amp;\\\n                triplets[\"tail_index\"].isin(list(self.emb_dict.keys()))\n            ].reset_index(drop=True)\n\n            # Perform splitting of the triplets\n            list_split = {\n                \"train\": [],\n                \"test\": [],\n            }\n            node_split = {\n                \"train\": {\n                    \"node_index\": [],\n                    \"node_type\": [],\n                },\n                \"test\": {\n                    \"node_index\": [],\n                    \"node_type\": [],\n                }\n            }\n            # Loop over the node types\n            for node_type in triplets[\"head_type\"].unique():\n                df_sub = triplets[triplets[\"head_type\"] == node_type]\n                all_x_indexes = df_sub[\"head_index\"].unique()\n                # By default, we use 90% of the nodes for training and 10% for testing\n                te_x_indexes = np.random.choice(\n                    all_x_indexes, size=int(0.1*len(all_x_indexes)), replace=False\n                )\n                df_subs = {}\n                df_subs[\"test\"] = df_sub[df_sub[\"head_index\"].isin(te_x_indexes)]\n                df_subs[\"train\"] = df_sub[~df_sub[\"head_index\"].isin(te_x_indexes)]\n                list_split[\"train\"].append(df_subs[\"train\"])\n                list_split[\"test\"].append(df_subs[\"test\"])\n\n                # record the split\n                node_index = {}\n                node_index[\"train\"] = df_subs[\"train\"][\"head_index\"].unique()\n                node_split[\"train\"][\"node_index\"].extend(node_index[\"train\"].tolist())\n                node_split[\"train\"][\"node_type\"].extend([node_type]*len(node_index[\"train\"]))\n                node_index[\"test\"] = df_subs[\"test\"][\"head_index\"].unique()\n                node_split[\"test\"][\"node_index\"].extend(node_index[\"test\"].tolist())\n                node_split[\"test\"][\"node_type\"].extend([node_type]*len(node_index[\"test\"]))\n\n                print(f\"Number of {node_type} nodes in train: {len(node_index['train'])}\")\n                print(f\"Number of {node_type} nodes in test: {len(node_index['test'])}\")\n\n            # Prepare train and test DataFrames\n            df_train = pd.concat(list_split[\"train\"])\n            df_node_train = pd.DataFrame(node_split[\"train\"])\n            df_test = pd.concat(list_split[\"test\"])\n            df_node_test = pd.DataFrame(node_split[\"test\"])\n\n            # Store each dataframe in the local directory\n            df_train.to_csv(os.path.join(self.local_dir, \"processed\", \"triplet_train.tsv.gz\"),\n                            sep=\"\\t\", compression=\"gzip\", index=False)\n            df_node_train.to_csv(os.path.join(self.local_dir, \"processed\", \"node_train.tsv.gz\"),\n                                sep=\"\\t\", compression=\"gzip\", index=False)\n            df_test.to_csv(os.path.join(self.local_dir, \"processed\", \"triplet_test.tsv.gz\"),\n                           sep=\"\\t\", compression=\"gzip\", index=False)\n            df_node_test.to_csv(os.path.join(self.local_dir, \"processed\", \"node_test.tsv.gz\"),\n                                sep=\"\\t\", compression=\"gzip\", index=False)\n            # Store altered full triplets as well\n            triplets.to_csv(os.path.join(self.local_dir, \"processed\",\n                                         \"triplet_full_altered.tsv.gz\"),\n                            sep=\"\\t\", compression=\"gzip\", index=False)\n\n        return df_train, df_node_train, df_test, df_node_test, triplets\n\n    # def _negative_sampling(self,\n    #                        batch_df: pd.DataFrame,\n    #                        process_index: int,\n    #                        index_map: dict,\n    #                        node_train_dict: dict) -&gt; pd.DataFrame:\n    #     \"\"\"\n    #     A helper function to perform negative sampling for a batch of triplets.\n    #     \"\"\"\n    #     negative_y_index_list = []\n    #     for _, row in tqdm(batch_df.iterrows(),\n    #                        total=batch_df.shape[0],\n    #                        desc=f\"Process {process_index}\"):\n    #         x_index = row['head_index']\n    #         # y_index = row['y_index']\n    #         y_index_type = row['tail_type']\n    #         paired_y_index_list = index_map[x_index]\n\n    #         # sample a list of negative y_index\n    #         node_train_sub = node_train_dict[y_index_type]\n    #         negative_y_index = node_train_sub[\n    #             ~node_train_sub['node_index'].isin(paired_y_index_list)\n    #         ]['node_index'].sample(self.n_neg_samples).tolist()\n    #         negative_y_index_list.append(negative_y_index)\n\n    #     batch_df.loc[:, 'negative_tail_index'] = negative_y_index_list\n    #     return batch_df\n\n    # def _build_negative_triplets(self,\n    #                              chunk_size: int=100000,\n    #                              n_neg_samples: int=10):\n    #     \"\"\"\n    #     Build the negative triplets for BioBridgePrimeKG dataset.\n    #     \"\"\"\n    #     processed_file_path = os.path.join(self.local_dir,\n    #                                        \"processed\",\n    #                                        \"triplet_train_negative.tsv.gz\")\n    #     if os.path.exists(processed_file_path):\n    #         # Load the negative triplets from the local directory\n    #         with open(processed_file_path, \"rb\") as f:\n    #             triplets_negative = pd.read_csv(f, sep=\"\\t\", compression=\"gzip\", low_memory=False)\n    #     else:\n    #         # Set the number samples for negative sampling\n    #         self.n_neg_samples = n_neg_samples\n\n    #         # Split node list by type\n    #         node_train_dict = {}\n    #         type_list = self.df_node_train['node_type'].unique()\n    #         for node_type in type_list:\n    #             node_train_dict[node_type] = self.df_node_train[\n    #                 self.df_node_train['node_type'] == node_type\n    #             ].reset_index(drop=True)\n\n    #         # create an index mapping from x_index to y_index\n    #         index_map = self.df_train[\n    #             ['head_index', 'tail_index']\n    #         ].drop_duplicates().groupby('head_index').agg(list).to_dict()['tail_index']\n\n    #         # Negative sampling\n    #         batch_df_list = []\n    #         for i in tqdm(range(0, self.df_train.shape[0], chunk_size)):\n    #             batch_df_list.append(self.df_train.iloc[i:i+chunk_size])\n    #         # Process negative sampling\n    #         results = [\n    #             self._negative_sampling(batch_df,\n    #                                     num_piece,\n    #                                     index_map,\n    #                                     node_train_dict)\n    #                                     for num_piece, batch_df in enumerate(batch_df_list)\n    #         ]\n\n    #         # Store the negative triplets\n    #         triplets_negative = pd.concat(results, axis=0)\n    #         triplets_negative.to_csv(processed_file_path,\n    #                                  sep=\"\\t\", compression=\"gzip\", index=False)\n\n    #     # Set attribute\n    #     self.primekg_triplets_negative = triplets_negative\n\n    #     return triplets_negative\n\n    # def load_data(self,\n    #               build_neg_triplest: bool= False,\n    #               chunk_size: int=100000,\n    #               n_neg_samples: int=10):\n\n    def load_data(self):\n        \"\"\"\n        Load the BioBridgePrimeKG dataset into pandas DataFrame of nodes and edges.\n\n        Args:\n            build_neg_triplest (bool): Whether to build negative triplets.\n            chunk_size (int): The chunk size for negative sampling.\n            n_neg_samples (int): The number of negative samples for negative sampling.\n        \"\"\"\n        # Load PrimeKG dataset\n        print(\"Loading PrimeKG dataset...\")\n        self.primekg = self._load_primekg()\n\n        # Load data config file of BioBridgePrimeKG\n        print(\"Loading data config file of BioBridgePrimeKG...\")\n        self.data_config = self._load_data_config()\n\n        # Build node embeddings\n        print(\"Building node embeddings...\")\n        self.emb_dict = self._build_node_embeddings()\n\n        # Build full triplets\n        print(\"Building full triplets...\")\n        self.primekg_triplets, self.node_info_dict = self._build_full_triplets()\n\n        # Build train-test split\n        print(\"Building train-test split...\")\n        self.df_train, self.df_node_train, self.df_test, self.df_node_test, self.primekg_triplets =\\\n        self._build_train_test_split()\n\n        # if build_neg_triplest:\n        #     # Build negative triplets\n        #     print(\"Building negative triplets...\")\n        #     self.primekg_triplets_negative = self._build_negative_triplets(\n        #         chunk_size=chunk_size,\n        #         n_neg_samples=n_neg_samples\n        #     )\n\n    def set_random_seed(self, seed: int):\n        \"\"\"\n        Set the random seed for reproducibility.\n\n        Args:\n            seed (int): The random seed value.\n        \"\"\"\n        np.random.seed(seed)\n\n    def get_primekg(self) -&gt; PrimeKG:\n        \"\"\"\n        Get the PrimeKG dataset.\n\n        Returns:\n            The PrimeKG dataset.\n        \"\"\"\n        return self.primekg\n\n    def get_data_config(self) -&gt; dict:\n        \"\"\"\n        Get the data config file of BioBridgePrimeKG dataset.\n\n        Returns:\n            The data config file of BioBridgePrimeKG dataset.\n        \"\"\"\n        return self.data_config\n\n    def get_node_embeddings(self) -&gt; dict:\n        \"\"\"\n        Get the node embeddings for BioBridgePrimeKG dataset.\n\n        Returns:\n            The dictionary of node embeddings.\n        \"\"\"\n        return self.emb_dict\n\n    def get_primekg_triplets(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Get the full triplets for BioBridgePrimeKG dataset.\n\n        Returns:\n            The full triplets for BioBridgePrimeKG dataset.\n        \"\"\"\n        return self.primekg_triplets\n\n    # def get_primekg_triplets_negative(self) -&gt; pd.DataFrame:\n    #     \"\"\"\n    #     Get the negative triplets for BioBridgePrimeKG dataset.\n\n    #     Returns:\n    #         The negative triplets for BioBridgePrimeKG dataset.\n    #     \"\"\"\n    #     return self.primekg_triplets_negative\n\n    def get_train_test_split(self) -&gt; dict:\n        \"\"\"\n        Get the train-test split for BioBridgePrimeKG dataset.\n\n        Returns:\n            The train-test split for BioBridgePrimeKG dataset.\n        \"\"\"\n        return {\n            \"train\": self.df_train,\n            \"node_train\": self.df_node_train,\n            \"test\": self.df_test,\n            \"node_test\": self.df_node_test\n        }\n\n    def get_node_info_dict(self) -&gt; dict:\n        \"\"\"\n        Get the node information dictionary for BioBridgePrimeKG dataset.\n\n        Returns:\n            The node information dictionary for BioBridgePrimeKG dataset.\n        \"\"\"\n        return self.node_info_dict\n</code></pre>"},{"location":"data/biobridge_primekg/#vpeleaderboard.data.src.kg.biobridge_primekg.BioBridgePrimeKG.__init__","title":"<code>__init__(cfg)</code>","text":"<p>Constructor for BioBridgePrimeKG class.</p> <p>Parameters:</p> Name Type Description Default <code>primekg_dir</code> <code>str</code> <p>The directory of PrimeKG dataset.</p> required <code>local_dir</code> <code>str</code> <p>The directory to store the downloaded data.</p> required <code>random_seed</code> <code>int</code> <p>The random seed value.</p> required Source code in <code>vpeleaderboard/data/src/kg/biobridge_primekg.py</code> <pre><code>def __init__(self, cfg: DictConfig):\n    \"\"\"\n    Constructor for BioBridgePrimeKG class.\n\n    Args:\n        primekg_dir (str): The directory of PrimeKG dataset.\n        local_dir (str): The directory to store the downloaded data.\n        random_seed (int): The random seed value.\n    \"\"\"\n    self.name: str = \"biobridge_primekg\"\n    # self.cfg = cfg\n    self.primekg_dir: str = cfg.data.primekg_dir\n    self.local_dir: str = cfg.data.biobridge_dir\n    self.random_seed: int = cfg.data.random_seed if \"random_seed\" in cfg.data else 0\n    self.n_neg_samples: int = cfg.data.n_neg_samples if \"n_neg_samples\" in cfg.data else 5\n    # Preselected node types:\n    # protein, molecular function, cellular component, biological process, drug, disease\n    self.preselected_node_types = [\"protein\", \"mf\", \"cc\", \"bp\", \"drug\", \"disease\"]\n    self.node_type_map = {\n        \"protein\": \"gene/protein\",\n        \"mf\": \"molecular_function\",\n        \"cc\": \"cellular_component\",\n        \"bp\": \"biological_process\",\n        \"drug\": \"drug\",\n        \"disease\": \"disease\",\n    }\n\n    # Attributes to store the data\n    self.primekg = None\n    self.primekg_triplets = None\n    # self.primekg_triplets_negative = None\n    self.data_config = None\n    self.emb_dict = None\n    self.df_train = None\n    self.df_node_train = None\n    self.df_test = None\n    self.df_node_test = None\n    self.node_info_dict = None\n\n    # Set up the dataset\n    self.setup()\n</code></pre>"},{"location":"data/biobridge_primekg/#vpeleaderboard.data.src.kg.biobridge_primekg.BioBridgePrimeKG._build_full_triplets","title":"<code>_build_full_triplets()</code>","text":"<p>Build the full triplets for BioBridgePrimeKG dataset.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The full triplets for BioBridgePrimeKG dataset.</p> <code>dict</code> <p>The dictionary of node information.</p> Source code in <code>vpeleaderboard/data/src/kg/biobridge_primekg.py</code> <pre><code>def _build_full_triplets(self) -&gt; tuple[pd.DataFrame, dict]:\n    \"\"\"\n    Build the full triplets for BioBridgePrimeKG dataset.\n\n    Returns:\n        The full triplets for BioBridgePrimeKG dataset.\n        The dictionary of node information.\n    \"\"\"\n    processed_file_path = os.path.join(self.local_dir, \"processed\", \"triplet_full.tsv.gz\")\n    if os.path.exists(processed_file_path):\n        # Load the file from the local directory\n        with open(processed_file_path, \"rb\") as f:\n            primekg_triplets = pd.read_csv(f, sep=\"\\t\", compression=\"gzip\", low_memory=False)\n\n        # Load each dataframe in the local directory\n        node_info_dict = {}\n        for i, node_type in enumerate(self.preselected_node_types):\n            with open(os.path.join(self.local_dir, \"processed\",\n                                   f\"{node_type}.csv\"), \"rb\") as f:\n                df_node = pd.read_csv(f)\n            node_info_dict[self.node_type_map[node_type]] = df_node\n    else:\n        # Download the related files from the BioBridge repo and further process them\n        # List of processed files\n        url = ('https://media.githubusercontent.com/media/RyanWangZf/BioBridge/'\n               'refs/heads/main/data/Processed/')\n        file_list = [\"protein\", \"molecular\", \"cellular\", \"biological\", \"drug\", \"disease\"]\n\n        # Download the processed files\n        for i, file in enumerate(file_list):\n            self._download_file(remote_url=os.path.join(url, f\"{file}.csv\"),\n                                local_dir=os.path.join(self.local_dir, \"processed\"),\n                                local_filename=f\"{self.preselected_node_types[i]}.csv\")\n\n        # Build the node index list\n        node_info_dict = {}\n        node_index_list = []\n        for i, file in enumerate(file_list):\n            df_node = pd.read_csv(os.path.join(self.local_dir, \"processed\",\n                                               f\"{self.preselected_node_types[i]}.csv\"))\n            node_info_dict[self.node_type_map[self.preselected_node_types[i]]] = df_node\n            node_index_list.extend(df_node[\"node_index\"].tolist())\n\n        # Filter the PrimeKG dataset to take into account only the selected node types\n        primekg_triplets = self.primekg.get_edges().copy()\n        primekg_triplets = primekg_triplets[\n            primekg_triplets[\"head_index\"].isin(node_index_list) &amp;\\\n            primekg_triplets[\"tail_index\"].isin(node_index_list)\n        ]\n        primekg_triplets = primekg_triplets.reset_index(drop=True)\n\n        # Perform mapping of node types\n        primekg_triplets[\"head_type\"] = primekg_triplets[\"head_type\"].apply(\n            lambda x: self.data_config[\"node_type\"][x]\n        )\n        primekg_triplets[\"tail_type\"] = primekg_triplets[\"tail_type\"].apply(\n            lambda x: self.data_config[\"node_type\"][x]\n        )\n\n        # Perform mapping of relation types\n        primekg_triplets[\"display_relation\"] = primekg_triplets[\"display_relation\"].apply(\n            lambda x: self.data_config[\"relation_type\"][x]\n        )\n\n        # Store the processed triplets\n        primekg_triplets.to_csv(processed_file_path, sep=\"\\t\", compression=\"gzip\", index=False)\n\n    return primekg_triplets, node_info_dict\n</code></pre>"},{"location":"data/biobridge_primekg/#vpeleaderboard.data.src.kg.biobridge_primekg.BioBridgePrimeKG._build_node_embeddings","title":"<code>_build_node_embeddings()</code>","text":"<p>Build the node embeddings for BioBridgePrimeKG dataset.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The dictionary of node embeddings.</p> Source code in <code>vpeleaderboard/data/src/kg/biobridge_primekg.py</code> <pre><code>def _build_node_embeddings(self) -&gt; dict:\n    \"\"\"\n    Build the node embeddings for BioBridgePrimeKG dataset.\n\n    Returns:\n        The dictionary of node embeddings.\n    \"\"\"\n    processed_file_path = os.path.join(self.local_dir, \"embeddings\", \"embedding_dict.pkl\")\n    if os.path.exists(processed_file_path):\n        # Load the embeddings from the local directory\n        with open(processed_file_path, \"rb\") as f:\n            emb_dict_all = pickle.load(f)\n    else:\n        # Download the embeddings from the BioBridge repo and further process them\n        # List of embedding source files\n        url = ('https://media.githubusercontent.com/media/RyanWangZf/BioBridge/'\n               'refs/heads/main/data/embeddings/esm2b_unimo_pubmedbert/')\n        file_list = [f\"{n}.pkl\" for n in self.preselected_node_types]\n\n        # Download the embeddings\n        for file in file_list:\n            self._download_file(remote_url=os.path.join(url, file),\n                                local_dir=os.path.join(self.local_dir, \"embeddings\"),\n                                local_filename=file)\n\n        # Unified embeddings\n        emb_dict_all = {}\n        for file in file_list:\n            with open(os.path.join(self.local_dir, \"embeddings\", file), \"rb\") as f:\n                emb = pickle.load(f)\n            emb_ar = emb[\"embedding\"]\n            if not isinstance(emb_ar, list):\n                emb_ar = emb_ar.tolist()\n            emb_dict_all.update(dict(zip(emb[\"node_index\"], emb_ar)))\n\n        # Store embeddings\n        with open(processed_file_path, \"wb\") as f:\n            pickle.dump(emb_dict_all, f)\n\n    return emb_dict_all\n</code></pre>"},{"location":"data/biobridge_primekg/#vpeleaderboard.data.src.kg.biobridge_primekg.BioBridgePrimeKG._build_train_test_split","title":"<code>_build_train_test_split()</code>","text":"<p>Build the train-test split for BioBridgePrimeKG dataset.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The train triplets for BioBridgePrimeKG dataset.</p> <code>DataFrame</code> <p>The train nodes for BioBridgePrimeKG dataset.</p> <code>DataFrame</code> <p>The test triplets for BioBridgePrimeKG dataset.</p> <code>DataFrame</code> <p>The test nodes for BioBridgePrimeKG dataset.</p> <code>DataFrame</code> <p>The full triplets for BioBridgePrimeKG dataset.</p> Source code in <code>vpeleaderboard/data/src/kg/biobridge_primekg.py</code> <pre><code>def _build_train_test_split(self) -&gt; tuple[pd.DataFrame, pd.DataFrame,\n                                           pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Build the train-test split for BioBridgePrimeKG dataset.\n\n    Returns:\n        The train triplets for BioBridgePrimeKG dataset.\n        The train nodes for BioBridgePrimeKG dataset.\n        The test triplets for BioBridgePrimeKG dataset.\n        The test nodes for BioBridgePrimeKG dataset.\n        The full triplets for BioBridgePrimeKG dataset.\n    \"\"\"\n    if os.path.exists(os.path.join(self.local_dir, \"processed\",\n                                   \"triplet_full_altered.tsv.gz\")):\n        # Load each dataframe in the local directory\n        with open(os.path.join(self.local_dir, \"processed\",\n                               \"triplet_train.tsv.gz\"), \"rb\") as f:\n            df_train = pd.read_csv(f, sep=\"\\t\", compression=\"gzip\", low_memory=False)\n\n        with open(os.path.join(self.local_dir, \"processed\",\n                               \"node_train.tsv.gz\"), \"rb\") as f:\n            df_node_train = pd.read_csv(f, sep=\"\\t\", compression=\"gzip\", low_memory=False)\n\n        with open(os.path.join(self.local_dir, \"processed\",\n                               \"triplet_test.tsv.gz\"), \"rb\") as f:\n            df_test = pd.read_csv(f, sep=\"\\t\", compression=\"gzip\", low_memory=False)\n\n        with open(os.path.join(self.local_dir, \"processed\",\n                               \"node_test.tsv.gz\"), \"rb\") as f:\n            df_node_test = pd.read_csv(f, sep=\"\\t\", compression=\"gzip\", low_memory=False)\n\n        with open(os.path.join(self.local_dir, \"processed\",\n                               \"triplet_full_altered.tsv.gz\"), \"rb\") as f:\n            triplets = pd.read_csv(f, sep=\"\\t\", compression=\"gzip\", low_memory=False)\n    else:\n        # Filtering out some nodes in the embedding dictionary\n        triplets = self.primekg_triplets.copy()\n        triplets = triplets[\n            triplets[\"head_index\"].isin(list(self.emb_dict.keys())) &amp;\\\n            triplets[\"tail_index\"].isin(list(self.emb_dict.keys()))\n        ].reset_index(drop=True)\n\n        # Perform splitting of the triplets\n        list_split = {\n            \"train\": [],\n            \"test\": [],\n        }\n        node_split = {\n            \"train\": {\n                \"node_index\": [],\n                \"node_type\": [],\n            },\n            \"test\": {\n                \"node_index\": [],\n                \"node_type\": [],\n            }\n        }\n        # Loop over the node types\n        for node_type in triplets[\"head_type\"].unique():\n            df_sub = triplets[triplets[\"head_type\"] == node_type]\n            all_x_indexes = df_sub[\"head_index\"].unique()\n            # By default, we use 90% of the nodes for training and 10% for testing\n            te_x_indexes = np.random.choice(\n                all_x_indexes, size=int(0.1*len(all_x_indexes)), replace=False\n            )\n            df_subs = {}\n            df_subs[\"test\"] = df_sub[df_sub[\"head_index\"].isin(te_x_indexes)]\n            df_subs[\"train\"] = df_sub[~df_sub[\"head_index\"].isin(te_x_indexes)]\n            list_split[\"train\"].append(df_subs[\"train\"])\n            list_split[\"test\"].append(df_subs[\"test\"])\n\n            # record the split\n            node_index = {}\n            node_index[\"train\"] = df_subs[\"train\"][\"head_index\"].unique()\n            node_split[\"train\"][\"node_index\"].extend(node_index[\"train\"].tolist())\n            node_split[\"train\"][\"node_type\"].extend([node_type]*len(node_index[\"train\"]))\n            node_index[\"test\"] = df_subs[\"test\"][\"head_index\"].unique()\n            node_split[\"test\"][\"node_index\"].extend(node_index[\"test\"].tolist())\n            node_split[\"test\"][\"node_type\"].extend([node_type]*len(node_index[\"test\"]))\n\n            print(f\"Number of {node_type} nodes in train: {len(node_index['train'])}\")\n            print(f\"Number of {node_type} nodes in test: {len(node_index['test'])}\")\n\n        # Prepare train and test DataFrames\n        df_train = pd.concat(list_split[\"train\"])\n        df_node_train = pd.DataFrame(node_split[\"train\"])\n        df_test = pd.concat(list_split[\"test\"])\n        df_node_test = pd.DataFrame(node_split[\"test\"])\n\n        # Store each dataframe in the local directory\n        df_train.to_csv(os.path.join(self.local_dir, \"processed\", \"triplet_train.tsv.gz\"),\n                        sep=\"\\t\", compression=\"gzip\", index=False)\n        df_node_train.to_csv(os.path.join(self.local_dir, \"processed\", \"node_train.tsv.gz\"),\n                            sep=\"\\t\", compression=\"gzip\", index=False)\n        df_test.to_csv(os.path.join(self.local_dir, \"processed\", \"triplet_test.tsv.gz\"),\n                       sep=\"\\t\", compression=\"gzip\", index=False)\n        df_node_test.to_csv(os.path.join(self.local_dir, \"processed\", \"node_test.tsv.gz\"),\n                            sep=\"\\t\", compression=\"gzip\", index=False)\n        # Store altered full triplets as well\n        triplets.to_csv(os.path.join(self.local_dir, \"processed\",\n                                     \"triplet_full_altered.tsv.gz\"),\n                        sep=\"\\t\", compression=\"gzip\", index=False)\n\n    return df_train, df_node_train, df_test, df_node_test, triplets\n</code></pre>"},{"location":"data/biobridge_primekg/#vpeleaderboard.data.src.kg.biobridge_primekg.BioBridgePrimeKG._download_file","title":"<code>_download_file(remote_url, local_dir, local_filename)</code>","text":"<p>A helper function to download a file from remote URL to the local directory.</p> <p>Parameters:</p> Name Type Description Default <code>remote_url</code> <code>str</code> <p>The remote URL of the file to be downloaded.</p> required <code>local_dir</code> <code>str</code> <p>The local directory to store the downloaded file.</p> required <code>local_filename</code> <code>str</code> <p>The local filename to store the downloaded file.</p> required Source code in <code>vpeleaderboard/data/src/kg/biobridge_primekg.py</code> <pre><code>def _download_file(self,\n                   remote_url:str,\n                   local_dir: str,\n                   local_filename: str):\n    \"\"\"\n    A helper function to download a file from remote URL to the local directory.\n\n    Args:\n        remote_url (str): The remote URL of the file to be downloaded.\n        local_dir (str): The local directory to store the downloaded file.\n        local_filename (str): The local filename to store the downloaded file.\n    \"\"\"\n    # Make the local directory if it does not exist\n    if not os.path.exists(local_dir):\n        os.makedirs(local_dir)\n    # Download the file from remote URL to local directory\n    local_path = os.path.join(local_dir, local_filename)\n    if os.path.exists(local_path):\n        print(f\"File {local_filename} already exists in {local_dir}.\")\n    else:\n        print(f\"Downloading {local_filename} from {remote_url} to {local_dir}...\")\n        response = requests.get(remote_url, stream=True, timeout=300)\n        response.raise_for_status()\n        progress_bar = tqdm(\n            total=int(response.headers.get(\"content-length\", 0)),\n            unit=\"iB\",\n            unit_scale=True,\n        )\n        with open(os.path.join(local_dir, local_filename), \"wb\") as file:\n            for data in response.iter_content(1024):\n                progress_bar.update(len(data))\n                file.write(data)\n        progress_bar.close()\n</code></pre>"},{"location":"data/biobridge_primekg/#vpeleaderboard.data.src.kg.biobridge_primekg.BioBridgePrimeKG._load_data_config","title":"<code>_load_data_config()</code>","text":"<p>Load the data config file of BioBridgePrimeKG dataset.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The data config file of BioBridgePrimeKG dataset.</p> Source code in <code>vpeleaderboard/data/src/kg/biobridge_primekg.py</code> <pre><code>def _load_data_config(self) -&gt; dict:\n    \"\"\"\n    Load the data config file of BioBridgePrimeKG dataset.\n\n    Returns:\n        The data config file of BioBridgePrimeKG dataset.\n    \"\"\"\n    # Download the data config file of BioBridgePrimeKG\n    self._download_file(\n        remote_url= ('https://raw.githubusercontent.com/RyanWangZf/BioBridge/'\n                     'refs/heads/main/data/BindData/data_config.json'),\n        local_dir=self.local_dir,\n        local_filename='data_config.json')\n\n    # Load the downloaded data config file\n    with open(os.path.join(self.local_dir, 'data_config.json'), 'r', encoding='utf-8') as f:\n        data_config = json.load(f)\n\n    return data_config\n</code></pre>"},{"location":"data/biobridge_primekg/#vpeleaderboard.data.src.kg.biobridge_primekg.BioBridgePrimeKG._load_primekg","title":"<code>_load_primekg()</code>","text":"<p>Private method to load related files of PrimeKG dataset.</p> <p>Returns:</p> Type Description <code>PrimeKG</code> <p>The PrimeKG dataset.</p> Source code in <code>vpeleaderboard/data/src/kg/biobridge_primekg.py</code> <pre><code>def _load_primekg(self) -&gt; PrimeKG:\n    \"\"\"\n    Private method to load related files of PrimeKG dataset.\n\n    Returns:\n        The PrimeKG dataset.\n    \"\"\"\n    primekg_data = PrimeKG(self.primekg_dir)\n    primekg_data.load_data()\n\n    return primekg_data\n</code></pre>"},{"location":"data/biobridge_primekg/#vpeleaderboard.data.src.kg.biobridge_primekg.BioBridgePrimeKG.get_data_config","title":"<code>get_data_config()</code>","text":"<p>Get the data config file of BioBridgePrimeKG dataset.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The data config file of BioBridgePrimeKG dataset.</p> Source code in <code>vpeleaderboard/data/src/kg/biobridge_primekg.py</code> <pre><code>def get_data_config(self) -&gt; dict:\n    \"\"\"\n    Get the data config file of BioBridgePrimeKG dataset.\n\n    Returns:\n        The data config file of BioBridgePrimeKG dataset.\n    \"\"\"\n    return self.data_config\n</code></pre>"},{"location":"data/biobridge_primekg/#vpeleaderboard.data.src.kg.biobridge_primekg.BioBridgePrimeKG.get_node_embeddings","title":"<code>get_node_embeddings()</code>","text":"<p>Get the node embeddings for BioBridgePrimeKG dataset.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The dictionary of node embeddings.</p> Source code in <code>vpeleaderboard/data/src/kg/biobridge_primekg.py</code> <pre><code>def get_node_embeddings(self) -&gt; dict:\n    \"\"\"\n    Get the node embeddings for BioBridgePrimeKG dataset.\n\n    Returns:\n        The dictionary of node embeddings.\n    \"\"\"\n    return self.emb_dict\n</code></pre>"},{"location":"data/biobridge_primekg/#vpeleaderboard.data.src.kg.biobridge_primekg.BioBridgePrimeKG.get_node_info_dict","title":"<code>get_node_info_dict()</code>","text":"<p>Get the node information dictionary for BioBridgePrimeKG dataset.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The node information dictionary for BioBridgePrimeKG dataset.</p> Source code in <code>vpeleaderboard/data/src/kg/biobridge_primekg.py</code> <pre><code>def get_node_info_dict(self) -&gt; dict:\n    \"\"\"\n    Get the node information dictionary for BioBridgePrimeKG dataset.\n\n    Returns:\n        The node information dictionary for BioBridgePrimeKG dataset.\n    \"\"\"\n    return self.node_info_dict\n</code></pre>"},{"location":"data/biobridge_primekg/#vpeleaderboard.data.src.kg.biobridge_primekg.BioBridgePrimeKG.get_primekg","title":"<code>get_primekg()</code>","text":"<p>Get the PrimeKG dataset.</p> <p>Returns:</p> Type Description <code>PrimeKG</code> <p>The PrimeKG dataset.</p> Source code in <code>vpeleaderboard/data/src/kg/biobridge_primekg.py</code> <pre><code>def get_primekg(self) -&gt; PrimeKG:\n    \"\"\"\n    Get the PrimeKG dataset.\n\n    Returns:\n        The PrimeKG dataset.\n    \"\"\"\n    return self.primekg\n</code></pre>"},{"location":"data/biobridge_primekg/#vpeleaderboard.data.src.kg.biobridge_primekg.BioBridgePrimeKG.get_primekg_triplets","title":"<code>get_primekg_triplets()</code>","text":"<p>Get the full triplets for BioBridgePrimeKG dataset.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The full triplets for BioBridgePrimeKG dataset.</p> Source code in <code>vpeleaderboard/data/src/kg/biobridge_primekg.py</code> <pre><code>def get_primekg_triplets(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Get the full triplets for BioBridgePrimeKG dataset.\n\n    Returns:\n        The full triplets for BioBridgePrimeKG dataset.\n    \"\"\"\n    return self.primekg_triplets\n</code></pre>"},{"location":"data/biobridge_primekg/#vpeleaderboard.data.src.kg.biobridge_primekg.BioBridgePrimeKG.get_train_test_split","title":"<code>get_train_test_split()</code>","text":"<p>Get the train-test split for BioBridgePrimeKG dataset.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The train-test split for BioBridgePrimeKG dataset.</p> Source code in <code>vpeleaderboard/data/src/kg/biobridge_primekg.py</code> <pre><code>def get_train_test_split(self) -&gt; dict:\n    \"\"\"\n    Get the train-test split for BioBridgePrimeKG dataset.\n\n    Returns:\n        The train-test split for BioBridgePrimeKG dataset.\n    \"\"\"\n    return {\n        \"train\": self.df_train,\n        \"node_train\": self.df_node_train,\n        \"test\": self.df_test,\n        \"node_test\": self.df_node_test\n    }\n</code></pre>"},{"location":"data/biobridge_primekg/#vpeleaderboard.data.src.kg.biobridge_primekg.BioBridgePrimeKG.load_data","title":"<code>load_data()</code>","text":"<p>Load the BioBridgePrimeKG dataset into pandas DataFrame of nodes and edges.</p> <p>Parameters:</p> Name Type Description Default <code>build_neg_triplest</code> <code>bool</code> <p>Whether to build negative triplets.</p> required <code>chunk_size</code> <code>int</code> <p>The chunk size for negative sampling.</p> required <code>n_neg_samples</code> <code>int</code> <p>The number of negative samples for negative sampling.</p> required Source code in <code>vpeleaderboard/data/src/kg/biobridge_primekg.py</code> <pre><code>def load_data(self):\n    \"\"\"\n    Load the BioBridgePrimeKG dataset into pandas DataFrame of nodes and edges.\n\n    Args:\n        build_neg_triplest (bool): Whether to build negative triplets.\n        chunk_size (int): The chunk size for negative sampling.\n        n_neg_samples (int): The number of negative samples for negative sampling.\n    \"\"\"\n    # Load PrimeKG dataset\n    print(\"Loading PrimeKG dataset...\")\n    self.primekg = self._load_primekg()\n\n    # Load data config file of BioBridgePrimeKG\n    print(\"Loading data config file of BioBridgePrimeKG...\")\n    self.data_config = self._load_data_config()\n\n    # Build node embeddings\n    print(\"Building node embeddings...\")\n    self.emb_dict = self._build_node_embeddings()\n\n    # Build full triplets\n    print(\"Building full triplets...\")\n    self.primekg_triplets, self.node_info_dict = self._build_full_triplets()\n\n    # Build train-test split\n    print(\"Building train-test split...\")\n    self.df_train, self.df_node_train, self.df_test, self.df_node_test, self.primekg_triplets =\\\n    self._build_train_test_split()\n</code></pre>"},{"location":"data/biobridge_primekg/#vpeleaderboard.data.src.kg.biobridge_primekg.BioBridgePrimeKG.set_random_seed","title":"<code>set_random_seed(seed)</code>","text":"<p>Set the random seed for reproducibility.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>The random seed value.</p> required Source code in <code>vpeleaderboard/data/src/kg/biobridge_primekg.py</code> <pre><code>def set_random_seed(self, seed: int):\n    \"\"\"\n    Set the random seed for reproducibility.\n\n    Args:\n        seed (int): The random seed value.\n    \"\"\"\n    np.random.seed(seed)\n</code></pre>"},{"location":"data/biobridge_primekg/#vpeleaderboard.data.src.kg.biobridge_primekg.BioBridgePrimeKG.setup","title":"<code>setup()</code>","text":"<p>A method to set up the dataset.</p> Source code in <code>vpeleaderboard/data/src/kg/biobridge_primekg.py</code> <pre><code>def setup(self):\n    \"\"\"\n    A method to set up the dataset.\n    \"\"\"\n    # Make the directories if it doesn't exist\n    # os.makedirs(os.path.dirname(self.primekg_dir), exist_ok=True)\n    os.makedirs(os.path.dirname(self.local_dir), exist_ok=True)\n\n    # Set the random seed\n    self.set_random_seed(self.random_seed)\n\n    # Set SettingWithCopyWarning  warnings to none\n    pd.options.mode.chained_assignment = None\n</code></pre>"},{"location":"data/index_kg/","title":"KG Data","text":"Knowledge Graph Data <p>This page was last updated on 2025-06-09 09:46:11 UTC</p>"},{"location":"data/index_kg/#algorithm_metrics","title":"KG Models","text":"Abstract model_name Number of Edges Number of Nodes visibility_off BioBridge 3904610 84981                BioBridge is the official implementation of the paper \" Bridging Biomedical Foundation Models via Knowledge Graphs (ICLR 2024)\". It leverages knowledge graphs to connect independently trained biomedical foundation models, enabling multimodal learning without fine-tuning. The repository includes trained model checkpoints, raw and processed data, embeddings from unimodal models like PubMedBERT and UniMol, and example scripts for training and evaluation. BioBridge supports applications such as cross-modality prediction, protein prediction, and molecule generation."},{"location":"data/kg_dataloader/","title":"Biobridge Dataloader","text":"<p>Loads the BioBridge dataset and prepares it for training and evaluation using LightningDataModule from PyTorch Lightning, with optional caching.</p>"},{"location":"data/kg_dataloader/#vpeleaderboard.data.src.kg.biobridge_datamodule_hetero.BioBridgeDataModule","title":"<code>BioBridgeDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>LightningDataModule for the BioBridge dataset.</p> Source code in <code>vpeleaderboard/data/src/kg/biobridge_datamodule_hetero.py</code> <pre><code>class BioBridgeDataModule(LightningDataModule):\n    \"\"\"\n    LightningDataModule for the BioBridge dataset.\n    \"\"\"\n    def __init__(self, cfg: DictConfig) -&gt; None:\n        \"\"\"\n        Initializes the BioBridgeDataModule.\n\n        Args:\n            cfg (DictConfig): Configuration object with dataset parameters.\n        \"\"\"\n        super().__init__()\n        self.save_hyperparameters(logger=False)\n        self.cfg = cfg\n        self.primekg_dir = cfg.data.primekg_dir\n        self.biobridge_dir = cfg.data.biobridge_dir\n        self.batch_size = cfg.data.batch_size\n        self.cache_path = cfg.data.cache_path\n        self.biobridge = None\n        self.mapper = {}\n        self.data = {}\n\n    def _load_biobridge_data(self) -&gt; None:\n        \"\"\"\n        Private method to load related files of PrimeKG dataset.\n\n        Returns:\n            None\n        \"\"\"\n        self.biobridge = BioBridgePrimeKG(self.cfg)\n        self.biobridge.load_data()\n\n        self.data['nt2ntid'] = self.biobridge.get_data_config()[\"node_type\"]\n        self.data['ntid2nt'] = {v: k for k, v in self.data['nt2ntid'].items()}\n\n    def _filter_triplets(self):\n        \"\"\"\n        Private method to filter valid triplets from the PrimeKG edges.\n\n        Returns:\n            triplets (pd.DataFrame): Filtered triplet data.\n        \"\"\"\n        node_index_list = []\n        for node_type in self.biobridge.preselected_node_types:\n            df_node = pd.read_csv(os.path.join(self.biobridge.local_dir,\n                                               \"processed\", f\"{node_type}.csv\"))\n            node_index_list.extend(df_node[\"node_index\"].tolist())\n\n        triplets = self.biobridge.primekg.get_edges().copy()\n        triplets = triplets[\n            triplets[\"head_index\"].isin(node_index_list) &amp;\n            triplets[\"tail_index\"].isin(node_index_list)\n        ].reset_index(drop=True)\n\n        triplets = triplets[\n            triplets[\"head_index\"].isin(self.biobridge.emb_dict) &amp;\n            triplets[\"tail_index\"].isin(self.biobridge.emb_dict)\n        ].reset_index(drop=True)\n        return triplets\n\n    def _add_node_features(self, nodes: pd.DataFrame, node_types: np.ndarray) -&gt; None:\n        \"\"\"\n        Private method to add node features to HeteroData.\n\n        Args:\n            nodes (pd.DataFrame): DataFrame containing node metadata.\n            node_types (np.ndarray): Array of unique node types.\n\n        Returns:\n            None\n        \"\"\"\n        self.data[\"init\"] = HeteroData()\n\n        for nt in node_types:\n            self.mapper[nt] = {}\n            nt_nodes = nodes[nodes['node_type'] == nt].reset_index(drop=True)\n\n            self.mapper[nt]['to_nidx'] = nt_nodes[\"node_index\"].to_dict()\n            self.mapper[nt]['from_nidx'] = {v: k for k, v in self.mapper[nt]['to_nidx'].items()}\n\n            self.data[\"init\"][nt].num_nodes = len(self.mapper[nt]['from_nidx'])\n\n            keys = list(self.mapper[nt]['from_nidx'].keys())\n            emb_ = np.array([self.biobridge.emb_dict[i] for i in keys])\n            self.data[\"init\"][nt].x = torch.tensor(emb_, dtype=torch.float32)\n\n            node_names = nt_nodes[\"node_name\"].tolist()\n            self.data[\"init\"][nt][\"node_name\"] = node_names\n\n    def _add_edge_indices(self, triplets: pd.DataFrame) -&gt; None:\n        \"\"\"\n        Private method to add edge indices to the graph.\n\n        Args:\n            triplets (pd.DataFrame): DataFrame containing filtered triplet edges.\n\n        Returns:\n            None\n        \"\"\"\n        for ht, rt, tt in triplets[[\"head_type\",\n        \"display_relation\",\n        \"tail_type\"]].drop_duplicates().values:\n            t_ = triplets[\n                (triplets['head_type'] == ht) &amp;\n                (triplets['display_relation'] == rt) &amp;\n                (triplets['tail_type'] == tt)\n            ]\n            src_ids = t_['head_index'].map(self.mapper[ht]['from_nidx']).values\n            dst_ids = t_['tail_index'].map(self.mapper[tt]['from_nidx']).values\n\n            self.data[\"init\"][(ht, rt, tt)].edge_index = torch.tensor([src_ids,\n            dst_ids], dtype=torch.long)\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"\n        Loads and processes the data, optionally using cached data.\n\n        Returns:\n            None\n        \"\"\"\n        if os.path.exists(self.cache_path):\n            print(f\"\ud83d\udd01 Loading cached data from {self.cache_path}\")\n            with open(self.cache_path, \"rb\") as f:\n                self.data = pickle.load(f)\n            return\n\n        self._load_biobridge_data()\n        triplets = self._filter_triplets()\n\n        nodes = self.biobridge.primekg.get_nodes().copy()\n        nodes = nodes[nodes[\"node_index\"].isin(\n            np.unique(np.concatenate([triplets.head_index.unique(),\n                                      triplets.tail_index.unique()])))\n        ].reset_index(drop=True)\n        node_types = np.unique(nodes['node_type'].tolist())\n\n        self._add_node_features(nodes, node_types)\n        self._add_edge_indices(triplets)\n\n        with open(self.cache_path, \"wb\") as f:\n            pickle.dump(self.data, f)\n        print(f\"\u2705 Cached processed data to {self.cache_path}\")\n\n    def setup(self, stage: Optional[str] = None) -&gt; None:\n        \"\"\"\n        Sets up training, validation, and test splits using RandomLinkSplit.\n\n        Args:\n            stage (Optional[str]): Optional stage indicator.\n\n        Returns:\n            None\n        \"\"\"\n        if \"train\" in self.data:\n            return\n        with hydra.initialize(version_base=None, config_path=\"../../../configs\"):\n            cfg: DictConfig = hydra.compose(config_name=\"config\")\n\n        transform = RandomLinkSplit(\n            num_val=cfg.random_link_split.num_val,\n            num_test=cfg.random_link_split.num_test,\n            is_undirected=cfg.random_link_split.is_undirected,\n            add_negative_train_samples=cfg.random_link_split.add_negative_train_samples,\n            neg_sampling_ratio=cfg.random_link_split.neg_sampling_ratio,\n            split_labels=cfg.random_link_split.split_labels,\n            edge_types=self.data[\"init\"].edge_types,\n        )\n\n        self.data[\"train\"], self.data[\"val\"], self.data[\"test\"] = transform(self.data[\"init\"])\n\n        with open(self.cache_path, \"wb\") as f:\n            pickle.dump(self.data, f)\n        print(f\"\u2705 Cached train/val/test splits to {self.cache_path}\")\n\n    def train_dataloader(self) -&gt; GeoDataLoader:\n        \"\"\"\n        Returns the training dataloader.\n\n        Returns:\n            GeoDataLoader: DataLoader for training set.\n        \"\"\"\n        if \"train\" not in self.data:\n            raise RuntimeError(\"Please run `setup()` before calling train_dataloader().\")\n        return GeoDataLoader([self.data[\"train\"]], batch_size=1, shuffle=False)\n\n    def val_dataloader(self) -&gt; GeoDataLoader:\n        \"\"\"\n        Returns the validation dataloader.\n\n        Returns:\n            GeoDataLoader: DataLoader for validation set.\n        \"\"\"\n        if \"val\" not in self.data:\n            raise RuntimeError(\"Please run `setup()` before calling val_dataloader().\")\n        return GeoDataLoader([self.data[\"val\"]], batch_size=1, shuffle=False)\n\n    def test_dataloader(self) -&gt; GeoDataLoader:\n        \"\"\"\n        Returns the test dataloader.\n\n        Returns:\n            GeoDataLoader: DataLoader for test set.\n        \"\"\"\n        if \"test\" not in self.data:\n            raise RuntimeError(\"Please run `setup()` before calling test_dataloader().\")\n        return GeoDataLoader([self.data[\"test\"]], batch_size=1, shuffle=False)\n\n    def teardown(self, stage: Optional[str] = None) -&gt; None:\n        pass\n\n    def state_dict(self) -&gt; Dict[Any, Any]:\n        \"\"\"\n        Returns the internal state of the data module.\n\n        Returns:\n            dict: Empty dictionary (no state to save).\n        \"\"\"\n        return {}\n\n    def load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        pass\n</code></pre>"},{"location":"data/kg_dataloader/#vpeleaderboard.data.src.kg.biobridge_datamodule_hetero.BioBridgeDataModule.__init__","title":"<code>__init__(cfg)</code>","text":"<p>Initializes the BioBridgeDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Configuration object with dataset parameters.</p> required Source code in <code>vpeleaderboard/data/src/kg/biobridge_datamodule_hetero.py</code> <pre><code>def __init__(self, cfg: DictConfig) -&gt; None:\n    \"\"\"\n    Initializes the BioBridgeDataModule.\n\n    Args:\n        cfg (DictConfig): Configuration object with dataset parameters.\n    \"\"\"\n    super().__init__()\n    self.save_hyperparameters(logger=False)\n    self.cfg = cfg\n    self.primekg_dir = cfg.data.primekg_dir\n    self.biobridge_dir = cfg.data.biobridge_dir\n    self.batch_size = cfg.data.batch_size\n    self.cache_path = cfg.data.cache_path\n    self.biobridge = None\n    self.mapper = {}\n    self.data = {}\n</code></pre>"},{"location":"data/kg_dataloader/#vpeleaderboard.data.src.kg.biobridge_datamodule_hetero.BioBridgeDataModule._add_edge_indices","title":"<code>_add_edge_indices(triplets)</code>","text":"<p>Private method to add edge indices to the graph.</p> <p>Parameters:</p> Name Type Description Default <code>triplets</code> <code>DataFrame</code> <p>DataFrame containing filtered triplet edges.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vpeleaderboard/data/src/kg/biobridge_datamodule_hetero.py</code> <pre><code>def _add_edge_indices(self, triplets: pd.DataFrame) -&gt; None:\n    \"\"\"\n    Private method to add edge indices to the graph.\n\n    Args:\n        triplets (pd.DataFrame): DataFrame containing filtered triplet edges.\n\n    Returns:\n        None\n    \"\"\"\n    for ht, rt, tt in triplets[[\"head_type\",\n    \"display_relation\",\n    \"tail_type\"]].drop_duplicates().values:\n        t_ = triplets[\n            (triplets['head_type'] == ht) &amp;\n            (triplets['display_relation'] == rt) &amp;\n            (triplets['tail_type'] == tt)\n        ]\n        src_ids = t_['head_index'].map(self.mapper[ht]['from_nidx']).values\n        dst_ids = t_['tail_index'].map(self.mapper[tt]['from_nidx']).values\n\n        self.data[\"init\"][(ht, rt, tt)].edge_index = torch.tensor([src_ids,\n        dst_ids], dtype=torch.long)\n</code></pre>"},{"location":"data/kg_dataloader/#vpeleaderboard.data.src.kg.biobridge_datamodule_hetero.BioBridgeDataModule._add_node_features","title":"<code>_add_node_features(nodes, node_types)</code>","text":"<p>Private method to add node features to HeteroData.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>DataFrame</code> <p>DataFrame containing node metadata.</p> required <code>node_types</code> <code>ndarray</code> <p>Array of unique node types.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vpeleaderboard/data/src/kg/biobridge_datamodule_hetero.py</code> <pre><code>def _add_node_features(self, nodes: pd.DataFrame, node_types: np.ndarray) -&gt; None:\n    \"\"\"\n    Private method to add node features to HeteroData.\n\n    Args:\n        nodes (pd.DataFrame): DataFrame containing node metadata.\n        node_types (np.ndarray): Array of unique node types.\n\n    Returns:\n        None\n    \"\"\"\n    self.data[\"init\"] = HeteroData()\n\n    for nt in node_types:\n        self.mapper[nt] = {}\n        nt_nodes = nodes[nodes['node_type'] == nt].reset_index(drop=True)\n\n        self.mapper[nt]['to_nidx'] = nt_nodes[\"node_index\"].to_dict()\n        self.mapper[nt]['from_nidx'] = {v: k for k, v in self.mapper[nt]['to_nidx'].items()}\n\n        self.data[\"init\"][nt].num_nodes = len(self.mapper[nt]['from_nidx'])\n\n        keys = list(self.mapper[nt]['from_nidx'].keys())\n        emb_ = np.array([self.biobridge.emb_dict[i] for i in keys])\n        self.data[\"init\"][nt].x = torch.tensor(emb_, dtype=torch.float32)\n\n        node_names = nt_nodes[\"node_name\"].tolist()\n        self.data[\"init\"][nt][\"node_name\"] = node_names\n</code></pre>"},{"location":"data/kg_dataloader/#vpeleaderboard.data.src.kg.biobridge_datamodule_hetero.BioBridgeDataModule._filter_triplets","title":"<code>_filter_triplets()</code>","text":"<p>Private method to filter valid triplets from the PrimeKG edges.</p> <p>Returns:</p> Name Type Description <code>triplets</code> <code>DataFrame</code> <p>Filtered triplet data.</p> Source code in <code>vpeleaderboard/data/src/kg/biobridge_datamodule_hetero.py</code> <pre><code>def _filter_triplets(self):\n    \"\"\"\n    Private method to filter valid triplets from the PrimeKG edges.\n\n    Returns:\n        triplets (pd.DataFrame): Filtered triplet data.\n    \"\"\"\n    node_index_list = []\n    for node_type in self.biobridge.preselected_node_types:\n        df_node = pd.read_csv(os.path.join(self.biobridge.local_dir,\n                                           \"processed\", f\"{node_type}.csv\"))\n        node_index_list.extend(df_node[\"node_index\"].tolist())\n\n    triplets = self.biobridge.primekg.get_edges().copy()\n    triplets = triplets[\n        triplets[\"head_index\"].isin(node_index_list) &amp;\n        triplets[\"tail_index\"].isin(node_index_list)\n    ].reset_index(drop=True)\n\n    triplets = triplets[\n        triplets[\"head_index\"].isin(self.biobridge.emb_dict) &amp;\n        triplets[\"tail_index\"].isin(self.biobridge.emb_dict)\n    ].reset_index(drop=True)\n    return triplets\n</code></pre>"},{"location":"data/kg_dataloader/#vpeleaderboard.data.src.kg.biobridge_datamodule_hetero.BioBridgeDataModule._load_biobridge_data","title":"<code>_load_biobridge_data()</code>","text":"<p>Private method to load related files of PrimeKG dataset.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vpeleaderboard/data/src/kg/biobridge_datamodule_hetero.py</code> <pre><code>def _load_biobridge_data(self) -&gt; None:\n    \"\"\"\n    Private method to load related files of PrimeKG dataset.\n\n    Returns:\n        None\n    \"\"\"\n    self.biobridge = BioBridgePrimeKG(self.cfg)\n    self.biobridge.load_data()\n\n    self.data['nt2ntid'] = self.biobridge.get_data_config()[\"node_type\"]\n    self.data['ntid2nt'] = {v: k for k, v in self.data['nt2ntid'].items()}\n</code></pre>"},{"location":"data/kg_dataloader/#vpeleaderboard.data.src.kg.biobridge_datamodule_hetero.BioBridgeDataModule.prepare_data","title":"<code>prepare_data()</code>","text":"<p>Loads and processes the data, optionally using cached data.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vpeleaderboard/data/src/kg/biobridge_datamodule_hetero.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"\n    Loads and processes the data, optionally using cached data.\n\n    Returns:\n        None\n    \"\"\"\n    if os.path.exists(self.cache_path):\n        print(f\"\ud83d\udd01 Loading cached data from {self.cache_path}\")\n        with open(self.cache_path, \"rb\") as f:\n            self.data = pickle.load(f)\n        return\n\n    self._load_biobridge_data()\n    triplets = self._filter_triplets()\n\n    nodes = self.biobridge.primekg.get_nodes().copy()\n    nodes = nodes[nodes[\"node_index\"].isin(\n        np.unique(np.concatenate([triplets.head_index.unique(),\n                                  triplets.tail_index.unique()])))\n    ].reset_index(drop=True)\n    node_types = np.unique(nodes['node_type'].tolist())\n\n    self._add_node_features(nodes, node_types)\n    self._add_edge_indices(triplets)\n\n    with open(self.cache_path, \"wb\") as f:\n        pickle.dump(self.data, f)\n    print(f\"\u2705 Cached processed data to {self.cache_path}\")\n</code></pre>"},{"location":"data/kg_dataloader/#vpeleaderboard.data.src.kg.biobridge_datamodule_hetero.BioBridgeDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Sets up training, validation, and test splits using RandomLinkSplit.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>Optional[str]</code> <p>Optional stage indicator.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>vpeleaderboard/data/src/kg/biobridge_datamodule_hetero.py</code> <pre><code>def setup(self, stage: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Sets up training, validation, and test splits using RandomLinkSplit.\n\n    Args:\n        stage (Optional[str]): Optional stage indicator.\n\n    Returns:\n        None\n    \"\"\"\n    if \"train\" in self.data:\n        return\n    with hydra.initialize(version_base=None, config_path=\"../../../configs\"):\n        cfg: DictConfig = hydra.compose(config_name=\"config\")\n\n    transform = RandomLinkSplit(\n        num_val=cfg.random_link_split.num_val,\n        num_test=cfg.random_link_split.num_test,\n        is_undirected=cfg.random_link_split.is_undirected,\n        add_negative_train_samples=cfg.random_link_split.add_negative_train_samples,\n        neg_sampling_ratio=cfg.random_link_split.neg_sampling_ratio,\n        split_labels=cfg.random_link_split.split_labels,\n        edge_types=self.data[\"init\"].edge_types,\n    )\n\n    self.data[\"train\"], self.data[\"val\"], self.data[\"test\"] = transform(self.data[\"init\"])\n\n    with open(self.cache_path, \"wb\") as f:\n        pickle.dump(self.data, f)\n    print(f\"\u2705 Cached train/val/test splits to {self.cache_path}\")\n</code></pre>"},{"location":"data/kg_dataloader/#vpeleaderboard.data.src.kg.biobridge_datamodule_hetero.BioBridgeDataModule.state_dict","title":"<code>state_dict()</code>","text":"<p>Returns the internal state of the data module.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[Any, Any]</code> <p>Empty dictionary (no state to save).</p> Source code in <code>vpeleaderboard/data/src/kg/biobridge_datamodule_hetero.py</code> <pre><code>def state_dict(self) -&gt; Dict[Any, Any]:\n    \"\"\"\n    Returns the internal state of the data module.\n\n    Returns:\n        dict: Empty dictionary (no state to save).\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"data/kg_dataloader/#vpeleaderboard.data.src.kg.biobridge_datamodule_hetero.BioBridgeDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Returns the test dataloader.</p> <p>Returns:</p> Name Type Description <code>GeoDataLoader</code> <code>DataLoader</code> <p>DataLoader for test set.</p> Source code in <code>vpeleaderboard/data/src/kg/biobridge_datamodule_hetero.py</code> <pre><code>def test_dataloader(self) -&gt; GeoDataLoader:\n    \"\"\"\n    Returns the test dataloader.\n\n    Returns:\n        GeoDataLoader: DataLoader for test set.\n    \"\"\"\n    if \"test\" not in self.data:\n        raise RuntimeError(\"Please run `setup()` before calling test_dataloader().\")\n    return GeoDataLoader([self.data[\"test\"]], batch_size=1, shuffle=False)\n</code></pre>"},{"location":"data/kg_dataloader/#vpeleaderboard.data.src.kg.biobridge_datamodule_hetero.BioBridgeDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns the training dataloader.</p> <p>Returns:</p> Name Type Description <code>GeoDataLoader</code> <code>DataLoader</code> <p>DataLoader for training set.</p> Source code in <code>vpeleaderboard/data/src/kg/biobridge_datamodule_hetero.py</code> <pre><code>def train_dataloader(self) -&gt; GeoDataLoader:\n    \"\"\"\n    Returns the training dataloader.\n\n    Returns:\n        GeoDataLoader: DataLoader for training set.\n    \"\"\"\n    if \"train\" not in self.data:\n        raise RuntimeError(\"Please run `setup()` before calling train_dataloader().\")\n    return GeoDataLoader([self.data[\"train\"]], batch_size=1, shuffle=False)\n</code></pre>"},{"location":"data/kg_dataloader/#vpeleaderboard.data.src.kg.biobridge_datamodule_hetero.BioBridgeDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Returns the validation dataloader.</p> <p>Returns:</p> Name Type Description <code>GeoDataLoader</code> <code>DataLoader</code> <p>DataLoader for validation set.</p> Source code in <code>vpeleaderboard/data/src/kg/biobridge_datamodule_hetero.py</code> <pre><code>def val_dataloader(self) -&gt; GeoDataLoader:\n    \"\"\"\n    Returns the validation dataloader.\n\n    Returns:\n        GeoDataLoader: DataLoader for validation set.\n    \"\"\"\n    if \"val\" not in self.data:\n        raise RuntimeError(\"Please run `setup()` before calling val_dataloader().\")\n    return GeoDataLoader([self.data[\"val\"]], batch_size=1, shuffle=False)\n</code></pre>"},{"location":"data/loading_model/","title":"How to Load and Contribute SBML Models","text":"<p>This guide outlines the correct procedure for contributing SBML models to the Virtual Patient Leaderboard repository. By following these instructions, your models will be automatically validated, processed, and integrated into the leaderboard system via CI/CD.</p>"},{"location":"data/loading_model/#required-files","title":"Required Files","text":"<p>To submit a model, you must provide both of the following:</p> File Type Naming Example Target Directory SBML Model File <code>your_model.xml</code> <code>vpeleaderboard/data/src/sbml/models/</code> YAML Configuration File <code>your_model.yaml</code> <code>vpeleaderboard/data/configs/</code> <p>\u26a0\ufe0f Kindly ensure both files are present when submitting your model. Submissions with only one file will not be processed. Every XML model file must have a corresponding YAML configuration file with the same base name.</p>"},{"location":"data/loading_model/#folder-structure-reference","title":"Folder Structure Reference","text":"<p>Ensure your file placement adheres to this structure:</p> <pre><code>vpeleaderboard/data/\n\u251c\u2500\u2500 src/sbml/models/\n\u2502   \u2514\u2500\u2500 your_model.xml\n|   \u2514\u2500\u2500 BIOMD0000000537_url.xml\n\u251c\u2500\u2500 configs/\n\u2502   \u2514\u2500\u2500 your_model.yaml\n|   \u2514\u2500\u2500 BIOMD0000000537_url.yaml\n</code></pre> <p>Please ensure that both the XML file and the corresponding YAML configuration file share the same base name.  Submissions that do not follow this structure will fail the automated GitHub Actions validation.</p>"},{"location":"data/loading_model/#how-to-add-a-new-model","title":"How to add a new model","text":""},{"location":"data/loading_model/#1-fork-the-repository","title":"1. Fork the Repository","text":"<p>Navigate to the repository and click the \u201cFork\u201d button in the top-right corner of the repository on GitHub to create a copy under your GitHub account.</p>"},{"location":"data/loading_model/#2-clone-your-fork-locally","title":"2. Clone Your Fork Locally","text":"<pre><code>git clone https://github.com/&lt;your-username&gt;/&lt;repo-name&gt;.git\ncd &lt;repo-name&gt;\n</code></pre>"},{"location":"data/loading_model/#3-create-a-new-branch","title":"3. Create a New Branch","text":"<p>Ensure you use a descriptive branch name that clearly reflects the purpose of your changes:</p> <p><pre><code>git checkout -b add-new-your_model\n</code></pre> This helps maintain clarity and consistency across the project, making it easier to track changes and collaborate effectively.</p>"},{"location":"data/loading_model/#4-add-your-files","title":"4. Add Your Files","text":""},{"location":"data/loading_model/#yaml-configuration-parameters","title":"YAML Configuration Parameters","text":"<p>Each SBML model must be accompanied by a .yaml configuration file that defines the simulation durations. These parameters control how long the model runs for training, validation, and testing phases during automated processing.</p> <p>Your <code>.yaml</code> file must include the following fields:</p> Key Type Description <code>train_duration</code> <code>int</code> Defines the duration (in simulation time units) allocated for the training phase of the model. This value should match the scale of the model's dataset. <code>val_duration</code> <code>int</code> Defines the duration (in simulation time units) allocated for the validation phase. <code>test_duration</code> <code>int</code> Defines the duration (in simulation time units) allocated for the testing phase. Should align with testing scenarios in the model\u2019s intended use. <p>Example Configuration</p> <pre><code>train_duration: 6\nval_duration: 3\ntest_duration: 5\n</code></pre> <ul> <li> <p>Put the XML model file into the models/ directory.</p> </li> <li> <p>Put the YAML configuration file into the configs/ directory.</p> </li> </ul> <p>\u26a0\ufe0f Make sure both files follow the naming conventions described above and that a YAML file is provided for every XML model file.</p>"},{"location":"data/loading_model/#5-commit-and-push-your-changes","title":"5. Commit and Push Your Changes","text":"<pre><code>git add models/your_model.xml configs/your_model.yaml\ngit commit -m \"Add model your_model with configuration\"\ngit push origin add-new-model-your_model\n</code></pre>"},{"location":"data/loading_model/#6-open-a-pull-request-pr","title":"6. Open a Pull Request (PR)","text":"<ol> <li>Go to your fork on GitHub.</li> <li>Click \"Compare &amp; pull request\".</li> <li>Set the base branch to <code>main</code> of the original repository.</li> <li>Provide a clear and concise title and description.</li> <li>Click \"Create pull request\".</li> </ol> <p>Virtual Patient Engine Docs</p>"},{"location":"data/overview/","title":"\ud83e\uddec Data Overview","text":"<p>Welcome to the Data Section of the VPE Leaderboard platform. This section provides the tools, documentation, and loaders needed to work with two primary data formats: SBML models and Knowledge Graphs (KGs).</p> <p>Choose the workflow relevant to your data:</p> <ul> <li> <p> SBML Dataloader</p> <p>Parse and load biological models in SBML (Systems Biology Markup Language) format. This loader extracts species, reactions, and parameters, preparing them for simulation and analysis.</p> <p>\ud83d\udcc4 Code Documentation \ud83d\udcd8 Tutorial Notebook</p> </li> <li> <p> Knowledge Graph Dataloader</p> <p>Import and standardize structured graph datasets such as biomedical or molecular knowledge graphs. Includes preprocessing, normalization, and integration into our evaluation pipelines.</p> <p>\ud83d\udcc4 Code Documentation \ud83d\udcd8 Tutorial Notebook</p> </li> </ul> <p>If you're unsure which data format your model fits into, we recommend reviewing both loaders above and their respective notebooks before proceeding. For further questions, reach out to the development team or file an issue on the GitHub repository.</p>"},{"location":"data/primekg/","title":"Primekg","text":"<p>Test cases for datasets/primekg_loader.py</p>"},{"location":"data/primekg/#vpeleaderboard.data.utils.primekg.PrimeKG","title":"<code>PrimeKG</code>","text":"<p>Class for loading PrimeKG dataset. It downloads the data from the Harvard Dataverse and stores it in the local directory. The data is then loaded into pandas DataFrame of nodes and edges.</p> Source code in <code>vpeleaderboard/data/utils/primekg.py</code> <pre><code>class PrimeKG:\n    \"\"\"\n    Class for loading PrimeKG dataset.\n    It downloads the data from the Harvard Dataverse and stores it in the local directory.\n    The data is then loaded into pandas DataFrame of nodes and edges.\n    \"\"\"\n\n    def __init__(self, cfg: DictConfig) -&gt; None:\n        \"\"\"\n        Constructor for PrimeKG class.\n\n        Args:\n            local_dir (str): The local directory where the data will be stored.\n        \"\"\"\n        self.name: str = \"primekg\"\n        self.server_path: str = \"https://dataverse.harvard.edu/api/access/datafile/\"\n        self.file_ids: dict = {\"nodes\": 6180617, \"edges\": 6180616}\n\n        if isinstance(cfg, DictConfig):\n            self.local_dir: str = cfg.data.primekg_dir\n        elif isinstance(cfg, dict):\n            self.local_dir: str = cfg[\"data\"][\"primekg_dir\"]\n        elif isinstance(cfg, str):\n            self.local_dir: str = cfg\n        else:\n            raise TypeError(f\"Unsupported config type: {type(cfg)}\")\n\n        # Attributes to store the data\n        self.nodes: pd.DataFrame = None\n        self.edges: pd.DataFrame = None\n        os.makedirs(os.path.dirname(self.local_dir), exist_ok=True)\n\n\n    def _download_file(self, remote_url:str, local_path: str):\n        \"\"\"\n        A helper function to download a file from remote URL to the local directory.\n\n        Args:\n            remote_url (str): The remote URL of the file to be downloaded.\n            local_path (str): The local path where the file will be saved.\n        \"\"\"\n        response = requests.get(remote_url, stream=True, timeout=300)\n        response.raise_for_status()\n        progress_bar = tqdm(\n            total=int(response.headers.get(\"content-length\", 0)),\n            unit=\"iB\",\n            unit_scale=True,\n        )\n        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n        with open(local_path, \"wb\") as file:\n            for data in response.iter_content(1024):\n                progress_bar.update(len(data))\n                file.write(data)\n        progress_bar.close()\n\n    def _load_nodes(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Private method to load the nodes dataframe of PrimeKG dataset.\n        This method downloads the nodes file from the Harvard Dataverse if it does not exist\n        in the local directory. Otherwise, it loads the data from the local directory.\n        It further processes the dataframe of nodes and returns it.\n\n        Returns:\n            The nodes dataframe of PrimeKG dataset.\n        \"\"\"\n        local_file = os.path.join(self.local_dir, f\"{self.name}_nodes.tsv.gz\")\n        if os.path.exists(local_file):\n            print(f\"{local_file} already exists. Loading the data from the local directory.\")\n            nodes = pd.read_csv(local_file, sep=\"\\t\", compression=\"gzip\", low_memory=False)\n        else:\n            print(f\"Downloading node file from {self.server_path}{self.file_ids['nodes']}\")\n            self._download_file(f\"{self.server_path}{self.file_ids['nodes']}\",\n                                os.path.join(self.local_dir, \"nodes.tab\"))\n            nodes = pd.read_csv(os.path.join(self.local_dir, \"nodes.tab\"),\n                                     sep=\"\\t\", low_memory=False)\n            nodes = nodes[\n                [\"node_index\", \"node_name\", \"node_source\", \"node_id\", \"node_type\"]\n            ]\n            nodes.to_csv(local_file, index=False, sep=\"\\t\", compression=\"gzip\")\n\n        return nodes\n\n    def _load_edges(self, nodes: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Private method to load the edges dataframe of PrimeKG dataset.\n        This method downloads the edges file from the Harvard Dataverse if it does not exist\n        in the local directory. Otherwise, it loads the data from the local directory.\n        It further processes the dataframe of edges and returns it.\n\n        Args:\n            nodes (pd.DataFrame): The nodes dataframe of PrimeKG dataset.\n\n        Returns:\n            The edges dataframe of PrimeKG dataset.\n        \"\"\"\n        local_file = os.path.join(self.local_dir, f\"{self.name}_edges.tsv.gz\")\n        if os.path.exists(local_file):\n            print(f\"{local_file} already exists. Loading the data from the local directory.\")\n            edges = pd.read_csv(local_file, sep=\"\\t\", compression=\"gzip\", low_memory=False)\n        else:\n            print(f\"Downloading edge file from {self.server_path}{self.file_ids['edges']}\")\n            self._download_file(f\"{self.server_path}{self.file_ids['edges']}\",\n                                os.path.join(self.local_dir, \"edges.csv\"))\n            edges = pd.read_csv(os.path.join(self.local_dir, \"edges.csv\"),\n                                     sep=\",\", low_memory=False)\n            edges = edges.merge(\n                nodes, left_on=\"x_index\", right_on=\"node_index\"\n            )\n            edges.drop([\"x_index\"], axis=1, inplace=True)\n            edges.rename(\n                columns={\n                    \"node_index\": \"head_index\",\n                    \"node_name\": \"head_name\",\n                    \"node_source\": \"head_source\",\n                    \"node_id\": \"head_id\",\n                    \"node_type\": \"head_type\",\n                },\n                inplace=True,\n            )\n            edges = edges.merge(\n                nodes, left_on=\"y_index\", right_on=\"node_index\"\n            )\n            edges.drop([\"y_index\"], axis=1, inplace=True)\n            edges.rename(\n                columns={\n                    \"node_index\": \"tail_index\",\n                    \"node_name\": \"tail_name\",\n                    \"node_source\": \"tail_source\",\n                    \"node_id\": \"tail_id\",\n                    \"node_type\": \"tail_type\"\n                },\n                inplace=True,\n            )\n            edges = edges[\n                [\n                    \"head_index\", \"head_name\", \"head_source\", \"head_id\", \"head_type\",\n                    \"tail_index\", \"tail_name\", \"tail_source\", \"tail_id\", \"tail_type\",\n                    \"display_relation\", \"relation\",\n                ]\n            ]\n            edges.to_csv(local_file, index=False, sep=\"\\t\", compression=\"gzip\")\n\n        return edges\n\n    def load_data(self):\n        \"\"\"\n        Load the PrimeKG dataset into pandas DataFrame of nodes and edges.\n        \"\"\"\n        self.nodes = self._load_nodes()\n        self.edges = self._load_edges(self.nodes)\n\n    def get_nodes(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Get the nodes dataframe of PrimeKG dataset.\n\n        Returns:\n            The nodes dataframe of PrimeKG dataset.\n        \"\"\"\n        return self.nodes\n\n    def get_edges(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Get the edges dataframe of PrimeKG dataset.\n\n        Returns:\n            The edges dataframe of PrimeKG dataset.\n        \"\"\"\n        return self.edges\n</code></pre>"},{"location":"data/primekg/#vpeleaderboard.data.utils.primekg.PrimeKG.__init__","title":"<code>__init__(cfg)</code>","text":"<p>Constructor for PrimeKG class.</p> <p>Parameters:</p> Name Type Description Default <code>local_dir</code> <code>str</code> <p>The local directory where the data will be stored.</p> required Source code in <code>vpeleaderboard/data/utils/primekg.py</code> <pre><code>def __init__(self, cfg: DictConfig) -&gt; None:\n    \"\"\"\n    Constructor for PrimeKG class.\n\n    Args:\n        local_dir (str): The local directory where the data will be stored.\n    \"\"\"\n    self.name: str = \"primekg\"\n    self.server_path: str = \"https://dataverse.harvard.edu/api/access/datafile/\"\n    self.file_ids: dict = {\"nodes\": 6180617, \"edges\": 6180616}\n\n    if isinstance(cfg, DictConfig):\n        self.local_dir: str = cfg.data.primekg_dir\n    elif isinstance(cfg, dict):\n        self.local_dir: str = cfg[\"data\"][\"primekg_dir\"]\n    elif isinstance(cfg, str):\n        self.local_dir: str = cfg\n    else:\n        raise TypeError(f\"Unsupported config type: {type(cfg)}\")\n\n    # Attributes to store the data\n    self.nodes: pd.DataFrame = None\n    self.edges: pd.DataFrame = None\n    os.makedirs(os.path.dirname(self.local_dir), exist_ok=True)\n</code></pre>"},{"location":"data/primekg/#vpeleaderboard.data.utils.primekg.PrimeKG._download_file","title":"<code>_download_file(remote_url, local_path)</code>","text":"<p>A helper function to download a file from remote URL to the local directory.</p> <p>Parameters:</p> Name Type Description Default <code>remote_url</code> <code>str</code> <p>The remote URL of the file to be downloaded.</p> required <code>local_path</code> <code>str</code> <p>The local path where the file will be saved.</p> required Source code in <code>vpeleaderboard/data/utils/primekg.py</code> <pre><code>def _download_file(self, remote_url:str, local_path: str):\n    \"\"\"\n    A helper function to download a file from remote URL to the local directory.\n\n    Args:\n        remote_url (str): The remote URL of the file to be downloaded.\n        local_path (str): The local path where the file will be saved.\n    \"\"\"\n    response = requests.get(remote_url, stream=True, timeout=300)\n    response.raise_for_status()\n    progress_bar = tqdm(\n        total=int(response.headers.get(\"content-length\", 0)),\n        unit=\"iB\",\n        unit_scale=True,\n    )\n    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n    with open(local_path, \"wb\") as file:\n        for data in response.iter_content(1024):\n            progress_bar.update(len(data))\n            file.write(data)\n    progress_bar.close()\n</code></pre>"},{"location":"data/primekg/#vpeleaderboard.data.utils.primekg.PrimeKG._load_edges","title":"<code>_load_edges(nodes)</code>","text":"<p>Private method to load the edges dataframe of PrimeKG dataset. This method downloads the edges file from the Harvard Dataverse if it does not exist in the local directory. Otherwise, it loads the data from the local directory. It further processes the dataframe of edges and returns it.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>DataFrame</code> <p>The nodes dataframe of PrimeKG dataset.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The edges dataframe of PrimeKG dataset.</p> Source code in <code>vpeleaderboard/data/utils/primekg.py</code> <pre><code>def _load_edges(self, nodes: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Private method to load the edges dataframe of PrimeKG dataset.\n    This method downloads the edges file from the Harvard Dataverse if it does not exist\n    in the local directory. Otherwise, it loads the data from the local directory.\n    It further processes the dataframe of edges and returns it.\n\n    Args:\n        nodes (pd.DataFrame): The nodes dataframe of PrimeKG dataset.\n\n    Returns:\n        The edges dataframe of PrimeKG dataset.\n    \"\"\"\n    local_file = os.path.join(self.local_dir, f\"{self.name}_edges.tsv.gz\")\n    if os.path.exists(local_file):\n        print(f\"{local_file} already exists. Loading the data from the local directory.\")\n        edges = pd.read_csv(local_file, sep=\"\\t\", compression=\"gzip\", low_memory=False)\n    else:\n        print(f\"Downloading edge file from {self.server_path}{self.file_ids['edges']}\")\n        self._download_file(f\"{self.server_path}{self.file_ids['edges']}\",\n                            os.path.join(self.local_dir, \"edges.csv\"))\n        edges = pd.read_csv(os.path.join(self.local_dir, \"edges.csv\"),\n                                 sep=\",\", low_memory=False)\n        edges = edges.merge(\n            nodes, left_on=\"x_index\", right_on=\"node_index\"\n        )\n        edges.drop([\"x_index\"], axis=1, inplace=True)\n        edges.rename(\n            columns={\n                \"node_index\": \"head_index\",\n                \"node_name\": \"head_name\",\n                \"node_source\": \"head_source\",\n                \"node_id\": \"head_id\",\n                \"node_type\": \"head_type\",\n            },\n            inplace=True,\n        )\n        edges = edges.merge(\n            nodes, left_on=\"y_index\", right_on=\"node_index\"\n        )\n        edges.drop([\"y_index\"], axis=1, inplace=True)\n        edges.rename(\n            columns={\n                \"node_index\": \"tail_index\",\n                \"node_name\": \"tail_name\",\n                \"node_source\": \"tail_source\",\n                \"node_id\": \"tail_id\",\n                \"node_type\": \"tail_type\"\n            },\n            inplace=True,\n        )\n        edges = edges[\n            [\n                \"head_index\", \"head_name\", \"head_source\", \"head_id\", \"head_type\",\n                \"tail_index\", \"tail_name\", \"tail_source\", \"tail_id\", \"tail_type\",\n                \"display_relation\", \"relation\",\n            ]\n        ]\n        edges.to_csv(local_file, index=False, sep=\"\\t\", compression=\"gzip\")\n\n    return edges\n</code></pre>"},{"location":"data/primekg/#vpeleaderboard.data.utils.primekg.PrimeKG._load_nodes","title":"<code>_load_nodes()</code>","text":"<p>Private method to load the nodes dataframe of PrimeKG dataset. This method downloads the nodes file from the Harvard Dataverse if it does not exist in the local directory. Otherwise, it loads the data from the local directory. It further processes the dataframe of nodes and returns it.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The nodes dataframe of PrimeKG dataset.</p> Source code in <code>vpeleaderboard/data/utils/primekg.py</code> <pre><code>def _load_nodes(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Private method to load the nodes dataframe of PrimeKG dataset.\n    This method downloads the nodes file from the Harvard Dataverse if it does not exist\n    in the local directory. Otherwise, it loads the data from the local directory.\n    It further processes the dataframe of nodes and returns it.\n\n    Returns:\n        The nodes dataframe of PrimeKG dataset.\n    \"\"\"\n    local_file = os.path.join(self.local_dir, f\"{self.name}_nodes.tsv.gz\")\n    if os.path.exists(local_file):\n        print(f\"{local_file} already exists. Loading the data from the local directory.\")\n        nodes = pd.read_csv(local_file, sep=\"\\t\", compression=\"gzip\", low_memory=False)\n    else:\n        print(f\"Downloading node file from {self.server_path}{self.file_ids['nodes']}\")\n        self._download_file(f\"{self.server_path}{self.file_ids['nodes']}\",\n                            os.path.join(self.local_dir, \"nodes.tab\"))\n        nodes = pd.read_csv(os.path.join(self.local_dir, \"nodes.tab\"),\n                                 sep=\"\\t\", low_memory=False)\n        nodes = nodes[\n            [\"node_index\", \"node_name\", \"node_source\", \"node_id\", \"node_type\"]\n        ]\n        nodes.to_csv(local_file, index=False, sep=\"\\t\", compression=\"gzip\")\n\n    return nodes\n</code></pre>"},{"location":"data/primekg/#vpeleaderboard.data.utils.primekg.PrimeKG.get_edges","title":"<code>get_edges()</code>","text":"<p>Get the edges dataframe of PrimeKG dataset.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The edges dataframe of PrimeKG dataset.</p> Source code in <code>vpeleaderboard/data/utils/primekg.py</code> <pre><code>def get_edges(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Get the edges dataframe of PrimeKG dataset.\n\n    Returns:\n        The edges dataframe of PrimeKG dataset.\n    \"\"\"\n    return self.edges\n</code></pre>"},{"location":"data/primekg/#vpeleaderboard.data.utils.primekg.PrimeKG.get_nodes","title":"<code>get_nodes()</code>","text":"<p>Get the nodes dataframe of PrimeKG dataset.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The nodes dataframe of PrimeKG dataset.</p> Source code in <code>vpeleaderboard/data/utils/primekg.py</code> <pre><code>def get_nodes(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Get the nodes dataframe of PrimeKG dataset.\n\n    Returns:\n        The nodes dataframe of PrimeKG dataset.\n    \"\"\"\n    return self.nodes\n</code></pre>"},{"location":"data/primekg/#vpeleaderboard.data.utils.primekg.PrimeKG.load_data","title":"<code>load_data()</code>","text":"<p>Load the PrimeKG dataset into pandas DataFrame of nodes and edges.</p> Source code in <code>vpeleaderboard/data/utils/primekg.py</code> <pre><code>def load_data(self):\n    \"\"\"\n    Load the PrimeKG dataset into pandas DataFrame of nodes and edges.\n    \"\"\"\n    self.nodes = self._load_nodes()\n    self.edges = self._load_edges(self.nodes)\n</code></pre>"},{"location":"data/sbml_dataloader/","title":"SBML Dataloader","text":"<p>This module handles loading and simulating SBML models using PyTorch Lightning.</p>"},{"location":"data/sbml_dataloader/#vpeleaderboard.data.src.sbml.sbml_dataloader.SBMLDataModule","title":"<code>SBMLDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>A LightningDataModule for simulating and loading SBML-based time course data.</p> Source code in <code>vpeleaderboard/data/src/sbml/sbml_dataloader.py</code> <pre><code>class SBMLDataModule(LightningDataModule):\n    \"\"\"\n    A LightningDataModule for simulating and loading SBML-based time course data.\n    \"\"\"\n\n    class SBMLTimeCourseDataset(IterableDataset):\n        \"\"\"\n        Dataset class for iterating over SBML simulation results.\n        \"\"\"\n\n        def __init__(self, data: Any):\n            \"\"\"\n            Args:\n                data (Any): Data to be used in the dataset (e.g., pandas DataFrame).\n            \"\"\"\n            self.data = data\n\n        def __iter__(self):\n            \"\"\"\n            Iterator method for the dataset.\n\n            Yields:\n                torch.Tensor: Each row in the dataset as a tensor.\n            \"\"\"\n            for _, row in self.data.iterrows():\n                yield torch.tensor(row.values, dtype=torch.float)\n\n        def __getitem__(self, index: int):\n            raise NotImplementedError(\"Indexing is not supported for this IterableDataset.\")\n\n    def __init__(self, file_name: str):\n        \"\"\"\n        Initializes the SBMLDataModule with the given SBML model file name.\n\n        Args:\n            file_name (str): The name of the SBML model to load.\n        \"\"\"\n        super().__init__()\n        self.fine_name = file_name\n\n        self.config = None\n        self.sbml_file_path = None\n        self.copasi_model: Optional[Any] = None\n\n        self._is_prepared = False\n        self._is_setup = False\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"\n        Loads YAML config and locates the SBML file.\n\n        This method will locate and load the YAML configuration file, validate its contents,\n        and check for the necessary SBML model file.\n\n        Raises:\n            FileNotFoundError: If the SBML model or YAML config file is not found.\n            ValueError: If the YAML config file is empty or missing required keys.\n        \"\"\"\n        script_dir = os.path.dirname(__file__)\n        script_dir = \"\\\\\".join(script_dir.split(\"\\\\\")[:-1])\n        script_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n        with hydra.initialize(version_base=None,\n                              config_path=\"../../../configs\"):\n            cfg = hydra.compose(config_name=\"config\")\n\n        model_directory = os.path.join(script_dir, cfg.model_directory)\n        model_directory = os.path.abspath(model_directory)\n\n        sbml_path = os.path.join(model_directory, f\"{self.fine_name}.xml\")\n        if not os.path.exists(sbml_path):\n            raise FileNotFoundError(\n                f\"SBML model file not found for model '{self.fine_name}'. \"\n                f\"Expected at: {sbml_path}\"\n            )\n        self.sbml_file_path = sbml_path\n        yaml_file = os.path.join(script_dir, \"../../configs/data\", f\"{self.fine_name}.yaml\")\n        if not os.path.exists(yaml_file):\n            raise FileNotFoundError(f\"YAML file not found: {yaml_file}\")\n\n        with open(yaml_file, 'r', encoding='utf-8') as file:\n            self.config = yaml.safe_load(file)\n\n        if not self.config:\n            raise ValueError(f\"YAML config {yaml_file} is empty or malformed.\")\n\n        required_keys = ['train_duration', 'test_duration', 'val_duration']\n        missing_keys = [key for key in required_keys\n                        if key not in self.config or self.config[key] is None]\n        if missing_keys:\n            raise ValueError(\n                f\"Missing required key(s) in YAML config {yaml_file}: {', '.join(missing_keys)}\"\n            )\n\n        self._is_prepared = True\n\n    def setup(self, stage: Optional[str] = None) -&gt; None:\n        \"\"\"\n        Loads the SBML model only after prepare_data has been called.\n\n        Args:\n            stage (Optional[str]): The stage of setup, typically used in multi-stage setups.\n\n        Raises:\n            RuntimeError: If `prepare_data()` has not been called before `setup()`.\n            FileNotFoundError: If the SBML file is not found.\n        \"\"\"\n        if not self._is_prepared:\n            raise RuntimeError(\"You must call `prepare_data()` before `setup()`.\")\n\n        if not os.path.exists(self.sbml_file_path):\n            raise FileNotFoundError(f\"SBML model file not found: {self.sbml_file_path}\")\n\n        self.copasi_model = basico.load_model(self.sbml_file_path)\n\n    def train_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Creates the DataLoader for the training dataset based on the SBML simulation results.\n\n        Returns:\n            DataLoader: The DataLoader for the training dataset.\n        \"\"\"\n        train_df = basico.run_time_course(\n            duration=self.config['train_duration'],\n            use_initial_values=False\n        )\n        dataset = self.SBMLTimeCourseDataset(train_df)\n        return DataLoader(dataset)\n    def val_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Creates the DataLoader for the validating dataset based on the SBML simulation results.\n\n        Returns:\n            DataLoader: The DataLoader for the validating dataset.\n        \"\"\"\n        val_df = basico.run_time_course(\n            duration=self.config['val_duration'],\n            use_initial_values=False\n        )\n        dataset = self.SBMLTimeCourseDataset(val_df)\n        return DataLoader(dataset)\n\n    def test_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Creates the DataLoader for the test dataset based on the SBML simulation results.\n\n        Returns:\n            DataLoader: The DataLoader for the test dataset.\n        \"\"\"\n        test_df = basico.run_time_course(\n            duration=self.config['test_duration'],\n            automatic=False,\n            use_initial_values=False,\n            update_model=False\n        )\n        dataset = self.SBMLTimeCourseDataset(test_df)\n        return DataLoader(dataset)\n</code></pre>"},{"location":"data/sbml_dataloader/#vpeleaderboard.data.src.sbml.sbml_dataloader.SBMLDataModule.SBMLTimeCourseDataset","title":"<code>SBMLTimeCourseDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>Dataset class for iterating over SBML simulation results.</p> Source code in <code>vpeleaderboard/data/src/sbml/sbml_dataloader.py</code> <pre><code>class SBMLTimeCourseDataset(IterableDataset):\n    \"\"\"\n    Dataset class for iterating over SBML simulation results.\n    \"\"\"\n\n    def __init__(self, data: Any):\n        \"\"\"\n        Args:\n            data (Any): Data to be used in the dataset (e.g., pandas DataFrame).\n        \"\"\"\n        self.data = data\n\n    def __iter__(self):\n        \"\"\"\n        Iterator method for the dataset.\n\n        Yields:\n            torch.Tensor: Each row in the dataset as a tensor.\n        \"\"\"\n        for _, row in self.data.iterrows():\n            yield torch.tensor(row.values, dtype=torch.float)\n\n    def __getitem__(self, index: int):\n        raise NotImplementedError(\"Indexing is not supported for this IterableDataset.\")\n</code></pre>"},{"location":"data/sbml_dataloader/#vpeleaderboard.data.src.sbml.sbml_dataloader.SBMLDataModule.SBMLTimeCourseDataset.__init__","title":"<code>__init__(data)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>Data to be used in the dataset (e.g., pandas DataFrame).</p> required Source code in <code>vpeleaderboard/data/src/sbml/sbml_dataloader.py</code> <pre><code>def __init__(self, data: Any):\n    \"\"\"\n    Args:\n        data (Any): Data to be used in the dataset (e.g., pandas DataFrame).\n    \"\"\"\n    self.data = data\n</code></pre>"},{"location":"data/sbml_dataloader/#vpeleaderboard.data.src.sbml.sbml_dataloader.SBMLDataModule.SBMLTimeCourseDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterator method for the dataset.</p> <p>Yields:</p> Type Description <p>torch.Tensor: Each row in the dataset as a tensor.</p> Source code in <code>vpeleaderboard/data/src/sbml/sbml_dataloader.py</code> <pre><code>def __iter__(self):\n    \"\"\"\n    Iterator method for the dataset.\n\n    Yields:\n        torch.Tensor: Each row in the dataset as a tensor.\n    \"\"\"\n    for _, row in self.data.iterrows():\n        yield torch.tensor(row.values, dtype=torch.float)\n</code></pre>"},{"location":"data/sbml_dataloader/#vpeleaderboard.data.src.sbml.sbml_dataloader.SBMLDataModule.__init__","title":"<code>__init__(file_name)</code>","text":"<p>Initializes the SBMLDataModule with the given SBML model file name.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the SBML model to load.</p> required Source code in <code>vpeleaderboard/data/src/sbml/sbml_dataloader.py</code> <pre><code>def __init__(self, file_name: str):\n    \"\"\"\n    Initializes the SBMLDataModule with the given SBML model file name.\n\n    Args:\n        file_name (str): The name of the SBML model to load.\n    \"\"\"\n    super().__init__()\n    self.fine_name = file_name\n\n    self.config = None\n    self.sbml_file_path = None\n    self.copasi_model: Optional[Any] = None\n\n    self._is_prepared = False\n    self._is_setup = False\n</code></pre>"},{"location":"data/sbml_dataloader/#vpeleaderboard.data.src.sbml.sbml_dataloader.SBMLDataModule.prepare_data","title":"<code>prepare_data()</code>","text":"<p>Loads YAML config and locates the SBML file.</p> <p>This method will locate and load the YAML configuration file, validate its contents, and check for the necessary SBML model file.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the SBML model or YAML config file is not found.</p> <code>ValueError</code> <p>If the YAML config file is empty or missing required keys.</p> Source code in <code>vpeleaderboard/data/src/sbml/sbml_dataloader.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"\n    Loads YAML config and locates the SBML file.\n\n    This method will locate and load the YAML configuration file, validate its contents,\n    and check for the necessary SBML model file.\n\n    Raises:\n        FileNotFoundError: If the SBML model or YAML config file is not found.\n        ValueError: If the YAML config file is empty or missing required keys.\n    \"\"\"\n    script_dir = os.path.dirname(__file__)\n    script_dir = \"\\\\\".join(script_dir.split(\"\\\\\")[:-1])\n    script_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n    with hydra.initialize(version_base=None,\n                          config_path=\"../../../configs\"):\n        cfg = hydra.compose(config_name=\"config\")\n\n    model_directory = os.path.join(script_dir, cfg.model_directory)\n    model_directory = os.path.abspath(model_directory)\n\n    sbml_path = os.path.join(model_directory, f\"{self.fine_name}.xml\")\n    if not os.path.exists(sbml_path):\n        raise FileNotFoundError(\n            f\"SBML model file not found for model '{self.fine_name}'. \"\n            f\"Expected at: {sbml_path}\"\n        )\n    self.sbml_file_path = sbml_path\n    yaml_file = os.path.join(script_dir, \"../../configs/data\", f\"{self.fine_name}.yaml\")\n    if not os.path.exists(yaml_file):\n        raise FileNotFoundError(f\"YAML file not found: {yaml_file}\")\n\n    with open(yaml_file, 'r', encoding='utf-8') as file:\n        self.config = yaml.safe_load(file)\n\n    if not self.config:\n        raise ValueError(f\"YAML config {yaml_file} is empty or malformed.\")\n\n    required_keys = ['train_duration', 'test_duration', 'val_duration']\n    missing_keys = [key for key in required_keys\n                    if key not in self.config or self.config[key] is None]\n    if missing_keys:\n        raise ValueError(\n            f\"Missing required key(s) in YAML config {yaml_file}: {', '.join(missing_keys)}\"\n        )\n\n    self._is_prepared = True\n</code></pre>"},{"location":"data/sbml_dataloader/#vpeleaderboard.data.src.sbml.sbml_dataloader.SBMLDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Loads the SBML model only after prepare_data has been called.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>Optional[str]</code> <p>The stage of setup, typically used in multi-stage setups.</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>prepare_data()</code> has not been called before <code>setup()</code>.</p> <code>FileNotFoundError</code> <p>If the SBML file is not found.</p> Source code in <code>vpeleaderboard/data/src/sbml/sbml_dataloader.py</code> <pre><code>def setup(self, stage: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Loads the SBML model only after prepare_data has been called.\n\n    Args:\n        stage (Optional[str]): The stage of setup, typically used in multi-stage setups.\n\n    Raises:\n        RuntimeError: If `prepare_data()` has not been called before `setup()`.\n        FileNotFoundError: If the SBML file is not found.\n    \"\"\"\n    if not self._is_prepared:\n        raise RuntimeError(\"You must call `prepare_data()` before `setup()`.\")\n\n    if not os.path.exists(self.sbml_file_path):\n        raise FileNotFoundError(f\"SBML model file not found: {self.sbml_file_path}\")\n\n    self.copasi_model = basico.load_model(self.sbml_file_path)\n</code></pre>"},{"location":"data/sbml_dataloader/#vpeleaderboard.data.src.sbml.sbml_dataloader.SBMLDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Creates the DataLoader for the test dataset based on the SBML simulation results.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The DataLoader for the test dataset.</p> Source code in <code>vpeleaderboard/data/src/sbml/sbml_dataloader.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Creates the DataLoader for the test dataset based on the SBML simulation results.\n\n    Returns:\n        DataLoader: The DataLoader for the test dataset.\n    \"\"\"\n    test_df = basico.run_time_course(\n        duration=self.config['test_duration'],\n        automatic=False,\n        use_initial_values=False,\n        update_model=False\n    )\n    dataset = self.SBMLTimeCourseDataset(test_df)\n    return DataLoader(dataset)\n</code></pre>"},{"location":"data/sbml_dataloader/#vpeleaderboard.data.src.sbml.sbml_dataloader.SBMLDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Creates the DataLoader for the training dataset based on the SBML simulation results.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The DataLoader for the training dataset.</p> Source code in <code>vpeleaderboard/data/src/sbml/sbml_dataloader.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Creates the DataLoader for the training dataset based on the SBML simulation results.\n\n    Returns:\n        DataLoader: The DataLoader for the training dataset.\n    \"\"\"\n    train_df = basico.run_time_course(\n        duration=self.config['train_duration'],\n        use_initial_values=False\n    )\n    dataset = self.SBMLTimeCourseDataset(train_df)\n    return DataLoader(dataset)\n</code></pre>"},{"location":"data/sbml_dataloader/#vpeleaderboard.data.src.sbml.sbml_dataloader.SBMLDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Creates the DataLoader for the validating dataset based on the SBML simulation results.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The DataLoader for the validating dataset.</p> Source code in <code>vpeleaderboard/data/src/sbml/sbml_dataloader.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Creates the DataLoader for the validating dataset based on the SBML simulation results.\n\n    Returns:\n        DataLoader: The DataLoader for the validating dataset.\n    \"\"\"\n    val_df = basico.run_time_course(\n        duration=self.config['val_duration'],\n        use_initial_values=False\n    )\n    dataset = self.SBMLTimeCourseDataset(val_df)\n    return DataLoader(dataset)\n</code></pre>"},{"location":"data/sys_bio_model/","title":"System Bio Model","text":"<p>An abstract base class for Models in the data module.</p>"},{"location":"data/sys_bio_model/#vpeleaderboard.data.src.sbml.sys_bio_model.SysBioModel","title":"<code>SysBioModel</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Abstract base class for Models in the data section, allowing different mathematical approaches to be implemented in subclasses.</p> <p>This class enforces a standard interface for models working with SBML (Systems Biology Markup Language) files.</p> Source code in <code>vpeleaderboard/data/src/sbml/sys_bio_model.py</code> <pre><code>class SysBioModel(BaseModel, ABC):\n    \"\"\"\n    Abstract base class for Models in the data section, allowing\n    different mathematical approaches to be implemented in subclasses.\n\n    This class enforces a standard interface for models working\n    with SBML (Systems Biology Markup Language) files.\n    \"\"\"\n    sbml_file_path: str = Field(..., description=\"Path to an SBML file\")\n    name: Optional[str] = Field(..., description=\"Name of the model\")\n    description: Optional[str] = Field(\"\", description=\"Description of the model\")\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        if not self.sbml_file_path:\n            raise ValueError(\"sbml_file_path must be provided.\")\n\n    @abstractmethod\n    def get_model_metadata(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Abstract method to retrieve metadata of the SBML model.\n\n        This method must be implemented in subclasses to extract and return\n        relevant details about the SBML model, such as its structure, components,\n        and parameters.\n\n        Returns:\n            pd.DataFrame: A pandas DataFrame containing the metadata of the model.\n        \"\"\"\n</code></pre>"},{"location":"data/sys_bio_model/#vpeleaderboard.data.src.sbml.sys_bio_model.SysBioModel.get_model_metadata","title":"<code>get_model_metadata()</code>  <code>abstractmethod</code>","text":"<p>Abstract method to retrieve metadata of the SBML model.</p> <p>This method must be implemented in subclasses to extract and return relevant details about the SBML model, such as its structure, components, and parameters.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A pandas DataFrame containing the metadata of the model.</p> Source code in <code>vpeleaderboard/data/src/sbml/sys_bio_model.py</code> <pre><code>@abstractmethod\ndef get_model_metadata(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Abstract method to retrieve metadata of the SBML model.\n\n    This method must be implemented in subclasses to extract and return\n    relevant details about the SBML model, such as its structure, components,\n    and parameters.\n\n    Returns:\n        pd.DataFrame: A pandas DataFrame containing the metadata of the model.\n    \"\"\"\n</code></pre>"},{"location":"experiment/test/","title":"Test","text":"In\u00a0[31]: Copied! <pre>import hydra\nfrom omegaconf import DictConfig\nimport matplotlib.pyplot as plt\nfrom torch_geometric.utils import to_networkx\nimport networkx as nx\nimport random\nfrom mlflow import MlflowClient\nimport sys\nsys.path.append('..')\n</pre> import hydra from omegaconf import DictConfig import matplotlib.pyplot as plt from torch_geometric.utils import to_networkx import networkx as nx import random from mlflow import MlflowClient import sys sys.path.append('..') In\u00a0[32]: Copied! <pre>model_name = 'gcn'\nwith hydra.initialize(version_base=None, config_path=\"../configs\"):\n    cfg = hydra.compose(config_name='config', overrides=[f'model={model_name}'])\n    cfg.logger.experiment_name = \"test\"\n    cfg.logger.run_name = f\"{model_name}_test\"\n</pre> model_name = 'gcn' with hydra.initialize(version_base=None, config_path=\"../configs\"):     cfg = hydra.compose(config_name='config', overrides=[f'model={model_name}'])     cfg.logger.experiment_name = \"test\"     cfg.logger.run_name = f\"{model_name}_test\" In\u00a0[33]: Copied! <pre># Create a MLflow client object with the tracking URI same as the one in the config\nclient = MlflowClient(tracking_uri=cfg.logger.tracking_uri)\n# Get the experiment name and run name from the config\nexperiment_name = cfg.logger.experiment_name\nrun_name = cfg.logger.run_name\n# Check if an experiment with the same name already exists\nexperiment = client.get_experiment_by_name(experiment_name)\nif experiment: # If the experiment exists\n    print (f\"Experiment with name \\\"{experiment_name}\\\" already exists\")\n</pre> # Create a MLflow client object with the tracking URI same as the one in the config client = MlflowClient(tracking_uri=cfg.logger.tracking_uri) # Get the experiment name and run name from the config experiment_name = cfg.logger.experiment_name run_name = cfg.logger.run_name # Check if an experiment with the same name already exists experiment = client.get_experiment_by_name(experiment_name) if experiment: # If the experiment exists     print (f\"Experiment with name \\\"{experiment_name}\\\" already exists\") <pre>Experiment with name \"test\" already exists\n</pre> In\u00a0[34]: Copied! <pre># Initialize the model object\nmodel_obj = hydra.utils.instantiate(cfg.model)\n\n# Initialize the data object\ncora_dataset = hydra.utils.instantiate(cfg.data)\n</pre> # Initialize the model object model_obj = hydra.utils.instantiate(cfg.model)  # Initialize the data object cora_dataset = hydra.utils.instantiate(cfg.data) In\u00a0[35]: Copied! <pre>cora_dataset.prepare_data()\ngraph = cora_dataset.data[0]\n</pre> cora_dataset.prepare_data() graph = cora_dataset.data[0] In\u00a0[36]: Copied! <pre>def convert_to_networkx(graph, n_sample=None):\n    '''\n    A method that takes in the graph and converts it\n    into networkx format\n    '''\n    g = to_networkx(graph, node_attrs=[\"x\"])\n    y = graph.y.numpy()\n\n    if n_sample is not None:\n        sampled_nodes = random.sample(g.nodes, n_sample)\n        g = g.subgraph(sampled_nodes)\n        y = y[sampled_nodes]\n\n    return g, y\n\n\ndef plot_graph(g, y):\n    '''\n    Plot tht networkx graph\n    '''\n    plt.figure(figsize=(9, 7))\n    nx.draw_spring(g, node_size=30, arrows=False, node_color=y)\n    # plt.show()\n    plt.savefig(\"graph.png\")\n\n# conver the graph into nextworkx format\ng, y = convert_to_networkx(graph, n_sample=1000)\n# plot the graph in networkx format\nplot_graph(g, y)\n</pre> def convert_to_networkx(graph, n_sample=None):     '''     A method that takes in the graph and converts it     into networkx format     '''     g = to_networkx(graph, node_attrs=[\"x\"])     y = graph.y.numpy()      if n_sample is not None:         sampled_nodes = random.sample(g.nodes, n_sample)         g = g.subgraph(sampled_nodes)         y = y[sampled_nodes]      return g, y   def plot_graph(g, y):     '''     Plot tht networkx graph     '''     plt.figure(figsize=(9, 7))     nx.draw_spring(g, node_size=30, arrows=False, node_color=y)     # plt.show()     plt.savefig(\"graph.png\")  # conver the graph into nextworkx format g, y = convert_to_networkx(graph, n_sample=1000) # plot the graph in networkx format plot_graph(g, y) <pre>/tmp/ipykernel_1529/2832839476.py:10: DeprecationWarning: Sampling from a set deprecated\nsince Python 3.9 and will be removed in a subsequent version.\n  sampled_nodes = random.sample(g.nodes, n_sample)\n</pre> In\u00a0[37]: Copied! <pre>trainer = hydra.utils.instantiate(cfg.trainer, logger=cfg.logger)\ntrainer.logger.log_hyperparams({'data': cfg.data,\n                                'model': cfg.model,\n                                'trainer': cfg.trainer})\nexperiment_id = trainer.logger.experiment_id\n</pre> trainer = hydra.utils.instantiate(cfg.trainer, logger=cfg.logger) trainer.logger.log_hyperparams({'data': cfg.data,                                 'model': cfg.model,                                 'trainer': cfg.trainer}) experiment_id = trainer.logger.experiment_id <pre>GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available: False, using: 0 HPUs\n</pre> In\u00a0[38]: Copied! <pre># Extract the run id\nrun_id = trainer.logger.run_id\n# Log the artifact graph.png\n# replace graph.png with the path to the image/file you want to log\ntrainer.logger.experiment.log_artifact(run_id, \"graph.png\")\n</pre> # Extract the run id run_id = trainer.logger.run_id # Log the artifact graph.png # replace graph.png with the path to the image/file you want to log trainer.logger.experiment.log_artifact(run_id, \"graph.png\") In\u00a0[39]: Copied! <pre>trainer.fit(model=model_obj, datamodule=cora_dataset)\n</pre> trainer.fit(model=model_obj, datamodule=cora_dataset) <pre>\n  | Name       | Type             | Params\n------------------------------------------------\n0 | conv1      | GCNConv          | 22.9 K\n1 | conv2      | GCNConv          | 119   \n2 | criterion  | CrossEntropyLoss | 0     \n3 | train_loss | MeanMetric       | 0     \n4 | val_loss   | MeanMetric       | 0     \n5 | test_loss  | MeanMetric       | 0     \n------------------------------------------------\n23.1 K    Trainable params\n0         Non-trainable params\n23.1 K    Total params\n0.092     Total estimated model params size (MB)\n</pre> <pre>Sanity Checking DataLoader 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 113.21it/s]val loss: 1.9494456052780151\nEpoch 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 46.49it/s, v_num=38fe]          train loss: 1.954062581062317\nEpoch 1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 48.10it/s, v_num=38fe]train loss: 1.9471311569213867\nEpoch 2: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 47.22it/s, v_num=38fe]train loss: 1.9404144287109375\nEpoch 3: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 47.15it/s, v_num=38fe]train loss: 1.9338120222091675\nEpoch 4: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 44.28it/s, v_num=38fe]train loss: 1.927283525466919\nEpoch 5: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 44.60it/s, v_num=38fe]train loss: 1.9207806587219238\nEpoch 6: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 43.78it/s, v_num=38fe]train loss: 1.9142428636550903\nEpoch 7: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 47.77it/s, v_num=38fe]train loss: 1.9076226949691772\nEpoch 8: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 47.50it/s, v_num=38fe]train loss: 1.900881290435791\nEpoch 9: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 49.54it/s, v_num=38fe]val loss: 1.873719334602356\nEpoch 9: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 14.66it/s, v_num=38fe]train loss: 1.894006371498108\nEpoch 10: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 48.63it/s, v_num=38fe]train loss: 1.8869781494140625\nEpoch 11: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 45.30it/s, v_num=38fe]train loss: 1.8797813653945923\nEpoch 12: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 47.02it/s, v_num=38fe]train loss: 1.872401475906372\nEpoch 13: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 41.39it/s, v_num=38fe]train loss: 1.864835500717163\nEpoch 14: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 52.10it/s, v_num=38fe]train loss: 1.857094407081604\nEpoch 15: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 43.48it/s, v_num=38fe]train loss: 1.8491896390914917\nEpoch 16: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 40.66it/s, v_num=38fe]train loss: 1.84113347530365\nEpoch 17: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 42.33it/s, v_num=38fe]train loss: 1.8329412937164307\nEpoch 18: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 46.07it/s, v_num=38fe]train loss: 1.8246253728866577\nEpoch 19: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 54.41it/s, v_num=38fe]val loss: 1.827527403831482\nEpoch 19: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 15.17it/s, v_num=38fe]train loss: 1.8161979913711548\nEpoch 20: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 45.49it/s, v_num=38fe]train loss: 1.8076695203781128\nEpoch 21: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 45.08it/s, v_num=38fe]train loss: 1.7990460395812988\nEpoch 22: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 47.56it/s, v_num=38fe]train loss: 1.79033625125885\nEpoch 23: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 46.45it/s, v_num=38fe]train loss: 1.7815455198287964\nEpoch 24: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 45.89it/s, v_num=38fe]train loss: 1.7726805210113525\nEpoch 25: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 44.43it/s, v_num=38fe]train loss: 1.7637462615966797\nEpoch 26: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 50.85it/s, v_num=38fe]train loss: 1.7547491788864136\nEpoch 27: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 53.42it/s, v_num=38fe]train loss: 1.7456934452056885\nEpoch 28: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 50.58it/s, v_num=38fe]train loss: 1.736584186553955\nEpoch 29: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 50.46it/s, v_num=38fe]val loss: 1.7792142629623413\nEpoch 29: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 14.06it/s, v_num=38fe]train loss: 1.7274270057678223\nEpoch 30: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 43.61it/s, v_num=38fe]train loss: 1.718226671218872\nEpoch 31: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 47.49it/s, v_num=38fe]train loss: 1.7089877128601074\nEpoch 32: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 48.53it/s, v_num=38fe]train loss: 1.6997146606445312\nEpoch 33: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 43.26it/s, v_num=38fe]train loss: 1.6904124021530151\nEpoch 34: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 46.56it/s, v_num=38fe]train loss: 1.6810860633850098\nEpoch 35: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 49.69it/s, v_num=38fe]train loss: 1.6717400550842285\nEpoch 36: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 45.33it/s, v_num=38fe]train loss: 1.6623783111572266\nEpoch 37: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 46.98it/s, v_num=38fe]train loss: 1.65300452709198\nEpoch 38: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 48.15it/s, v_num=38fe]train loss: 1.6436220407485962\nEpoch 39: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 47.34it/s, v_num=38fe]val loss: 1.7293224334716797\nEpoch 39: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 14.51it/s, v_num=38fe]train loss: 1.634233832359314\nEpoch 40: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 47.77it/s, v_num=38fe]train loss: 1.6248433589935303\nEpoch 41: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 45.76it/s, v_num=38fe]train loss: 1.6154531240463257\nEpoch 42: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 47.73it/s, v_num=38fe]train loss: 1.6060657501220703\nEpoch 43: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 42.06it/s, v_num=38fe]train loss: 1.596684217453003\nEpoch 44: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 46.99it/s, v_num=38fe]train loss: 1.587310791015625\nEpoch 45: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 46.31it/s, v_num=38fe]train loss: 1.5779482126235962\nEpoch 46: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 38.00it/s, v_num=38fe]train loss: 1.568598985671997\nEpoch 47: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 48.16it/s, v_num=38fe]train loss: 1.5592652559280396\nEpoch 48: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 44.54it/s, v_num=38fe]train loss: 1.549949049949646\nEpoch 49: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 52.21it/s, v_num=38fe]val loss: 1.6795060634613037\nEpoch 49: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 14.83it/s, v_num=38fe]train loss: 1.5406520366668701\nEpoch 50: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 52.14it/s, v_num=38fe]train loss: 1.5313762426376343\nEpoch 51: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 43.64it/s, v_num=38fe]train loss: 1.5221234560012817\nEpoch 52: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 49.67it/s, v_num=38fe]train loss: 1.5128955841064453\nEpoch 53: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 45.35it/s, v_num=38fe]train loss: 1.5036940574645996\nEpoch 54: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 45.87it/s, v_num=38fe]train loss: 1.494520664215088\nEpoch 55: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 41.95it/s, v_num=38fe]train loss: 1.4853771924972534\nEpoch 56: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 42.45it/s, v_num=38fe]train loss: 1.4762650728225708\nEpoch 57: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 45.31it/s, v_num=38fe]train loss: 1.4671859741210938\nEpoch 58: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 44.27it/s, v_num=38fe]train loss: 1.4581412076950073\nEpoch 59: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 48.47it/s, v_num=38fe]val loss: 1.6304421424865723\nEpoch 59: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 14.35it/s, v_num=38fe]train loss: 1.4491324424743652\nEpoch 60: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 45.13it/s, v_num=38fe]train loss: 1.4401606321334839\nEpoch 61: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 44.18it/s, v_num=38fe]train loss: 1.431227684020996\nEpoch 62: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 46.11it/s, v_num=38fe]train loss: 1.4223344326019287\nEpoch 63: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 44.15it/s, v_num=38fe]train loss: 1.4134823083877563\nEpoch 64: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 44.34it/s, v_num=38fe]train loss: 1.4046725034713745\nEpoch 65: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 48.45it/s, v_num=38fe]train loss: 1.3959062099456787\nEpoch 66: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 43.20it/s, v_num=38fe]train loss: 1.3871842622756958\nEpoch 67: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 50.11it/s, v_num=38fe]train loss: 1.3785077333450317\nEpoch 68: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 44.88it/s, v_num=38fe]train loss: 1.3698776960372925\nEpoch 69: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 51.07it/s, v_num=38fe]val loss: 1.5825204849243164\nEpoch 69: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 14.56it/s, v_num=38fe]train loss: 1.3612948656082153\nEpoch 70: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 49.03it/s, v_num=38fe]train loss: 1.3527601957321167\nEpoch 71: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 43.71it/s, v_num=38fe]train loss: 1.3442747592926025\nEpoch 72: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 52.52it/s, v_num=38fe]train loss: 1.3358391523361206\nEpoch 73: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 47.62it/s, v_num=38fe]train loss: 1.3274542093276978\nEpoch 74: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 42.97it/s, v_num=38fe]train loss: 1.3191207647323608\nEpoch 75: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 43.73it/s, v_num=38fe]train loss: 1.3108394145965576\nEpoch 76: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 47.64it/s, v_num=38fe]train loss: 1.3026108741760254\nEpoch 77: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 40.72it/s, v_num=38fe]train loss: 1.2944358587265015\nEpoch 78: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 39.55it/s, v_num=38fe]train loss: 1.286314845085144\nEpoch 79: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 42.68it/s, v_num=38fe]val loss: 1.5362273454666138\nEpoch 79: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 13.21it/s, v_num=38fe]train loss: 1.2782484292984009\nEpoch 80: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 40.21it/s, v_num=38fe]train loss: 1.2702369689941406\nEpoch 81: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 39.79it/s, v_num=38fe]train loss: 1.262281060218811\nEpoch 82: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 42.37it/s, v_num=38fe]train loss: 1.2543810606002808\nEpoch 83: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 42.41it/s, v_num=38fe]train loss: 1.246537208557129\nEpoch 84: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 45.24it/s, v_num=38fe]train loss: 1.2387501001358032\nEpoch 85: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 45.00it/s, v_num=38fe]train loss: 1.2310199737548828\nEpoch 86: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 39.96it/s, v_num=38fe]train loss: 1.2233470678329468\nEpoch 87: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 37.71it/s, v_num=38fe]train loss: 1.2157316207885742\nEpoch 88: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 47.43it/s, v_num=38fe]train loss: 1.2081738710403442\nEpoch 89: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 50.26it/s, v_num=38fe]val loss: 1.492038607597351\nEpoch 89: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 11.92it/s, v_num=38fe]train loss: 1.2006739377975464\nEpoch 90: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 29.84it/s, v_num=38fe]train loss: 1.1932320594787598\nEpoch 91: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 46.24it/s, v_num=38fe]train loss: 1.1858481168746948\nEpoch 92: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 45.30it/s, v_num=38fe]train loss: 1.1785224676132202\nEpoch 93: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 49.97it/s, v_num=38fe]train loss: 1.171255111694336\nEpoch 94: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 38.60it/s, v_num=38fe]train loss: 1.1640461683273315\nEpoch 95: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 45.73it/s, v_num=38fe]train loss: 1.1568952798843384\nEpoch 96: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 47.09it/s, v_num=38fe]train loss: 1.149802803993225\nEpoch 97: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 43.92it/s, v_num=38fe]train loss: 1.1427686214447021\nEpoch 98: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 44.31it/s, v_num=38fe]train loss: 1.13579261302948\nEpoch 99: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 48.23it/s, v_num=38fe]val loss: 1.4503182172775269\nEpoch 99: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 14.36it/s, v_num=38fe]</pre> <pre>`Trainer.fit` stopped: `max_epochs=100` reached.\n</pre> <pre>train loss: 1.1288747787475586\nEpoch 99: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 13.18it/s, v_num=38fe]\n</pre> In\u00a0[40]: Copied! <pre># get test_loss\nprint (trainer.test(model=model_obj, datamodule=cora_dataset))\n</pre> # get test_loss print (trainer.test(model=model_obj, datamodule=cora_dataset)) <pre>Testing: |          | 0/? [00:00&lt;?, ?it/s]</pre> <pre>Testing DataLoader 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 82.73it/s]test loss: 1.0347048044204712\nTesting DataLoader 0: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 54.23it/s]\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n       Test metric             DataLoader 0\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        test_loss           1.0347048044204712\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n[{'test_loss': 1.0347048044204712}]\n</pre>"},{"location":"experiment/test/#import-the-required-libraries","title":"Import the required libraries\u00b6","text":""},{"location":"experiment/test/#initialize-the-hydra-object","title":"Initialize the hydra object\u00b6","text":"<p>The following cell initializes the hydra object by telling it that:</p> <ol> <li>All the configs are stored in the folder configs.</li> <li>Within the folder configs, it should look into the yaml file config.yaml for further instructions</li> </ol> <p>Please note following is the way to initialize hydra object in jupyter noteboook. To do this in vs code editor, you will have to do</p>"},{"location":"experiment/test/#mlflow-set-up","title":"MLflow set up\u00b6","text":"<p>In the following cell, we create an MLflowClient to test whether an experiment with the same name exists or not, and notify the user.</p>"},{"location":"experiment/test/#instantiate-the-source-code","title":"Instantiate the source code\u00b6","text":"<ol> <li>Within the app folder, we have defined 2 subfolders - model and data</li> <li>While model contains the source for model-related modules (e.g.: MLP, GCN), data contains the source code to define the data-related modules (e.g.: CORA, MNIST).</li> <li>The default parameters of their source code is defined in configs &gt; model &gt; *.yaml and configs &gt; data &gt; *.yaml</li> <li>The yaml files also contain the path to the source code (check out the first line <code>_target_</code>)</li> <li>In the following cell, we instantiate the source code (in other words we initialize an object of the 2 classes and use the default parameters to initialize them)</li> </ol>"},{"location":"experiment/test/#preparedownload-data","title":"Prepare/download data\u00b6","text":"<p>Once you have intialized the datamodule, it's time to prepare the data (in other words download the data). Therefore, we call the function prepare_data(). Please note that you don't have to call this function if you were to just do the ML because the trainer method below takes care of that by itself. But we call this function here since we want to visualize the graph.</p>"},{"location":"experiment/test/#visualize-the-graph","title":"Visualize the graph\u00b6","text":"<p>In this step, we visualize the graph by defining some functions that help us do so.</p>"},{"location":"experiment/test/#initialize-the-trainer","title":"Initialize the trainer\u00b6","text":"<p>Once we have initialized the model and data, and visualized the graph (optional), we will now initialize the trainer. Please note that for the trainer, we don't need to write source code. We can just directly use the code provided by lightning (check out <code>_target</code> of configs &gt; trainer &gt; default.yaml). If you are unfamiliar with trainer, please checkout its description on lightning's website.</p>"},{"location":"experiment/test/#save-artifacts","title":"Save artifacts\u00b6","text":"<p>Now that the trainer is initialized, the experiment and run have been logged into MLflow. You can henceforth use the trainer object to access the logger (e.g. save artifacts)</p>"},{"location":"experiment/test/#fit-the-trainer-using-the-model-and-data","title":"Fit the trainer using the model and data\u00b6","text":"<p>Please note that in case of the CORA dataset, pytorch library has already masked values of the data into <code>train</code>, <code>val</code>, and <code>test</code>. We just need to inform the model about these masks. Checkout the method <code>model_step</code> in the model code.</p> <p>Therefore, we can provide the same data to both train_loader and val_loader, and let the model code figure out itself.</p>"},{"location":"leaderboard/","title":"Leaderboard","text":"Algorithm Metrics <p>This page was last updated on 2025-06-09 09:43:10 UTC</p>"},{"location":"leaderboard/#algorithm_metrics","title":"Algorithm Metrics Table","text":"Rank Algorithm Name Task/Data Metric 1 Metric 2 Metric 3 \ud83c\udfc5 1 Algo 1 Task A 0.85 0.75 0.65 \ud83c\udfc5 2 Algo 2 Task B 0.87 0.77 0.67 \ud83c\udfc5 3 Algo 3 Task C 0.89 0.79 0.69 \ud83c\udfc5 4 Algo 4 Task D 0.91 0.81 0.71 \ud83c\udfc5 5 Algo 5 Task E 0.93 0.83 0.73 \ud83c\udfc5 6 Algo 6 Task A 0.85 0.75 0.65 \ud83c\udfc5 7 Algo 7 Task B 0.87 0.77 0.67 \ud83c\udfc5 8 Algo 8 Task C 0.89 0.79 0.69 \ud83c\udfc5 9 Algo 9 Task D 0.91 0.81 0.71 \ud83c\udfc5 10 Algo 10 Task E 0.93 0.83 0.73 \ud83c\udfc5 11 Algo 11 Task A 0.85 0.75 0.65 \ud83c\udfc5 12 Algo 12 Task B 0.87 0.77 0.67 \ud83c\udfc5 13 Algo 13 Task C 0.89 0.79 0.69 \ud83c\udfc5 14 Algo 14 Task D 0.91 0.81 0.71 \ud83c\udfc5 15 Algo 15 Task E 0.93 0.83 0.73 \ud83c\udfc5 16 Algo 16 Task A 0.85 0.75 0.65 \ud83c\udfc5 17 Algo 17 Task B 0.87 0.77 0.67 \ud83c\udfc5 18 Algo 18 Task C 0.89 0.79 0.69 \ud83c\udfc5 19 Algo 19 Task D 0.91 0.81 0.71 \ud83c\udfc5 20 Algo 20 Task E 0.93 0.83 0.73"},{"location":"notebooks/KG_dataloader/","title":"Knowledge Graph (KG) Dataloader with Lightning","text":"<p>This guide shows how to load and use a Knowledge Graph dataset with the PyTorch Lightning LightningDataModule.</p> <p>To load and simulate data from the KG model, follow the steps below:</p> <p>Step 1: \ud83d\udce6 Import the module</p> In\u00a0[1]: Copied! <pre>import sys\nimport os\n\n# Go up to the root where `vpeleaderboard/` is located\nsys.path.append(os.path.abspath(\"../../\"))\n</pre> import sys import os  # Go up to the root where `vpeleaderboard/` is located sys.path.append(os.path.abspath(\"../../\")) In\u00a0[2]: Copied! <pre>from vpeleaderboard.data.src.kg.biobridge_datamodule_hetero import BioBridgeDataModule\n</pre> from vpeleaderboard.data.src.kg.biobridge_datamodule_hetero import BioBridgeDataModule <p>Step 2: \u2699\ufe0f Initialize the KGDataModule</p> <p>Specify the data directories and initialize the BioBridgeDataModule:</p> In\u00a0[3]: Copied! <pre># Initial\n# ize with paths to PrimeKG and BioBridge datasets\n# dm = BioBridgeDataModule(\n#     primekg_dir=\"../../../data/primekg/\",\n#     biobridge_dir=\"../../../data/biobridge_primekg/\",\n#     batch_size=8\n# )\nimport hydra\nwith hydra.initialize(config_path=\"../../vpeleaderboard/configs\", version_base=None):\n    cfg = hydra.compose(config_name=\"config\")\n    dm = BioBridgeDataModule(cfg)\n</pre> # Initial # ize with paths to PrimeKG and BioBridge datasets # dm = BioBridgeDataModule( #     primekg_dir=\"../../../data/primekg/\", #     biobridge_dir=\"../../../data/biobridge_primekg/\", #     batch_size=8 # ) import hydra with hydra.initialize(config_path=\"../../vpeleaderboard/configs\", version_base=None):     cfg = hydra.compose(config_name=\"config\")     dm = BioBridgeDataModule(cfg)  <p>Step 3: \ud83e\uddf9 Prepare data</p> <p>Prepare the KG data by loading and caching it:</p> In\u00a0[4]: Copied! <pre># Load data, embeddings, and node/edge mappings\ndm.prepare_data()\n</pre> # Load data, embeddings, and node/edge mappings dm.prepare_data() <pre>\ud83d\udd01 Loading cached data from ./biobridge_cache.pkl\n</pre> <p>Step 4: \ud83e\udde0 Setup the data splits</p> <p>Split the data into training, validation, and test sets:</p> In\u00a0[5]: Copied! <pre># Build HeteroData and apply RandomLinkSplit\ndm.setup()\n</pre> # Build HeteroData and apply RandomLinkSplit dm.setup() <p>Step 5: \ud83e\uddea Access the dataloaders</p> <p>Now, you can retrieve the standard Lightning dataloaders:</p> In\u00a0[6]: Copied! <pre>print(dm.data.keys())  # Should include 'train', 'val', 'test' if setup worked\n</pre>  print(dm.data.keys())  # Should include 'train', 'val', 'test' if setup worked  <pre>dict_keys(['nt2ntid', 'ntid2nt', 'init', 'train', 'val', 'test'])\n</pre> <p>Training data</p> In\u00a0[7]: Copied! <pre>train_loader = dm.train_dataloader()\ntrain_batch = next(iter(train_loader))\nprint(train_batch)\n</pre> train_loader = dm.train_dataloader() train_batch = next(iter(train_loader)) print(train_batch)  <pre>HeteroDataBatch(\n  biological_process={\n    num_nodes=27409,\n    x=[27409, 768],\n    node_name=[1],\n    batch=[27409],\n    ptr=[2],\n  },\n  cellular_component={\n    num_nodes=4011,\n    x=[4011, 768],\n    node_name=[1],\n    batch=[4011],\n    ptr=[2],\n  },\n  disease={\n    num_nodes=17054,\n    x=[17054, 768],\n    node_name=[1],\n    batch=[17054],\n    ptr=[2],\n  },\n  drug={\n    num_nodes=6759,\n    x=[6759, 512],\n    node_name=[1],\n    batch=[6759],\n    ptr=[2],\n  },\n  gene/protein={\n    num_nodes=18797,\n    x=[18797, 2560],\n    node_name=[1],\n    batch=[18797],\n    ptr=[2],\n  },\n  molecular_function={\n    num_nodes=10951,\n    x=[10951, 768],\n    node_name=[1],\n    batch=[10951],\n    ptr=[2],\n  },\n  (gene/protein, ppi, gene/protein)={\n    edge_index=[2, 440447],\n    pos_edge_label=[440447],\n    pos_edge_label_index=[2, 440447],\n    neg_edge_label=[440447],\n    neg_edge_label_index=[2, 440447],\n  },\n  (drug, carrier, gene/protein)={\n    edge_index=[2, 571],\n    pos_edge_label=[571],\n    pos_edge_label_index=[2, 571],\n    neg_edge_label=[571],\n    neg_edge_label_index=[2, 571],\n  },\n  (drug, enzyme, gene/protein)={\n    edge_index=[2, 3542],\n    pos_edge_label=[3542],\n    pos_edge_label_index=[2, 3542],\n    neg_edge_label=[3542],\n    neg_edge_label_index=[2, 3542],\n  },\n  (drug, target, gene/protein)={\n    edge_index=[2, 10444],\n    pos_edge_label=[10444],\n    pos_edge_label_index=[2, 10444],\n    neg_edge_label=[10444],\n    neg_edge_label_index=[2, 10444],\n  },\n  (drug, transporter, gene/protein)={\n    edge_index=[2, 2065],\n    pos_edge_label=[2065],\n    pos_edge_label_index=[2, 2065],\n    neg_edge_label=[2065],\n    neg_edge_label_index=[2, 2065],\n  },\n  (drug, contraindication, disease)={\n    edge_index=[2, 21028],\n    pos_edge_label=[21028],\n    pos_edge_label_index=[2, 21028],\n    neg_edge_label=[21028],\n    neg_edge_label_index=[2, 21028],\n  },\n  (drug, indication, disease)={\n    edge_index=[2, 6123],\n    pos_edge_label=[6123],\n    pos_edge_label_index=[2, 6123],\n    neg_edge_label=[6123],\n    neg_edge_label_index=[2, 6123],\n  },\n  (drug, off-label use, disease)={\n    edge_index=[2, 1712],\n    pos_edge_label=[1712],\n    pos_edge_label_index=[2, 1712],\n    neg_edge_label=[1712],\n    neg_edge_label_index=[2, 1712],\n  },\n  (drug, synergistic interaction, drug)={\n    edge_index=[2, 1563945],\n    pos_edge_label=[1563945],\n    pos_edge_label_index=[2, 1563945],\n    neg_edge_label=[1563945],\n    neg_edge_label_index=[2, 1563945],\n  },\n  (gene/protein, associated with, disease)={\n    edge_index=[2, 54574],\n    pos_edge_label=[54574],\n    pos_edge_label_index=[2, 54574],\n    neg_edge_label=[54574],\n    neg_edge_label_index=[2, 54574],\n  },\n  (disease, parent-child, disease)={\n    edge_index=[2, 45073],\n    pos_edge_label=[45073],\n    pos_edge_label_index=[2, 45073],\n    neg_edge_label=[45073],\n    neg_edge_label_index=[2, 45073],\n  },\n  (biological_process, parent-child, biological_process)={\n    edge_index=[2, 69741],\n    pos_edge_label=[69741],\n    pos_edge_label_index=[2, 69741],\n    neg_edge_label=[69741],\n    neg_edge_label_index=[2, 69741],\n  },\n  (molecular_function, parent-child, molecular_function)={\n    edge_index=[2, 18506],\n    pos_edge_label=[18506],\n    pos_edge_label_index=[2, 18506],\n    neg_edge_label=[18506],\n    neg_edge_label_index=[2, 18506],\n  },\n  (cellular_component, parent-child, cellular_component)={\n    edge_index=[2, 6440],\n    pos_edge_label=[6440],\n    pos_edge_label_index=[2, 6440],\n    neg_edge_label=[6440],\n    neg_edge_label_index=[2, 6440],\n  },\n  (gene/protein, interacts with, molecular_function)={\n    edge_index=[2, 46733],\n    pos_edge_label=[46733],\n    pos_edge_label_index=[2, 46733],\n    neg_edge_label=[46733],\n    neg_edge_label_index=[2, 46733],\n  },\n  (gene/protein, interacts with, cellular_component)={\n    edge_index=[2, 52327],\n    pos_edge_label=[52327],\n    pos_edge_label_index=[2, 52327],\n    neg_edge_label=[52327],\n    neg_edge_label_index=[2, 52327],\n  },\n  (gene/protein, interacts with, biological_process)={\n    edge_index=[2, 95425],\n    pos_edge_label=[95425],\n    pos_edge_label_index=[2, 95425],\n    neg_edge_label=[95425],\n    neg_edge_label_index=[2, 95425],\n  },\n  (gene/protein, carrier, drug)={\n    edge_index=[2, 571],\n    pos_edge_label=[571],\n    pos_edge_label_index=[2, 571],\n    neg_edge_label=[571],\n    neg_edge_label_index=[2, 571],\n  },\n  (gene/protein, enzyme, drug)={\n    edge_index=[2, 3542],\n    pos_edge_label=[3542],\n    pos_edge_label_index=[2, 3542],\n    neg_edge_label=[3542],\n    neg_edge_label_index=[2, 3542],\n  },\n  (gene/protein, target, drug)={\n    edge_index=[2, 10444],\n    pos_edge_label=[10444],\n    pos_edge_label_index=[2, 10444],\n    neg_edge_label=[10444],\n    neg_edge_label_index=[2, 10444],\n  },\n  (gene/protein, transporter, drug)={\n    edge_index=[2, 2065],\n    pos_edge_label=[2065],\n    pos_edge_label_index=[2, 2065],\n    neg_edge_label=[2065],\n    neg_edge_label_index=[2, 2065],\n  },\n  (disease, contraindication, drug)={\n    edge_index=[2, 21028],\n    pos_edge_label=[21028],\n    pos_edge_label_index=[2, 21028],\n    neg_edge_label=[21028],\n    neg_edge_label_index=[2, 21028],\n  },\n  (disease, indication, drug)={\n    edge_index=[2, 6123],\n    pos_edge_label=[6123],\n    pos_edge_label_index=[2, 6123],\n    neg_edge_label=[6123],\n    neg_edge_label_index=[2, 6123],\n  },\n  (disease, off-label use, drug)={\n    edge_index=[2, 1712],\n    pos_edge_label=[1712],\n    pos_edge_label_index=[2, 1712],\n    neg_edge_label=[1712],\n    neg_edge_label_index=[2, 1712],\n  },\n  (disease, associated with, gene/protein)={\n    edge_index=[2, 54574],\n    pos_edge_label=[54574],\n    pos_edge_label_index=[2, 54574],\n    neg_edge_label=[54574],\n    neg_edge_label_index=[2, 54574],\n  },\n  (molecular_function, interacts with, gene/protein)={\n    edge_index=[2, 46733],\n    pos_edge_label=[46733],\n    pos_edge_label_index=[2, 46733],\n    neg_edge_label=[46733],\n    neg_edge_label_index=[2, 46733],\n  },\n  (cellular_component, interacts with, gene/protein)={\n    edge_index=[2, 52327],\n    pos_edge_label=[52327],\n    pos_edge_label_index=[2, 52327],\n    neg_edge_label=[52327],\n    neg_edge_label_index=[2, 52327],\n  },\n  (biological_process, interacts with, gene/protein)={\n    edge_index=[2, 95425],\n    pos_edge_label=[95425],\n    pos_edge_label_index=[2, 95425],\n    neg_edge_label=[95425],\n    neg_edge_label_index=[2, 95425],\n  }\n)\n</pre> <p>Validation data</p> In\u00a0[8]: Copied! <pre>val_loader = dm.val_dataloader()\nval_batch = next(iter(val_loader))\nprint(val_batch)\n</pre> val_loader = dm.val_dataloader() val_batch = next(iter(val_loader)) print(val_batch)  <pre>HeteroDataBatch(\n  biological_process={\n    num_nodes=27409,\n    x=[27409, 768],\n    node_name=[1],\n    batch=[27409],\n    ptr=[2],\n  },\n  cellular_component={\n    num_nodes=4011,\n    x=[4011, 768],\n    node_name=[1],\n    batch=[4011],\n    ptr=[2],\n  },\n  disease={\n    num_nodes=17054,\n    x=[17054, 768],\n    node_name=[1],\n    batch=[17054],\n    ptr=[2],\n  },\n  drug={\n    num_nodes=6759,\n    x=[6759, 512],\n    node_name=[1],\n    batch=[6759],\n    ptr=[2],\n  },\n  gene/protein={\n    num_nodes=18797,\n    x=[18797, 2560],\n    node_name=[1],\n    batch=[18797],\n    ptr=[2],\n  },\n  molecular_function={\n    num_nodes=10951,\n    x=[10951, 768],\n    node_name=[1],\n    batch=[10951],\n    ptr=[2],\n  },\n  (gene/protein, ppi, gene/protein)={\n    edge_index=[2, 440447],\n    pos_edge_label=[62920],\n    pos_edge_label_index=[2, 62920],\n    neg_edge_label=[62920],\n    neg_edge_label_index=[2, 62920],\n  },\n  (drug, carrier, gene/protein)={\n    edge_index=[2, 571],\n    pos_edge_label=[81],\n    pos_edge_label_index=[2, 81],\n    neg_edge_label=[81],\n    neg_edge_label_index=[2, 81],\n  },\n  (drug, enzyme, gene/protein)={\n    edge_index=[2, 3542],\n    pos_edge_label=[506],\n    pos_edge_label_index=[2, 506],\n    neg_edge_label=[506],\n    neg_edge_label_index=[2, 506],\n  },\n  (drug, target, gene/protein)={\n    edge_index=[2, 10444],\n    pos_edge_label=[1492],\n    pos_edge_label_index=[2, 1492],\n    neg_edge_label=[1492],\n    neg_edge_label_index=[2, 1492],\n  },\n  (drug, transporter, gene/protein)={\n    edge_index=[2, 2065],\n    pos_edge_label=[295],\n    pos_edge_label_index=[2, 295],\n    neg_edge_label=[295],\n    neg_edge_label_index=[2, 295],\n  },\n  (drug, contraindication, disease)={\n    edge_index=[2, 21028],\n    pos_edge_label=[3004],\n    pos_edge_label_index=[2, 3004],\n    neg_edge_label=[3004],\n    neg_edge_label_index=[2, 3004],\n  },\n  (drug, indication, disease)={\n    edge_index=[2, 6123],\n    pos_edge_label=[874],\n    pos_edge_label_index=[2, 874],\n    neg_edge_label=[874],\n    neg_edge_label_index=[2, 874],\n  },\n  (drug, off-label use, disease)={\n    edge_index=[2, 1712],\n    pos_edge_label=[244],\n    pos_edge_label_index=[2, 244],\n    neg_edge_label=[244],\n    neg_edge_label_index=[2, 244],\n  },\n  (drug, synergistic interaction, drug)={\n    edge_index=[2, 1563945],\n    pos_edge_label=[223420],\n    pos_edge_label_index=[2, 223420],\n    neg_edge_label=[223420],\n    neg_edge_label_index=[2, 223420],\n  },\n  (gene/protein, associated with, disease)={\n    edge_index=[2, 54574],\n    pos_edge_label=[7796],\n    pos_edge_label_index=[2, 7796],\n    neg_edge_label=[7796],\n    neg_edge_label_index=[2, 7796],\n  },\n  (disease, parent-child, disease)={\n    edge_index=[2, 45073],\n    pos_edge_label=[6438],\n    pos_edge_label_index=[2, 6438],\n    neg_edge_label=[6438],\n    neg_edge_label_index=[2, 6438],\n  },\n  (biological_process, parent-child, biological_process)={\n    edge_index=[2, 69741],\n    pos_edge_label=[9963],\n    pos_edge_label_index=[2, 9963],\n    neg_edge_label=[9963],\n    neg_edge_label_index=[2, 9963],\n  },\n  (molecular_function, parent-child, molecular_function)={\n    edge_index=[2, 18506],\n    pos_edge_label=[2643],\n    pos_edge_label_index=[2, 2643],\n    neg_edge_label=[2643],\n    neg_edge_label_index=[2, 2643],\n  },\n  (cellular_component, parent-child, cellular_component)={\n    edge_index=[2, 6440],\n    pos_edge_label=[920],\n    pos_edge_label_index=[2, 920],\n    neg_edge_label=[920],\n    neg_edge_label_index=[2, 920],\n  },\n  (gene/protein, interacts with, molecular_function)={\n    edge_index=[2, 46733],\n    pos_edge_label=[6676],\n    pos_edge_label_index=[2, 6676],\n    neg_edge_label=[6676],\n    neg_edge_label_index=[2, 6676],\n  },\n  (gene/protein, interacts with, cellular_component)={\n    edge_index=[2, 52327],\n    pos_edge_label=[7475],\n    pos_edge_label_index=[2, 7475],\n    neg_edge_label=[7475],\n    neg_edge_label_index=[2, 7475],\n  },\n  (gene/protein, interacts with, biological_process)={\n    edge_index=[2, 95425],\n    pos_edge_label=[13632],\n    pos_edge_label_index=[2, 13632],\n    neg_edge_label=[13632],\n    neg_edge_label_index=[2, 13632],\n  },\n  (gene/protein, carrier, drug)={\n    edge_index=[2, 571],\n    pos_edge_label=[81],\n    pos_edge_label_index=[2, 81],\n    neg_edge_label=[81],\n    neg_edge_label_index=[2, 81],\n  },\n  (gene/protein, enzyme, drug)={\n    edge_index=[2, 3542],\n    pos_edge_label=[506],\n    pos_edge_label_index=[2, 506],\n    neg_edge_label=[506],\n    neg_edge_label_index=[2, 506],\n  },\n  (gene/protein, target, drug)={\n    edge_index=[2, 10444],\n    pos_edge_label=[1492],\n    pos_edge_label_index=[2, 1492],\n    neg_edge_label=[1492],\n    neg_edge_label_index=[2, 1492],\n  },\n  (gene/protein, transporter, drug)={\n    edge_index=[2, 2065],\n    pos_edge_label=[295],\n    pos_edge_label_index=[2, 295],\n    neg_edge_label=[295],\n    neg_edge_label_index=[2, 295],\n  },\n  (disease, contraindication, drug)={\n    edge_index=[2, 21028],\n    pos_edge_label=[3004],\n    pos_edge_label_index=[2, 3004],\n    neg_edge_label=[3004],\n    neg_edge_label_index=[2, 3004],\n  },\n  (disease, indication, drug)={\n    edge_index=[2, 6123],\n    pos_edge_label=[874],\n    pos_edge_label_index=[2, 874],\n    neg_edge_label=[874],\n    neg_edge_label_index=[2, 874],\n  },\n  (disease, off-label use, drug)={\n    edge_index=[2, 1712],\n    pos_edge_label=[244],\n    pos_edge_label_index=[2, 244],\n    neg_edge_label=[244],\n    neg_edge_label_index=[2, 244],\n  },\n  (disease, associated with, gene/protein)={\n    edge_index=[2, 54574],\n    pos_edge_label=[7796],\n    pos_edge_label_index=[2, 7796],\n    neg_edge_label=[7796],\n    neg_edge_label_index=[2, 7796],\n  },\n  (molecular_function, interacts with, gene/protein)={\n    edge_index=[2, 46733],\n    pos_edge_label=[6676],\n    pos_edge_label_index=[2, 6676],\n    neg_edge_label=[6676],\n    neg_edge_label_index=[2, 6676],\n  },\n  (cellular_component, interacts with, gene/protein)={\n    edge_index=[2, 52327],\n    pos_edge_label=[7475],\n    pos_edge_label_index=[2, 7475],\n    neg_edge_label=[7475],\n    neg_edge_label_index=[2, 7475],\n  },\n  (biological_process, interacts with, gene/protein)={\n    edge_index=[2, 95425],\n    pos_edge_label=[13632],\n    pos_edge_label_index=[2, 13632],\n    neg_edge_label=[13632],\n    neg_edge_label_index=[2, 13632],\n  }\n)\n</pre> <p>Test data</p> In\u00a0[9]: Copied! <pre>test_loader = dm.test_dataloader()\ntest_batch = next(iter(test_loader))\nprint(test_batch)\n</pre> test_loader = dm.test_dataloader() test_batch = next(iter(test_loader)) print(test_batch)  <pre>HeteroDataBatch(\n  biological_process={\n    num_nodes=27409,\n    x=[27409, 768],\n    node_name=[1],\n    batch=[27409],\n    ptr=[2],\n  },\n  cellular_component={\n    num_nodes=4011,\n    x=[4011, 768],\n    node_name=[1],\n    batch=[4011],\n    ptr=[2],\n  },\n  disease={\n    num_nodes=17054,\n    x=[17054, 768],\n    node_name=[1],\n    batch=[17054],\n    ptr=[2],\n  },\n  drug={\n    num_nodes=6759,\n    x=[6759, 512],\n    node_name=[1],\n    batch=[6759],\n    ptr=[2],\n  },\n  gene/protein={\n    num_nodes=18797,\n    x=[18797, 2560],\n    node_name=[1],\n    batch=[18797],\n    ptr=[2],\n  },\n  molecular_function={\n    num_nodes=10951,\n    x=[10951, 768],\n    node_name=[1],\n    batch=[10951],\n    ptr=[2],\n  },\n  (gene/protein, ppi, gene/protein)={\n    edge_index=[2, 503367],\n    pos_edge_label=[125841],\n    pos_edge_label_index=[2, 125841],\n    neg_edge_label=[125841],\n    neg_edge_label_index=[2, 125841],\n  },\n  (drug, carrier, gene/protein)={\n    edge_index=[2, 652],\n    pos_edge_label=[162],\n    pos_edge_label_index=[2, 162],\n    neg_edge_label=[162],\n    neg_edge_label_index=[2, 162],\n  },\n  (drug, enzyme, gene/protein)={\n    edge_index=[2, 4048],\n    pos_edge_label=[1012],\n    pos_edge_label_index=[2, 1012],\n    neg_edge_label=[1012],\n    neg_edge_label_index=[2, 1012],\n  },\n  (drug, target, gene/protein)={\n    edge_index=[2, 11936],\n    pos_edge_label=[2984],\n    pos_edge_label_index=[2, 2984],\n    neg_edge_label=[2984],\n    neg_edge_label_index=[2, 2984],\n  },\n  (drug, transporter, gene/protein)={\n    edge_index=[2, 2360],\n    pos_edge_label=[590],\n    pos_edge_label_index=[2, 590],\n    neg_edge_label=[590],\n    neg_edge_label_index=[2, 590],\n  },\n  (drug, contraindication, disease)={\n    edge_index=[2, 24032],\n    pos_edge_label=[6008],\n    pos_edge_label_index=[2, 6008],\n    neg_edge_label=[6008],\n    neg_edge_label_index=[2, 6008],\n  },\n  (drug, indication, disease)={\n    edge_index=[2, 6997],\n    pos_edge_label=[1749],\n    pos_edge_label_index=[2, 1749],\n    neg_edge_label=[1749],\n    neg_edge_label_index=[2, 1749],\n  },\n  (drug, off-label use, disease)={\n    edge_index=[2, 1956],\n    pos_edge_label=[489],\n    pos_edge_label_index=[2, 489],\n    neg_edge_label=[489],\n    neg_edge_label_index=[2, 489],\n  },\n  (drug, synergistic interaction, drug)={\n    edge_index=[2, 1787365],\n    pos_edge_label=[446841],\n    pos_edge_label_index=[2, 446841],\n    neg_edge_label=[446841],\n    neg_edge_label_index=[2, 446841],\n  },\n  (gene/protein, associated with, disease)={\n    edge_index=[2, 62370],\n    pos_edge_label=[15592],\n    pos_edge_label_index=[2, 15592],\n    neg_edge_label=[15592],\n    neg_edge_label_index=[2, 15592],\n  },\n  (disease, parent-child, disease)={\n    edge_index=[2, 51511],\n    pos_edge_label=[12877],\n    pos_edge_label_index=[2, 12877],\n    neg_edge_label=[12877],\n    neg_edge_label_index=[2, 12877],\n  },\n  (biological_process, parent-child, biological_process)={\n    edge_index=[2, 79704],\n    pos_edge_label=[19926],\n    pos_edge_label_index=[2, 19926],\n    neg_edge_label=[19926],\n    neg_edge_label_index=[2, 19926],\n  },\n  (molecular_function, parent-child, molecular_function)={\n    edge_index=[2, 21149],\n    pos_edge_label=[5287],\n    pos_edge_label_index=[2, 5287],\n    neg_edge_label=[5287],\n    neg_edge_label_index=[2, 5287],\n  },\n  (cellular_component, parent-child, cellular_component)={\n    edge_index=[2, 7360],\n    pos_edge_label=[1840],\n    pos_edge_label_index=[2, 1840],\n    neg_edge_label=[1840],\n    neg_edge_label_index=[2, 1840],\n  },\n  (gene/protein, interacts with, molecular_function)={\n    edge_index=[2, 53409],\n    pos_edge_label=[13352],\n    pos_edge_label_index=[2, 13352],\n    neg_edge_label=[13352],\n    neg_edge_label_index=[2, 13352],\n  },\n  (gene/protein, interacts with, cellular_component)={\n    edge_index=[2, 59802],\n    pos_edge_label=[14950],\n    pos_edge_label_index=[2, 14950],\n    neg_edge_label=[14950],\n    neg_edge_label_index=[2, 14950],\n  },\n  (gene/protein, interacts with, biological_process)={\n    edge_index=[2, 109057],\n    pos_edge_label=[27264],\n    pos_edge_label_index=[2, 27264],\n    neg_edge_label=[27264],\n    neg_edge_label_index=[2, 27264],\n  },\n  (gene/protein, carrier, drug)={\n    edge_index=[2, 652],\n    pos_edge_label=[162],\n    pos_edge_label_index=[2, 162],\n    neg_edge_label=[162],\n    neg_edge_label_index=[2, 162],\n  },\n  (gene/protein, enzyme, drug)={\n    edge_index=[2, 4048],\n    pos_edge_label=[1012],\n    pos_edge_label_index=[2, 1012],\n    neg_edge_label=[1012],\n    neg_edge_label_index=[2, 1012],\n  },\n  (gene/protein, target, drug)={\n    edge_index=[2, 11936],\n    pos_edge_label=[2984],\n    pos_edge_label_index=[2, 2984],\n    neg_edge_label=[2984],\n    neg_edge_label_index=[2, 2984],\n  },\n  (gene/protein, transporter, drug)={\n    edge_index=[2, 2360],\n    pos_edge_label=[590],\n    pos_edge_label_index=[2, 590],\n    neg_edge_label=[590],\n    neg_edge_label_index=[2, 590],\n  },\n  (disease, contraindication, drug)={\n    edge_index=[2, 24032],\n    pos_edge_label=[6008],\n    pos_edge_label_index=[2, 6008],\n    neg_edge_label=[6008],\n    neg_edge_label_index=[2, 6008],\n  },\n  (disease, indication, drug)={\n    edge_index=[2, 6997],\n    pos_edge_label=[1749],\n    pos_edge_label_index=[2, 1749],\n    neg_edge_label=[1749],\n    neg_edge_label_index=[2, 1749],\n  },\n  (disease, off-label use, drug)={\n    edge_index=[2, 1956],\n    pos_edge_label=[489],\n    pos_edge_label_index=[2, 489],\n    neg_edge_label=[489],\n    neg_edge_label_index=[2, 489],\n  },\n  (disease, associated with, gene/protein)={\n    edge_index=[2, 62370],\n    pos_edge_label=[15592],\n    pos_edge_label_index=[2, 15592],\n    neg_edge_label=[15592],\n    neg_edge_label_index=[2, 15592],\n  },\n  (molecular_function, interacts with, gene/protein)={\n    edge_index=[2, 53409],\n    pos_edge_label=[13352],\n    pos_edge_label_index=[2, 13352],\n    neg_edge_label=[13352],\n    neg_edge_label_index=[2, 13352],\n  },\n  (cellular_component, interacts with, gene/protein)={\n    edge_index=[2, 59802],\n    pos_edge_label=[14950],\n    pos_edge_label_index=[2, 14950],\n    neg_edge_label=[14950],\n    neg_edge_label_index=[2, 14950],\n  },\n  (biological_process, interacts with, gene/protein)={\n    edge_index=[2, 109057],\n    pos_edge_label=[27264],\n    pos_edge_label_index=[2, 27264],\n    neg_edge_label=[27264],\n    neg_edge_label_index=[2, 27264],\n  }\n)\n</pre> In\u00a0[10]: Copied! <pre># Train edge index\nprint(\"Train Edge Index:\")\nprint(dm.data[\"train\"].edge_index_dict)\n\n# Validation edge index\nprint(\"Validation Edge Index:\")\nprint(dm.data[\"val\"].edge_index_dict)\n\n# Test edge index\nprint(\"Test Edge Index:\")\nprint(dm.data[\"test\"].edge_index_dict)\n</pre> # Train edge index print(\"Train Edge Index:\") print(dm.data[\"train\"].edge_index_dict)  # Validation edge index print(\"Validation Edge Index:\") print(dm.data[\"val\"].edge_index_dict)  # Test edge index print(\"Test Edge Index:\") print(dm.data[\"test\"].edge_index_dict)  <pre>Train Edge Index:\n{('gene/protein', 'ppi', 'gene/protein'): tensor([[ 4352,  1719,  3463,  ...,  3849,  4128, 12007],\n        [ 2757,   654,  2431,  ...,  1856,  1056,  7611]]), ('drug', 'carrier', 'gene/protein'): tensor([[  529,   139,   310,  ...,   519,   141,   161],\n        [11715,  4706,  4293,  ...,  8509,  4293,  4706]]), ('drug', 'enzyme', 'gene/protein'): tensor([[  353,   662,   187,  ...,   832,   515,  1712],\n        [12284, 12284, 13047,  ...,  2283, 14833, 11967]]), ('drug', 'target', 'gene/protein'): tensor([[1842, 3015, 2537,  ..., 1497, 2999, 2302],\n        [4400, 1308, 2905,  ..., 9170,  957, 1490]]), ('drug', 'transporter', 'gene/protein'): tensor([[ 1868,  5610,   648,  ...,  3959,  2156,   967],\n        [11499, 17660,  3169,  ...,  4131,  5448,  8392]]), ('drug', 'contraindication', 'disease'): tensor([[  453,  1030,  3352,  ...,   905,  1220,   747],\n        [ 8574,  6869, 11199,  ...,  6504,  7670,  6393]]), ('drug', 'indication', 'disease'): tensor([[ 1752,  5735,   263,  ...,   573,  1296,  1738],\n        [ 3938,  8748, 11260,  ...,  2021, 11100,  8893]]), ('drug', 'off-label use', 'disease'): tensor([[  912,   287,  5735,  ...,   581,  1319,  5691],\n        [10723,  6415,  3110,  ..., 11058, 11065,  9790]]), ('drug', 'synergistic interaction', 'drug'): tensor([[6305, 5815, 3076,  ...,  567, 1337, 6242],\n        [1090,  301, 6156,  ..., 2545, 1039, 5815]]), ('gene/protein', 'associated with', 'disease'): tensor([[ 8508,  2298,  5838,  ..., 12048,  2032,  6802],\n        [11116,  5009, 11680,  ...,  1104,  7108,  1189]]), ('disease', 'parent-child', 'disease'): tensor([[ 8157,  6680,  7230,  ...,  9517,  5577,  1011],\n        [ 8296, 12209,  6038,  ...,  2004,  8899,   607]]), ('biological_process', 'parent-child', 'biological_process'): tensor([[19093,  3372, 13620,  ..., 25370,  5243,  3035],\n        [ 4055, 11062,   758,  ..., 11519, 26783,  2319]]), ('molecular_function', 'parent-child', 'molecular_function'): tensor([[ 510, 3092,  311,  ..., 9321, 6161,  807],\n        [ 618,   70, 4317,  ..., 1210,  306,   12]]), ('cellular_component', 'parent-child', 'cellular_component'): tensor([[ 507, 2956, 2322,  ...,   31, 3552, 2067],\n        [3253,  253,   53,  ..., 1356,  602,  101]]), ('gene/protein', 'interacts with', 'molecular_function'): tensor([[6112,  119, 5972,  ..., 2398, 1253, 2934],\n        [ 178, 1885,  644,  ..., 1130, 6163, 5902]]), ('gene/protein', 'interacts with', 'cellular_component'): tensor([[13308,  1718,   455,  ...,  3970,  9281, 11437],\n        [  390,  2812,  2589,  ...,   850,   135,   905]]), ('gene/protein', 'interacts with', 'biological_process'): tensor([[ 1883, 11094,  6279,  ..., 11141,  2244,  8042],\n        [15100, 21787, 14868,  ...,  2337, 26200, 22473]]), ('gene/protein', 'carrier', 'drug'): tensor([[6755,  111,  111,  ..., 4293, 4293, 4706],\n        [   0,   70,   51,  ...,  144,  197,  138]]), ('gene/protein', 'enzyme', 'drug'): tensor([[14833, 13972, 13620,  ...,  5262,  8149, 13047],\n        [   29,   426,  1093,  ...,  1152,  1010,   388]]), ('gene/protein', 'target', 'drug'): tensor([[ 3493,   197,  6498,  ...,  7141, 13774,  8956],\n        [ 2703,  3925,  4868,  ...,   174,   542,   677]]), ('gene/protein', 'transporter', 'drug'): tensor([[ 4131, 13333,  5537,  ...,  3167, 10547,  1121],\n        [  850,    51,   236,  ...,   535,   416,   458]]), ('disease', 'contraindication', 'drug'): tensor([[11006,  7292,   720,  ...,  7142,  7337,  3953],\n        [  552,   169,   177,  ...,  2937,   713,  5091]]), ('disease', 'indication', 'drug'): tensor([[11172,  7143, 11284,  ..., 11024,  8895,  1481],\n        [ 1203,  1008,   536,  ...,   867,  5722,   934]]), ('disease', 'off-label use', 'drug'): tensor([[ 6440,  2553,  1267,  ..., 11274,  7149,  8970],\n        [  539,     7,  5737,  ...,    29,   298,   257]]), ('disease', 'associated with', 'gene/protein'): tensor([[ 2869, 11489,   630,  ..., 10601,  7646,    42],\n        [ 5070, 11147,  3364,  ...,  3368, 13124,  9594]]), ('molecular_function', 'interacts with', 'gene/protein'): tensor([[  359,  8459,   515,  ...,    99,  9646,   178],\n        [15176,  6678, 13634,  ...,  9738,  2805,   233]]), ('cellular_component', 'interacts with', 'gene/protein'): tensor([[  942,  2632,   905,  ...,   837,  1135,   342],\n        [ 9615, 11092, 11211,  ..., 10837, 13143,  3542]]), ('biological_process', 'interacts with', 'gene/protein'): tensor([[24674,    34, 15908,  ...,  1581,  1365,  2561],\n        [ 6408,  3518,  1699,  ...,  5683, 14033,   396]])}\nValidation Edge Index:\n{('gene/protein', 'ppi', 'gene/protein'): tensor([[ 4352,  1719,  3463,  ...,  3849,  4128, 12007],\n        [ 2757,   654,  2431,  ...,  1856,  1056,  7611]]), ('drug', 'carrier', 'gene/protein'): tensor([[  529,   139,   310,  ...,   519,   141,   161],\n        [11715,  4706,  4293,  ...,  8509,  4293,  4706]]), ('drug', 'enzyme', 'gene/protein'): tensor([[  353,   662,   187,  ...,   832,   515,  1712],\n        [12284, 12284, 13047,  ...,  2283, 14833, 11967]]), ('drug', 'target', 'gene/protein'): tensor([[1842, 3015, 2537,  ..., 1497, 2999, 2302],\n        [4400, 1308, 2905,  ..., 9170,  957, 1490]]), ('drug', 'transporter', 'gene/protein'): tensor([[ 1868,  5610,   648,  ...,  3959,  2156,   967],\n        [11499, 17660,  3169,  ...,  4131,  5448,  8392]]), ('drug', 'contraindication', 'disease'): tensor([[  453,  1030,  3352,  ...,   905,  1220,   747],\n        [ 8574,  6869, 11199,  ...,  6504,  7670,  6393]]), ('drug', 'indication', 'disease'): tensor([[ 1752,  5735,   263,  ...,   573,  1296,  1738],\n        [ 3938,  8748, 11260,  ...,  2021, 11100,  8893]]), ('drug', 'off-label use', 'disease'): tensor([[  912,   287,  5735,  ...,   581,  1319,  5691],\n        [10723,  6415,  3110,  ..., 11058, 11065,  9790]]), ('drug', 'synergistic interaction', 'drug'): tensor([[6305, 5815, 3076,  ...,  567, 1337, 6242],\n        [1090,  301, 6156,  ..., 2545, 1039, 5815]]), ('gene/protein', 'associated with', 'disease'): tensor([[ 8508,  2298,  5838,  ..., 12048,  2032,  6802],\n        [11116,  5009, 11680,  ...,  1104,  7108,  1189]]), ('disease', 'parent-child', 'disease'): tensor([[ 8157,  6680,  7230,  ...,  9517,  5577,  1011],\n        [ 8296, 12209,  6038,  ...,  2004,  8899,   607]]), ('biological_process', 'parent-child', 'biological_process'): tensor([[19093,  3372, 13620,  ..., 25370,  5243,  3035],\n        [ 4055, 11062,   758,  ..., 11519, 26783,  2319]]), ('molecular_function', 'parent-child', 'molecular_function'): tensor([[ 510, 3092,  311,  ..., 9321, 6161,  807],\n        [ 618,   70, 4317,  ..., 1210,  306,   12]]), ('cellular_component', 'parent-child', 'cellular_component'): tensor([[ 507, 2956, 2322,  ...,   31, 3552, 2067],\n        [3253,  253,   53,  ..., 1356,  602,  101]]), ('gene/protein', 'interacts with', 'molecular_function'): tensor([[6112,  119, 5972,  ..., 2398, 1253, 2934],\n        [ 178, 1885,  644,  ..., 1130, 6163, 5902]]), ('gene/protein', 'interacts with', 'cellular_component'): tensor([[13308,  1718,   455,  ...,  3970,  9281, 11437],\n        [  390,  2812,  2589,  ...,   850,   135,   905]]), ('gene/protein', 'interacts with', 'biological_process'): tensor([[ 1883, 11094,  6279,  ..., 11141,  2244,  8042],\n        [15100, 21787, 14868,  ...,  2337, 26200, 22473]]), ('gene/protein', 'carrier', 'drug'): tensor([[6755,  111,  111,  ..., 4293, 4293, 4706],\n        [   0,   70,   51,  ...,  144,  197,  138]]), ('gene/protein', 'enzyme', 'drug'): tensor([[14833, 13972, 13620,  ...,  5262,  8149, 13047],\n        [   29,   426,  1093,  ...,  1152,  1010,   388]]), ('gene/protein', 'target', 'drug'): tensor([[ 3493,   197,  6498,  ...,  7141, 13774,  8956],\n        [ 2703,  3925,  4868,  ...,   174,   542,   677]]), ('gene/protein', 'transporter', 'drug'): tensor([[ 4131, 13333,  5537,  ...,  3167, 10547,  1121],\n        [  850,    51,   236,  ...,   535,   416,   458]]), ('disease', 'contraindication', 'drug'): tensor([[11006,  7292,   720,  ...,  7142,  7337,  3953],\n        [  552,   169,   177,  ...,  2937,   713,  5091]]), ('disease', 'indication', 'drug'): tensor([[11172,  7143, 11284,  ..., 11024,  8895,  1481],\n        [ 1203,  1008,   536,  ...,   867,  5722,   934]]), ('disease', 'off-label use', 'drug'): tensor([[ 6440,  2553,  1267,  ..., 11274,  7149,  8970],\n        [  539,     7,  5737,  ...,    29,   298,   257]]), ('disease', 'associated with', 'gene/protein'): tensor([[ 2869, 11489,   630,  ..., 10601,  7646,    42],\n        [ 5070, 11147,  3364,  ...,  3368, 13124,  9594]]), ('molecular_function', 'interacts with', 'gene/protein'): tensor([[  359,  8459,   515,  ...,    99,  9646,   178],\n        [15176,  6678, 13634,  ...,  9738,  2805,   233]]), ('cellular_component', 'interacts with', 'gene/protein'): tensor([[  942,  2632,   905,  ...,   837,  1135,   342],\n        [ 9615, 11092, 11211,  ..., 10837, 13143,  3542]]), ('biological_process', 'interacts with', 'gene/protein'): tensor([[24674,    34, 15908,  ...,  1581,  1365,  2561],\n        [ 6408,  3518,  1699,  ...,  5683, 14033,   396]])}\nTest Edge Index:\n{('gene/protein', 'ppi', 'gene/protein'): tensor([[4352, 1719, 3463,  ...,   71,   15, 3008],\n        [2757,  654, 2431,  ..., 5587, 9792, 2727]]), ('drug', 'carrier', 'gene/protein'): tensor([[  529,   139,   310,  ...,    88,   434,   496],\n        [11715,  4706,  4293,  ...,   111,  4293,  7397]]), ('drug', 'enzyme', 'gene/protein'): tensor([[  353,   662,   187,  ...,  1318,   247,   339],\n        [12284, 12284, 13047,  ...,  7821, 12284,  7821]]), ('drug', 'target', 'gene/protein'): tensor([[ 1842,  3015,  2537,  ...,  2654,    90,  5480],\n        [ 4400,  1308,  2905,  ...,  9770, 13925,  4191]]), ('drug', 'transporter', 'gene/protein'): tensor([[ 1868,  5610,   648,  ...,   275,   399,   851],\n        [11499, 17660,  3169,  ..., 10044,  5448, 14035]]), ('drug', 'contraindication', 'disease'): tensor([[  453,  1030,  3352,  ...,  5632,  1017,   594],\n        [ 8574,  6869, 11199,  ...,  6398,  9892,  6452]]), ('drug', 'indication', 'disease'): tensor([[ 1752,  5735,   263,  ...,  5541,    18,  3209],\n        [ 3938,  8748, 11260,  ...,  9075,  5152, 11438]]), ('drug', 'off-label use', 'disease'): tensor([[  912,   287,  5735,  ...,  5494,    16,  1197],\n        [10723,  6415,  3110,  ...,  1383, 10980, 11217]]), ('drug', 'synergistic interaction', 'drug'): tensor([[6305, 5815, 3076,  ..., 1264,  551,  629],\n        [1090,  301, 6156,  ..., 1103,  205, 3762]]), ('gene/protein', 'associated with', 'disease'): tensor([[ 8508,  2298,  5838,  ..., 12791,  9841,  1533],\n        [11116,  5009, 11680,  ...,  6879,  9350,  1690]]), ('disease', 'parent-child', 'disease'): tensor([[ 8157,  6680,  7230,  ...,  5332,  6845,  9596],\n        [ 8296, 12209,  6038,  ..., 11268, 13494,  5046]]), ('biological_process', 'parent-child', 'biological_process'): tensor([[19093,  3372, 13620,  ...,  9034,   177,  2396],\n        [ 4055, 11062,   758,  ..., 25223, 13208, 14756]]), ('molecular_function', 'parent-child', 'molecular_function'): tensor([[ 510, 3092,  311,  ..., 1364,  787, 5273],\n        [ 618,   70, 4317,  ..., 1856, 1679,  408]]), ('cellular_component', 'parent-child', 'cellular_component'): tensor([[ 507, 2956, 2322,  ...,  439,   15,   90],\n        [3253,  253,   53,  ..., 3072, 3556, 1566]]), ('gene/protein', 'interacts with', 'molecular_function'): tensor([[ 6112,   119,  5972,  ...,  3569,  1148, 14731],\n        [  178,  1885,   644,  ...,   757,   178,   324]]), ('gene/protein', 'interacts with', 'cellular_component'): tensor([[13308,  1718,   455,  ...,  5068, 13367, 15515],\n        [  390,  2812,  2589,  ...,  2812,   652,   589]]), ('gene/protein', 'interacts with', 'biological_process'): tensor([[ 1883, 11094,  6279,  ...,  7056,  5918, 15652],\n        [15100, 21787, 14868,  ..., 12873,  9670,  8590]]), ('gene/protein', 'carrier', 'drug'): tensor([[6755,  111,  111,  ..., 4409, 4293, 4293],\n        [   0,   70,   51,  ...,  334,  207,  435]]), ('gene/protein', 'enzyme', 'drug'): tensor([[14833, 13972, 13620,  ...,  4205, 13620,  8149],\n        [   29,   426,  1093,  ...,   547,   967,  1693]]), ('gene/protein', 'target', 'drug'): tensor([[ 3493,   197,  6498,  ...,   532,  8760, 11406],\n        [ 2703,  3925,  4868,  ...,  1038,  2582,  5305]]), ('gene/protein', 'transporter', 'drug'): tensor([[ 4131, 13333,  5537,  ...,  2581, 10044,  4131],\n        [  850,    51,   236,  ...,  1080,  1686,   997]]), ('disease', 'contraindication', 'drug'): tensor([[11006,  7292,   720,  ...,  6396,  7143,  7069],\n        [  552,   169,   177,  ...,   301,  1060,  2234]]), ('disease', 'indication', 'drug'): tensor([[11172,  7143, 11284,  ...,  8720, 11135, 11102],\n        [ 1203,  1008,   536,  ...,  1383,  1214,  2279]]), ('disease', 'off-label use', 'drug'): tensor([[ 6440,  2553,  1267,  ..., 11252,  5640,  5967],\n        [  539,     7,  5737,  ...,  5614,  1729,   207]]), ('disease', 'associated with', 'gene/protein'): tensor([[ 2869, 11489,   630,  ...,  4739, 11103,  7799],\n        [ 5070, 11147,  3364,  ...,  2615,  1283, 14266]]), ('molecular_function', 'interacts with', 'gene/protein'): tensor([[  359,  8459,   515,  ...,   855, 10343,  1564],\n        [15176,  6678, 13634,  ..., 10366, 11589,   113]]), ('cellular_component', 'interacts with', 'gene/protein'): tensor([[  942,  2632,   905,  ...,    17,   905,   462],\n        [ 9615, 11092, 11211,  ...,  2106,  4363,  6032]]), ('biological_process', 'interacts with', 'gene/protein'): tensor([[24674,    34, 15908,  ...,  6900, 12627,  6227],\n        [ 6408,  3518,  1699,  ..., 15330, 11274,  8507]])}\n</pre> In\u00a0[11]: Copied! <pre>print(\"\ud83d\udccc Available node types:\")\nprint(dm.data[\"train\"].node_types)\n\nprint(\"\ud83d\udccc Available edge types:\")\nprint(dm.data[\"train\"].edge_types)\n</pre> print(\"\ud83d\udccc Available node types:\") print(dm.data[\"train\"].node_types)  print(\"\ud83d\udccc Available edge types:\") print(dm.data[\"train\"].edge_types)  <pre>\ud83d\udccc Available node types:\n[np.str_('biological_process'), np.str_('cellular_component'), np.str_('disease'), np.str_('drug'), np.str_('gene/protein'), np.str_('molecular_function')]\n\ud83d\udccc Available edge types:\n[('gene/protein', 'ppi', 'gene/protein'), ('drug', 'carrier', 'gene/protein'), ('drug', 'enzyme', 'gene/protein'), ('drug', 'target', 'gene/protein'), ('drug', 'transporter', 'gene/protein'), ('drug', 'contraindication', 'disease'), ('drug', 'indication', 'disease'), ('drug', 'off-label use', 'disease'), ('drug', 'synergistic interaction', 'drug'), ('gene/protein', 'associated with', 'disease'), ('disease', 'parent-child', 'disease'), ('biological_process', 'parent-child', 'biological_process'), ('molecular_function', 'parent-child', 'molecular_function'), ('cellular_component', 'parent-child', 'cellular_component'), ('gene/protein', 'interacts with', 'molecular_function'), ('gene/protein', 'interacts with', 'cellular_component'), ('gene/protein', 'interacts with', 'biological_process'), ('gene/protein', 'carrier', 'drug'), ('gene/protein', 'enzyme', 'drug'), ('gene/protein', 'target', 'drug'), ('gene/protein', 'transporter', 'drug'), ('disease', 'contraindication', 'drug'), ('disease', 'indication', 'drug'), ('disease', 'off-label use', 'drug'), ('disease', 'associated with', 'gene/protein'), ('molecular_function', 'interacts with', 'gene/protein'), ('cellular_component', 'interacts with', 'gene/protein'), ('biological_process', 'interacts with', 'gene/protein')]\n</pre> <p>Step 6: \ud83d\udd78\ufe0f Visualize a Subgraph of the Knowledge Graph</p> <p>Plotting Local Neighborhood of a Node in a Heterogeneous Knowledge Graph (DYNC1I2 Example)</p> In\u00a0[12]: Copied! <pre>import pickle\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nfrom torch_geometric.utils import to_networkx\n\n# Load data\nwith open(\"biobridge_cache.pkl\", \"rb\") as f:\n    data = pickle.load(f)\n\nhetero_data = data[\"init\"]\nmapper = data.get(\"mapper\", {})  # might be empty\n\ntarget_node_type = \"gene/protein\"\ntarget_node_name = \"DYNC1I2\"\n\n# Manual map fallback\nmanual_map = {\"DYNC1I2\": 6726}\nlocal_idx = mapper.get(target_node_type, {}).get(target_node_name) or manual_map.get(target_node_name)\nif local_idx is None:\n    raise ValueError(f\"Node {target_node_name} not found\")\n\n# Convert to NetworkX graph\nG_full = to_networkx(hetero_data)\nG_full = nx.Graph(G_full)  # undirected\n\n\n# Relabel nodes with (type, index)\noffset = 0\nmapping = {}\nfor node_type in hetero_data.node_types:\n    n_nodes = hetero_data[node_type].num_nodes\n    for i in range(n_nodes):\n        mapping[offset + i] = (node_type, i)\n    offset += n_nodes\nG_full = nx.relabel_nodes(G_full, mapping)\n\ntarget_node_nx = (target_node_type, local_idx)\nif target_node_nx not in G_full:\n    raise ValueError(f\"Node {target_node_nx} not found in graph\")\n\n# Build 1-hop subgraph\nneighbors = list(G_full.adj[target_node_nx])\nsub_nodes = [target_node_nx] + neighbors\nG_sub = G_full.subgraph(sub_nodes).copy()\nprint(G_sub)\n\n# Build reverse_mapper from hetero_data.node_name (more reliable)\nreverse_mapper = {}\nfor node_type in hetero_data.node_types:\n    node_names = hetero_data[node_type].node_name  # numpy array of names\n    if node_names is None:\n        continue  # skip if missing\n    for idx, name in enumerate(node_names):\n        # node_name might be bytes if pickled, decode if needed\n        if isinstance(name, bytes):\n            name = name.decode('utf-8')\n        reverse_mapper[(node_type, idx)] = name\n\n# Add manual reverse map for known missing mappings\nmanual_reverse_map = {\n    (target_node_type, local_idx): target_node_name\n}\nreverse_mapper.update(manual_reverse_map)\n\n# Create labels using node names, fallback to \"type_index\"\nlabels = {\n    node: reverse_mapper.get(node, f\"{node[0]}_{node[1]}\")\n    for node in G_sub.nodes\n}\n\nnode_types = {node: node[0] for node in G_sub.nodes}\nunique_types = sorted(set(node_types.values()))\npalette = list(mcolors.TABLEAU_COLORS.values())\ncolor_map = {ntype: palette[i % len(palette)] for i, ntype in enumerate(unique_types)}\nnode_colors = [color_map[node_types[node]] for node in G_sub.nodes]\n\n# Plot\nplt.figure(figsize=(10, 8))\npos = nx.spring_layout(G_sub, seed=42)\nnx.draw_networkx_nodes(G_sub, pos, node_color=node_colors, node_size=500, alpha=0.85)\nnx.draw_networkx_edges(G_sub, pos, edge_color='gray', alpha=0.6)\nnx.draw_networkx_labels(G_sub, pos, labels, font_size=8)\n\n# Legend\nfor ntype, color in color_map.items():\n    plt.scatter([], [], color=color, label=ntype)\nplt.legend(title=\"Node Types\", loc=\"upper left\")\n\nplt.title(f\"1-hop Neighbors of {target_node_name} in Knowledge Graph\")\nplt.axis(\"off\")\nplt.tight_layout()\nplt.show()\n</pre> import pickle import networkx as nx import matplotlib.pyplot as plt import matplotlib.colors as mcolors from torch_geometric.utils import to_networkx  # Load data with open(\"biobridge_cache.pkl\", \"rb\") as f:     data = pickle.load(f)  hetero_data = data[\"init\"] mapper = data.get(\"mapper\", {})  # might be empty  target_node_type = \"gene/protein\" target_node_name = \"DYNC1I2\"  # Manual map fallback manual_map = {\"DYNC1I2\": 6726} local_idx = mapper.get(target_node_type, {}).get(target_node_name) or manual_map.get(target_node_name) if local_idx is None:     raise ValueError(f\"Node {target_node_name} not found\")  # Convert to NetworkX graph G_full = to_networkx(hetero_data) G_full = nx.Graph(G_full)  # undirected   # Relabel nodes with (type, index) offset = 0 mapping = {} for node_type in hetero_data.node_types:     n_nodes = hetero_data[node_type].num_nodes     for i in range(n_nodes):         mapping[offset + i] = (node_type, i)     offset += n_nodes G_full = nx.relabel_nodes(G_full, mapping)  target_node_nx = (target_node_type, local_idx) if target_node_nx not in G_full:     raise ValueError(f\"Node {target_node_nx} not found in graph\")  # Build 1-hop subgraph neighbors = list(G_full.adj[target_node_nx]) sub_nodes = [target_node_nx] + neighbors G_sub = G_full.subgraph(sub_nodes).copy() print(G_sub)  # Build reverse_mapper from hetero_data.node_name (more reliable) reverse_mapper = {} for node_type in hetero_data.node_types:     node_names = hetero_data[node_type].node_name  # numpy array of names     if node_names is None:         continue  # skip if missing     for idx, name in enumerate(node_names):         # node_name might be bytes if pickled, decode if needed         if isinstance(name, bytes):             name = name.decode('utf-8')         reverse_mapper[(node_type, idx)] = name  # Add manual reverse map for known missing mappings manual_reverse_map = {     (target_node_type, local_idx): target_node_name } reverse_mapper.update(manual_reverse_map)  # Create labels using node names, fallback to \"type_index\" labels = {     node: reverse_mapper.get(node, f\"{node[0]}_{node[1]}\")     for node in G_sub.nodes }  node_types = {node: node[0] for node in G_sub.nodes} unique_types = sorted(set(node_types.values())) palette = list(mcolors.TABLEAU_COLORS.values()) color_map = {ntype: palette[i % len(palette)] for i, ntype in enumerate(unique_types)} node_colors = [color_map[node_types[node]] for node in G_sub.nodes]  # Plot plt.figure(figsize=(10, 8)) pos = nx.spring_layout(G_sub, seed=42) nx.draw_networkx_nodes(G_sub, pos, node_color=node_colors, node_size=500, alpha=0.85) nx.draw_networkx_edges(G_sub, pos, edge_color='gray', alpha=0.6) nx.draw_networkx_labels(G_sub, pos, labels, font_size=8)  # Legend for ntype, color in color_map.items():     plt.scatter([], [], color=color, label=ntype) plt.legend(title=\"Node Types\", loc=\"upper left\")  plt.title(f\"1-hop Neighbors of {target_node_name} in Knowledge Graph\") plt.axis(\"off\") plt.tight_layout() plt.show() <pre>Graph with 21 nodes and 47 edges\n</pre>"},{"location":"notebooks/KG_dataloader/#knowledge-graph-kg-dataloader-with-lightning","title":"Knowledge Graph (KG) Dataloader with Lightning\u00b6","text":""},{"location":"notebooks/sbml_dataloader/","title":"SBMLDataModule Dataloader","text":"<p>To load and simulate data from the SBML model, follow the steps below:</p> <p>Step 1: \ud83d\udce6 Import the module</p> In\u00a0[1]: Copied! <pre>import sys\nimport os\n\n# Go up to the root where `vpeleaderboard/` is located\nsys.path.append(os.path.abspath(\"../../\"))\n</pre> import sys import os  # Go up to the root where `vpeleaderboard/` is located sys.path.append(os.path.abspath(\"../../\"))  In\u00a0[\u00a0]: Copied! <pre>from vpeleaderboard.data.src.sbml.sbml_dataloader import SBMLDataModule\n</pre> from vpeleaderboard.data.src.sbml.sbml_dataloader import SBMLDataModule <pre>c:\\Users\\hsrak\\Documents\\VPELeaderboard\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <p>Step 2: \u2699\ufe0f Initialize the SBMLDataModule</p> <p>Initialize the module by specifying the model base name (without the .xml extension).</p> In\u00a0[3]: Copied! <pre># Initialize with model base name (without `.xml`)\nmodule = SBMLDataModule(file_name=\"BIOMD0000000537_url\")\n</pre> # Initialize with model base name (without `.xml`) module = SBMLDataModule(file_name=\"BIOMD0000000537_url\")  <p>Step 3: \ud83e\uddf9 Prepare the model and config</p> <p>Prepare the model by loading the configuration YAML and locating the SBML file.</p> In\u00a0[4]: Copied! <pre>module.prepare_data()  # Loads config YAML and locates SBML file\n</pre> module.prepare_data()  # Loads config YAML and locates SBML file <p>Step 4: \ud83e\udde0 Setup the SBML model</p> <p>Set up the model using the basico method.</p> In\u00a0[5]: Copied! <pre>module.setup()  # Loads the model using basico\n</pre> module.setup()  # Loads the model using basico  <p>Step 5: \ud83d\udcc8 Run simulation and access train DataFrame</p> <p>Run the simulation and extract the data into a Pandas DataFrame.</p> In\u00a0[6]: Copied! <pre># Load the dataloader\ntrain_loader = module.train_dataloader()\n\n# Extract the DataFrame directly from the dataset\ntrain_df = train_loader.dataset.data\n\n# Show simulation preview\ntrain_df\n</pre> # Load the dataloader train_loader = module.train_dataloader()  # Extract the DataFrame directly from the dataset train_df = train_loader.dataset.data  # Show simulation preview train_df  Out[6]: sR{serum} sgp130{serum} R_IL6_gp130{liver} IL6{serum} Ab{serum} R sR_IL6{gut} sR_IL6{liver} R_IL6_gp130{gut} Ab_sR{serum} ... sgp130{liver} sR_IL6_sgp130{gut} Ab{peripheral} sR_IL6_sgp130{liver} pSTAT3{gut} STAT3{liver} CRP Suppression (%) CRP (% of baseline) CRP{liver} geneProduct Time 0.0 4.253507 3.900000 0.000066 0.000436 2.381820e-29 0.438236 0.001307 0.000976 0.000084 6.104391e-26 ... 5.589699 0.136304 1.679209e-29 0.116344 9.389364 0.777537 -0.000000 100.000000 158.325847 159.803597 1.0 0.000031 3.901765 0.000064 0.000638 6.753452e+02 0.000178 0.001055 0.000951 0.000072 7.791481e+00 ... 5.591034 0.127067 2.144021e-01 0.114914 9.389266 0.777559 0.000001 99.999999 158.325653 159.802738 2.0 0.000037 3.905215 0.000062 0.000739 6.522828e+02 0.000070 0.000959 0.000916 0.000068 9.015753e+00 ... 5.594230 0.115733 4.442855e-01 0.110774 9.388692 0.777686 0.000029 99.999971 158.324515 159.797695 3.0 0.000043 3.907882 0.000060 0.000756 6.303828e+02 0.000049 0.000887 0.000868 0.000065 1.019966e+01 ... 5.597806 0.107157 6.659729e-01 0.105075 9.387640 0.777986 0.000156 99.999844 158.321844 159.788467 4.0 0.000049 3.909825 0.000058 0.000755 6.095828e+02 0.000039 0.000826 0.000817 0.000062 1.134655e+01 ... 5.600867 0.099879 8.798707e-01 0.098958 9.386196 0.778528 0.000495 99.999505 158.317000 159.775787 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 96.0 0.000960 3.900048 0.000008 0.000643 1.690247e+02 0.000086 0.000002 0.000002 0.000009 6.443776e+01 ... 5.581057 0.000269 8.875419e+00 0.000263 8.689731 1.591484 4.118263 95.881737 150.735143 153.432452 97.0 0.000972 3.900045 0.000007 0.000643 1.680433e+02 0.000087 0.000002 0.000002 0.000009 6.483892e+01 ... 5.581051 0.000258 8.918491e+00 0.000252 8.677568 1.605962 4.200370 95.799630 150.594039 153.317467 98.0 0.000984 3.900042 0.000007 0.000643 1.670710e+02 0.000088 0.000002 0.000002 0.000009 6.523864e+01 ... 5.581045 0.000247 8.961150e+00 0.000241 8.665300 1.620560 4.283303 95.716697 150.451544 153.201341 99.0 0.000995 3.900039 0.000007 0.000643 1.661073e+02 0.000089 0.000002 0.000002 0.000009 6.563691e+01 ... 5.581040 0.000237 9.003399e+00 0.000232 8.652927 1.635279 4.367068 95.632932 150.307646 153.084063 100.0 0.001007 3.900036 0.000007 0.000643 1.651518e+02 0.000090 0.000002 0.000002 0.000009 6.603375e+01 ... 5.581035 0.000228 9.045242e+00 0.000222 8.640447 1.650118 4.451671 95.548329 150.162334 152.965623 <p>101 rows \u00d7 44 columns</p> In\u00a0[7]: Copied! <pre># Load the dataloader\ntrain_loader = module.val_dataloader()\n\n# Extract the DataFrame directly from the dataset\ntrain_df = train_loader.dataset.data\n\n# Show simulation preview\ntrain_df\n</pre> # Load the dataloader train_loader = module.val_dataloader()  # Extract the DataFrame directly from the dataset train_df = train_loader.dataset.data  # Show simulation preview train_df Out[7]: sR{serum} sgp130{serum} R_IL6_gp130{liver} IL6{serum} Ab{serum} R sR_IL6{gut} sR_IL6{liver} R_IL6_gp130{gut} Ab_sR{serum} ... sgp130{liver} sR_IL6_sgp130{gut} Ab{peripheral} sR_IL6_sgp130{liver} pSTAT3{gut} STAT3{liver} CRP Suppression (%) CRP (% of baseline) CRP{liver} geneProduct Time 100.0 0.001007 3.900036 0.000007 0.000643 165.151830 0.000090 0.000002 1.885293e-06 0.000009 66.033750 ... 5.581035 0.000228 9.045242 0.000222 8.640447 1.650118 4.451671 95.548329 150.162334 152.965623 100.3 0.001011 3.900036 0.000007 0.000643 164.866762 0.000090 0.000002 1.863234e-06 0.000009 66.152523 ... 5.581034 0.000225 9.057716 0.000220 8.636683 1.654593 4.477216 95.522784 150.118463 152.929863 100.6 0.001014 3.900035 0.000007 0.000643 164.582401 0.000091 0.000002 1.841614e-06 0.000009 66.271167 ... 5.581032 0.000223 9.070154 0.000217 8.632909 1.659079 4.502837 95.497163 150.074463 152.893997 100.9 0.001018 3.900034 0.000007 0.000643 164.298740 0.000091 0.000002 1.820424e-06 0.000009 66.389683 ... 5.581031 0.000220 9.082556 0.000214 8.629125 1.663576 4.528535 95.471465 150.030335 152.858025 101.2 0.001021 3.900034 0.000007 0.000643 164.015772 0.000091 0.000002 1.799658e-06 0.000009 66.508072 ... 5.581030 0.000218 9.094922 0.000212 8.625332 1.668085 4.554309 95.445691 149.986077 152.821947 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 128.8 0.001380 3.900005 0.000005 0.000643 140.176195 0.000128 0.000001 9.195297e-07 0.000007 76.867934 ... 5.580976 0.000113 10.088813 0.000106 8.232916 2.131595 7.278681 92.721319 145.316474 149.008650 129.1 0.001384 3.900005 0.000005 0.000643 139.935180 0.000128 0.000001 9.159466e-07 0.000007 76.974824 ... 5.580976 0.000113 10.098145 0.000106 8.228154 2.137183 7.312435 92.687565 145.258702 148.961374 129.4 0.001388 3.900005 0.000005 0.000643 139.694472 0.000129 0.000001 9.124256e-07 0.000007 77.081590 ... 5.580976 0.000113 10.107447 0.000106 8.223382 2.142783 7.346285 92.653715 145.200767 148.913962 129.7 0.001392 3.900005 0.000005 0.000643 139.454070 0.000129 0.000001 9.089656e-07 0.000007 77.188233 ... 5.580976 0.000112 10.116719 0.000105 8.218598 2.148395 7.380231 92.619769 145.142669 148.866414 130.0 0.001397 3.900005 0.000005 0.000643 139.213971 0.000129 0.000001 9.055653e-07 0.000007 77.294752 ... 5.580975 0.000112 10.125962 0.000105 8.213803 2.154020 7.414273 92.585727 145.084408 148.818730 <p>101 rows \u00d7 44 columns</p> In\u00a0[8]: Copied! <pre># Load the dataloader\ntrain_loader = module.test_dataloader()\n\n# Extract the DataFrame directly from the dataset\ntrain_df = train_loader.dataset.data\n\n# Show simulation preview\ntrain_df\n</pre> # Load the dataloader train_loader = module.test_dataloader()  # Extract the DataFrame directly from the dataset train_df = train_loader.dataset.data  # Show simulation preview train_df Out[8]: sR{serum} sgp130{serum} R_IL6_gp130{liver} IL6{serum} Ab{serum} R sR_IL6{gut} sR_IL6{liver} R_IL6_gp130{gut} Ab_sR{serum} ... sgp130{liver} sR_IL6_sgp130{gut} Ab{peripheral} sR_IL6_sgp130{liver} pSTAT3{gut} STAT3{liver} CRP Suppression (%) CRP (% of baseline) CRP{liver} geneProduct Time 130.0 0.001397 3.900005 0.000005 0.000643 139.213971 0.000129 1.021915e-06 9.055653e-07 0.000007 77.294752 ... 5.580975 0.000112 10.125962 0.000105 8.213803 2.154020 7.414273 92.585727 145.084408 148.818730 130.5 0.001404 3.900005 0.000005 0.000643 138.814482 0.000130 1.016836e-06 9.000287e-07 0.000007 77.472009 ... 5.580975 0.000111 10.141299 0.000104 8.205787 2.163421 7.471227 92.528773 144.986941 148.738951 131.0 0.001411 3.900005 0.000005 0.000643 138.415819 0.000131 1.011916e-06 8.946486e-07 0.000007 77.648924 ... 5.580975 0.000111 10.156554 0.000103 8.197739 2.172857 7.528449 92.471551 144.889019 148.658791 131.5 0.001418 3.900005 0.000005 0.000643 138.017981 0.000132 1.007151e-06 8.894215e-07 0.000007 77.825496 ... 5.580975 0.000110 10.171727 0.000103 8.189660 2.182326 7.585942 92.414058 144.790638 148.578249 132.0 0.001426 3.900004 0.000005 0.000643 137.620962 0.000132 1.002538e-06 8.843432e-07 0.000006 78.001725 ... 5.580974 0.000109 10.186818 0.000102 8.181550 2.191830 7.643706 92.356294 144.691798 148.497322 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 178.0 0.002237 3.900000 0.000003 0.000643 104.073070 0.000217 8.993366e-07 7.262020e-07 0.000004 92.687652 ... 5.580967 0.000095 11.249528 0.000085 7.293227 3.216138 14.272712 85.727288 133.382560 139.173850 178.5 0.002248 3.900000 0.000003 0.000643 103.737957 0.000218 9.000409e-07 7.261637e-07 0.000004 92.829869 ... 5.580967 0.000095 11.257798 0.000085 7.281962 3.228905 14.360736 85.639264 133.232879 139.049463 179.0 0.002259 3.900000 0.000003 0.000643 103.403456 0.000219 9.007694e-07 7.261462e-07 0.000004 92.971691 ... 5.580967 0.000095 11.266003 0.000085 7.270662 3.241707 14.449138 85.550862 133.082567 138.924522 179.5 0.002269 3.900000 0.000003 0.000643 103.069566 0.000220 9.015220e-07 7.261493e-07 0.000004 93.113118 ... 5.580967 0.000095 11.274143 0.000085 7.259326 3.254543 14.537921 85.462079 132.931624 138.799027 180.0 0.002280 3.900000 0.000003 0.000643 102.736288 0.000221 9.022986e-07 7.261728e-07 0.000004 93.254148 ... 5.580967 0.000095 11.282218 0.000085 7.247955 3.267413 14.627084 85.372916 132.780047 138.672974 <p>101 rows \u00d7 44 columns</p>"},{"location":"notebooks/sbml_dataloader/#sbmldatamodule-dataloader","title":"SBMLDataModule Dataloader\u00b6","text":""},{"location":"user-guide/","title":"Getting started","text":""},{"location":"user-guide/#getting-started-with-vpeleaderboard","title":"Getting Started with VPELeaderboard","text":"<p>Welcome to VPELeaderboard, an open-source project developed by Team VPE. The primary objective of this project is to evaluate and benchmark the performance of advanced time series forecasting models when applied to simulated biological data. Unlike traditional methods that rely on handcrafted mathematical models, we focus on directly forecasting simulation results through AI-driven techniques. This leaderboard provides a comprehensive analysis of the strengths and limitations of these models, offering valuable insights into their effectiveness in predicting the dynamics of complex biological systems.</p> <p>Our toolkit currently consists of the following components:</p> <ul> <li>Data: Explore integrated datasets, including ordinary differential equation models in SBML format.</li> <li>Algorithms: A collection of cutting-edge algorithms that have been developed to tackle time series forecasting and predictive modeling tasks.</li> <li>Leaderboard: An interactive leaderboard that evaluates and compares the performance of time series forecasting models, focusing on how well they predict the trajectories of simulated biological processes.</li> </ul>"},{"location":"user-guide/#how-to-use-vpeleaderboard","title":"How to Use VPELeaderboard","text":"<p>Follow the steps below to get started with VPELeaderboard:</p>"},{"location":"user-guide/#1-clone-the-repository","title":"1. Clone the Repository","text":"<p>Clone the repository to your local machine:</p> <pre><code>git clone https://github.com/your-username/VPELeaderboard.git\n</code></pre> <p>Ensure that you replace <code>your-username</code> with your actual GitHub username.</p>"},{"location":"user-guide/#2-set-up-the-environment","title":"2. Set Up the Environment","text":"<p>Ensure that your Python environment is properly set up:</p> <pre><code>python -m venv vpe-env\nsource vpe-env/bin/activate  # For macOS/Linux\nvpe-env\\\\Scripts\\\\activate     # For Windows\n</code></pre>"},{"location":"user-guide/#3-install-dependencies","title":"3. Install Dependencies","text":"<p>Navigate to the project directory and install the required dependencies:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"user-guide/#contribute-your-models","title":"Contribute Your Models","text":"<p>To contribute new models to the leaderboard, please follow the instructions in the  How to Load and Contribute SBML Models section. This guide provides the necessary steps for preparing and submitting your models, ensuring they are automatically validated and integrated into the leaderboard system via our CI/CD pipeline.</p>"},{"location":"user-guide/#explore-tutorials","title":"Explore Tutorials","text":"<p>We provide a range of tutorials to assist you in using the platform effectively. Visit the Tutorials section in Data page for comprehensive guides and examples.</p>"},{"location":"user-guide/#benchmark-your-algorithm","title":"Benchmark Your Algorithm","text":"<p>With VPELeaderboard, you can benchmark the performance of your algorithm against others based on specific datasets and performance metrics.</p>"}]}